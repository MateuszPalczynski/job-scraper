{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "022445f9-a9a0-48a4-8be2-590e0b29fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_scrap(page_num, link_before=\"https://it.pracuj.pl/praca?pn=\", link_after=\"&its=big-data-science%2Cai-ml\"):   \n",
    "    \n",
    "    all_links = []\n",
    "\n",
    "    URL = link_before + str(page_num) + link_after\n",
    "\n",
    "    scraper = cloudscraper.create_scraper() \n",
    "    page = scraper.get(URL)\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    links = soup.find_all(\"a\", class_=\"tiles_o1859gd9 core_n194fgoq\") \n",
    "    \n",
    "    for link in links:\n",
    "        if \"href\" in link.attrs:\n",
    "            all_links.append(link[\"href\"])\n",
    "\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9977f2ec-2d25-4969-a708-f3ea3b475810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_job_listing(url):\n",
    "    \"\"\"\n",
    "    Scrapes a job listing page and returns a dictionary with parsed job details.\n",
    "    \n",
    "    The returned dictionary includes:\n",
    "        - url: The URL of the job listing.\n",
    "        - title: Job title from the <h1> element.\n",
    "        - work_location: The location of the work/company.\n",
    "        - validity: The validity period of the job offer.\n",
    "        - contract_type: The type of contract/employment.\n",
    "        - employment_type: Employment type (e.g., full-time).\n",
    "        - position: The job position title.\n",
    "        - work_arrangement: Work arrangement details (e.g., remote, hybrid).\n",
    "        - start: Information on immediate employment.\n",
    "        - recruitment_method: The method of recruitment.\n",
    "        - additional_info: Any additional information.\n",
    "        - technologies: List of technologies (from aggregate open dictionary model).\n",
    "        - responsibilities: List of responsibilities (from the first aggregate bullet model).\n",
    "        - requirements: List of requirements (from the second aggregate bullet model, if present).\n",
    "        - application_link: The first application link found on the page.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL of the job listing page.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with the scraped job details.\n",
    "    \"\"\"\n",
    "    # Fetch the page content\n",
    "    scraper = cloudscraper.create_scraper() \n",
    "    response = scraper.get(url)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error fetching the URL: {url} (Status code: {response.status_code})\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Initialize lists to hold various benefit details\n",
    "    sections_benefit_list = []\n",
    "    aggregate_open_dictionary_model = []\n",
    "    aggregate_bullet_model_1 = []\n",
    "    aggregate_bullet_model_2 = []\n",
    "    \n",
    "    # Extract all \"aggregate-bullet-model\" lists (these may occur in multiple locations)\n",
    "    aggregate_bullet_models = soup.find_all('ul', {'data-test': 'aggregate-bullet-model'})\n",
    "    if len(aggregate_bullet_models) > 0:\n",
    "        aggregate_bullet_model_1 = [li.get_text(strip=True) for li in aggregate_bullet_models[0].find_all('li')]\n",
    "    if len(aggregate_bullet_models) > 1:\n",
    "        aggregate_bullet_model_2 = [li.get_text(strip=True) for li in aggregate_bullet_models[1].find_all('li')]\n",
    "    \n",
    "    # Extract additional lists: \"sections-benefit-list\" and \"aggregate-open-dictionary-model\"\n",
    "    data_lists = soup.find_all('ul', {'data-test': ['sections-benefit-list', 'aggregate-open-dictionary-model']})\n",
    "    for data_list in data_lists:\n",
    "        list_type = data_list.get('data-test')\n",
    "        items = [li.get_text(strip=True) for li in data_list.find_all('li')]\n",
    "        if list_type == 'sections-benefit-list':\n",
    "            sections_benefit_list.extend(items)\n",
    "        elif list_type == 'aggregate-open-dictionary-model':\n",
    "            aggregate_open_dictionary_model.extend(items)\n",
    "    \n",
    "    # Extract all links with the specific class \"b14qiyz3\"\n",
    "    links_list = [a.get('href') for a in soup.find_all('a', class_='b14qiyz3')]\n",
    "    \n",
    "    def parse_benefit_list(benefit_list):\n",
    "        \"\"\"\n",
    "        Parses a list of benefit strings and returns a dictionary with job attributes.\n",
    "        \n",
    "        The returned dictionary includes:\n",
    "            - work_location: Work or company location.\n",
    "            - validity: Validity period of the job offer.\n",
    "            - contract_type: Type of contract or agreement.\n",
    "            - employment_type: Employment type (e.g., full-time).\n",
    "            - position: Job position title.\n",
    "            - work_arrangement: Details on work arrangement (e.g., remote or hybrid).\n",
    "            - start: Immediate employment information.\n",
    "            - recruitment_method: Method of recruitment.\n",
    "            - additional_info: Any other information.\n",
    "        \"\"\"\n",
    "        parsed = {\n",
    "            \"work_location\": None,\n",
    "            \"validity\": None,\n",
    "            \"contract_type\": None,\n",
    "            \"employment_type\": None,\n",
    "            \"position\": None,\n",
    "            \"work_arrangement\": None,\n",
    "            \"start\": None,\n",
    "            \"recruitment_method\": None,\n",
    "            \"additional_info\": []\n",
    "        }\n",
    "        \n",
    "        for item in benefit_list:\n",
    "            lower_item = item.lower()\n",
    "            # Work location details\n",
    "            if (\"siedziba firmy\" in lower_item or \"company location\" in lower_item or \n",
    "                \"miejsce pracy\" in lower_item or (\"work location\" in lower_item and parsed[\"work_location\"] is None)):\n",
    "                parsed[\"work_location\"] = item\n",
    "            # Validity period\n",
    "            elif \"valid for\" in lower_item or \"ważna jeszcze\" in lower_item:\n",
    "                parsed[\"validity\"] = item\n",
    "            # Contract type\n",
    "            elif \"b2b\" in lower_item or \"kontrakt\" in lower_item or \"umowa\" in lower_item:\n",
    "                parsed[\"contract_type\"] = item\n",
    "            # Employment type\n",
    "            elif \"full-time\" in lower_item or \"pełny etat\" in lower_item:\n",
    "                parsed[\"employment_type\"] = item\n",
    "            # Job position title\n",
    "            elif \"specialist\" in lower_item or \"specjalista\" in lower_item:\n",
    "                parsed[\"position\"] = item\n",
    "            # Work arrangement (e.g., hybrid, remote)\n",
    "            elif (\"hybrid\" in lower_item or \"home office\" in lower_item or \n",
    "                  \"praca zdalna\" in lower_item or \"praca hybrydowa\" in lower_item):\n",
    "                parsed[\"work_arrangement\"] = item\n",
    "            # Immediate start information\n",
    "            elif \"immediate\" in lower_item or \"od zaraz\" in lower_item:\n",
    "                parsed[\"start\"] = item\n",
    "            # Recruitment method\n",
    "            elif \"rekrutacja\" in lower_item or \"recruitment\" in lower_item:\n",
    "                parsed[\"recruitment_method\"] = item\n",
    "            # Any additional information\n",
    "            else:\n",
    "                parsed[\"additional_info\"].append(item)\n",
    "        \n",
    "        if not parsed[\"position\"]:\n",
    "            parsed[\"position\"] = None\n",
    "        if not parsed[\"additional_info\"]:\n",
    "            parsed[\"additional_info\"] = None\n",
    "        \n",
    "        return parsed\n",
    "    \n",
    "    # Parse the benefit list to extract job attributes\n",
    "    job_data = parse_benefit_list(sections_benefit_list)\n",
    "    \n",
    "    # Add additional scraped data to the job_data dictionary\n",
    "    job_data[\"technologies\"] = aggregate_open_dictionary_model\n",
    "    job_data[\"responsibilities\"] = aggregate_bullet_model_1\n",
    "    job_data[\"requirements\"] = aggregate_bullet_model_2  # Use second bullet model if available\n",
    "    job_data[\"application_link\"] = links_list[0] if links_list else None\n",
    "    \n",
    "    # Extract the job title from the <h1> element with data-test \"text-positionName\"\n",
    "    job_title_element = soup.find('h1', {'data-test': 'text-positionName'})\n",
    "    job_data[\"title\"] = job_title_element.get_text(strip=True) if job_title_element else None\n",
    "    \n",
    "    # Include the URL in the job data dictionary\n",
    "    job_data[\"url\"] = url\n",
    "    \n",
    "    return job_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64264dd3-81ac-4836-afe9-a2c903b368e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(db_name=\"jobs.db\"):\n",
    "    \"\"\"\n",
    "    Creates a SQLite database with a table for job records.\n",
    "    \n",
    "    The table has columns matching the job record dictionary keys.\n",
    "    List values are stored as JSON strings.\n",
    "    \n",
    "    Parameters:\n",
    "        db_name (str): Name of the SQLite database file.\n",
    "    \n",
    "    Returns:\n",
    "        sqlite3.Connection: The database connection.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS job_records (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            url TEXT,\n",
    "            title TEXT,\n",
    "            work_location TEXT,\n",
    "            validity TEXT,\n",
    "            contract_type TEXT,\n",
    "            employment_type TEXT,\n",
    "            position TEXT,\n",
    "            work_arrangement TEXT,\n",
    "            start TEXT,\n",
    "            recruitment_method TEXT,\n",
    "            additional_info TEXT,\n",
    "            technologies TEXT,\n",
    "            responsibilities TEXT,\n",
    "            requirements TEXT,\n",
    "            application_link TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "def insert_job_record(conn, record):\n",
    "    \"\"\"\n",
    "    Inserts a job record (as a dictionary) into the SQLite database.\n",
    "    \n",
    "    List values are converted to JSON strings.\n",
    "    \n",
    "    Parameters:\n",
    "        conn (sqlite3.Connection): The database connection.\n",
    "        record (dict): The job record dictionary.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Convert list values to JSON strings for storage.\n",
    "    for key in [\"additional_info\", \"technologies\", \"responsibilities\", \"requirements\"]:\n",
    "        if key in record and isinstance(record[key], list):\n",
    "            record[key] = json.dumps(record[key])\n",
    "    \n",
    "    columns = [\"url\", \"title\", \"work_location\", \"validity\", \"contract_type\", \n",
    "               \"employment_type\", \"position\", \"work_arrangement\", \"start\", \n",
    "               \"recruitment_method\", \"additional_info\", \"technologies\", \n",
    "               \"responsibilities\", \"requirements\", \"application_link\"]\n",
    "    \n",
    "    values = [record.get(col) for col in columns]\n",
    "    placeholders = ','.join(['?'] * len(columns))\n",
    "    query = f\"INSERT INTO job_records ({','.join(columns)}) VALUES ({placeholders})\"\n",
    "    cursor.execute(query, values)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf86fa44-47bb-4690-8630-726300f5c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(db_name=\"jobs.db\"):\n",
    "    \"\"\"\n",
    "    Queries the job_records table in the SQLite database and returns a list of records.\n",
    "    \n",
    "    List-type fields stored as JSON strings are converted back into Python objects.\n",
    "    \n",
    "    Parameters:\n",
    "        db_name (str): The SQLite database file name.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of dictionaries representing job records.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Execute a query to select all records from the job_records table.\n",
    "    cursor.execute(\"SELECT * FROM job_records\")\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    # Get the column names from the cursor description.\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    \n",
    "    records = []\n",
    "    for row in rows:\n",
    "        record = dict(zip(columns, row))\n",
    "        # Convert JSON fields back to Python lists if they are not None.\n",
    "        for key in [\"additional_info\", \"technologies\", \"responsibilities\", \"requirements\"]:\n",
    "            if record.get(key):\n",
    "                try:\n",
    "                    record[key] = json.loads(record[key])\n",
    "                except json.JSONDecodeError:\n",
    "                    record[key] = record[key]\n",
    "        records.append(record)\n",
    "    \n",
    "    conn.close()\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b474258-7996-4915-9614-e993ff50474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(conn):\n",
    "    \"\"\"\n",
    "    Removes duplicates from the job_records table, leaving only one entry for each combination of title, URL and application link.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"\"\"\n",
    "    DELETE FROM job_records\n",
    "    WHERE id NOT IN (\n",
    "        SELECT MIN(id) \n",
    "        FROM job_records \n",
    "        GROUP BY title, url, application_link\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        conn.commit()\n",
    "        print(\"Duplicate records removed successfully.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Database error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "524bb9db-f5ee-40db-8f5f-b252fbd07d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "0\n",
      "Record 1 added. Title: Administrator Baz Danych ORACLE\n",
      "1\n",
      "Record 2 added. Title: Machine Learning Engineer\n",
      "2\n",
      "Record 3 added. Title: Data Engineer (with GCP Cloud)\n",
      "3\n",
      "Record 4 added. Title: DevOps Engineer (with GCP Cloud)\n",
      "4\n",
      "Record 5 added. Title: Expert Data Engineer / Tech Lead\n",
      "5\n",
      "Record 6 added. Title: Data Engineer\n",
      "6\n",
      "Record 7 added. Title: Machine Learning Engineer\n",
      "7\n",
      "Record 8 added. Title: Senior Data Analyst\n",
      "8\n",
      "Record 9 added. Title: Business Intelligence Analyst\n",
      "9\n",
      "Record 10 added. Title: Data Architect\n",
      "10\n",
      "Record 11 added. Title: Senior AI Developer\n",
      "11\n",
      "Record 12 added. Title: Data Platform Engineer\n",
      "12\n",
      "Record 13 added. Title: Data Manager\n",
      "13\n",
      "Record 14 added. Title: Mid Data Analyst\n",
      "14\n",
      "Record 15 added. Title: Data Engineer with Azure\n",
      "15\n",
      "Record 16 added. Title: Senior Artificial Intelligence / Machine Learning Engineer\n",
      "16\n",
      "Record 17 added. Title: AI/ML engineer Python\n",
      "17\n",
      "Record 18 added. Title: Informatica PowerCenter Developer\n",
      "18\n",
      "Record 19 added. Title: Data Engineer\n",
      "19\n",
      "Record 20 added. Title: Data Warehouse Developer\n",
      "20\n",
      "Record 21 added. Title: Data Engineer\n",
      "21\n",
      "Record 22 added. Title: Data Engineering Team Lead\n",
      "22\n",
      "Record 23 added. Title: Lead / Senior Data Engineer\n",
      "23\n",
      "Record 24 added. Title: DataOps Engineer\n",
      "24\n",
      "Record 25 added. Title: Senior AI Developer\n",
      "25\n",
      "Record 26 added. Title: Migration Specialist\n",
      "26\n",
      "Record 27 added. Title: AI Engineer\n",
      "27\n",
      "Record 28 added. Title: Machine Learning Engineer in Samsung Ads Project\n",
      "28\n",
      "Record 29 added. Title: Product Master Data Specialist\n",
      "29\n",
      "Record 30 added. Title: Python Developer - Senior Consultant AI\n",
      "30\n",
      "Record 31 added. Title: Data Engineer\n",
      "31\n",
      "Record 32 added. Title: AI Solutions Architect with Azure, Copilot, Dynamics\n",
      "32\n",
      "Record 33 added. Title: IT & Data Master Data IT Product Owner Zone\n",
      "33\n",
      "Record 34 added. Title: Data and Financial Engineering Analyst\n",
      "34\n",
      "Record 35 added. Title: SQL Developer\n",
      "35\n",
      "Record 36 added. Title: Data Engineer (Azure / Databricks)\n",
      "36\n",
      "Record 37 added. Title: Staż w Commercial Excellence\n",
      "37\n",
      "Record 38 added. Title: Senior SQL Developer\n",
      "38\n",
      "Record 39 added. Title: Analityk Operacyjny\n",
      "39\n",
      "Record 40 added. Title: Specjalista ds. Danych Podstawowych\n",
      "40\n",
      "Record 41 added. Title: Power BI Developer\n",
      "41\n",
      "Record 42 added. Title: Data Scientist\n",
      "42\n",
      "Record 43 added. Title: Analityk danych e-commerce\n",
      "43\n",
      "Record 44 added. Title: Database Administrator with French\n",
      "44\n",
      "Record 45 added. Title: Data Engineer (GCP)\n",
      "45\n",
      "Record 46 added. Title: Python Developer (AI)\n",
      "Duplicate records removed successfully.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import requests\n",
    "    import sqlite3\n",
    "    import json\n",
    "    import time\n",
    "    import cloudscraper\n",
    "    from bs4 import BeautifulSoup  \n",
    "\n",
    "    result = []\n",
    "    for i in range(20):\n",
    "        time.sleep(0.2)\n",
    "        result += links_scrap(i)\n",
    "\n",
    "    print(len(result))\n",
    "   \n",
    "    job_records = []\n",
    "    \n",
    "    for i in range(len(result)):\n",
    "        time.sleep(0.2)\n",
    "        record = scrape_job_listing(result[i])\n",
    "        job_records.append(record)\n",
    "        print(f\"Record {i+1} added. Title: {record.get('title')}\")\n",
    "\n",
    "    # Save job records to the SQLite database\n",
    "    conn = sqlite3.connect(\"jobs.db\") # conn = create_db(\"jobs.db\")\n",
    "    for record in job_records:\n",
    "        insert_job_record(conn, record)\n",
    "    \n",
    "    # Usuwamy duplikaty\n",
    "    remove_duplicates(conn)\n",
    "\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ed45122d-0dd3-48b8-9785-7075e4579dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import requests\n",
    "    import sqlite3\n",
    "    import json\n",
    "    import time\n",
    "    from bs4 import BeautifulSoup \n",
    "    job_records = query_db(\"jobs.db\")\n",
    "    for idx, record in enumerate(job_records, start=1):\n",
    "        print(f\"\\n--- Job Record {idx} ---\")\n",
    "        for key, value in record.items():\n",
    "            print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf8ac1f-f388-4915-a0ac-df195f909ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
