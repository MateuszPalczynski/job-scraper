{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "022445f9-a9a0-48a4-8be2-590e0b29fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_scrap(page_num, link_before=\"https://it.pracuj.pl/praca?pn=\", link_after=\"&its=big-data-science%2Cai-ml\"):   \n",
    "    \n",
    "    all_links = []\n",
    "\n",
    "    URL = link_before + str(page_num) + link_after\n",
    "    page = requests.get(URL)\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    links = soup.find_all(\"a\", class_=\"tiles_o1859gd9 core_n194fgoq\") \n",
    "    \n",
    "    for link in links:\n",
    "        if \"href\" in link.attrs:\n",
    "            all_links.append(link[\"href\"])\n",
    "\n",
    "    return all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9977f2ec-2d25-4969-a708-f3ea3b475810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_job_listing(url):\n",
    "    \"\"\"\n",
    "    Scrapes a job listing page and returns a dictionary with parsed job details.\n",
    "    \n",
    "    The returned dictionary includes:\n",
    "        - url: The URL of the job listing.\n",
    "        - title: Job title from the <h1> element.\n",
    "        - work_location: The location of the work/company.\n",
    "        - validity: The validity period of the job offer.\n",
    "        - contract_type: The type of contract/employment.\n",
    "        - employment_type: Employment type (e.g., full-time).\n",
    "        - position: The job position title.\n",
    "        - work_arrangement: Work arrangement details (e.g., remote, hybrid).\n",
    "        - start: Information on immediate employment.\n",
    "        - recruitment_method: The method of recruitment.\n",
    "        - additional_info: Any additional information.\n",
    "        - technologies: List of technologies (from aggregate open dictionary model).\n",
    "        - responsibilities: List of responsibilities (from the first aggregate bullet model).\n",
    "        - requirements: List of requirements (from the second aggregate bullet model, if present).\n",
    "        - application_link: The first application link found on the page.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL of the job listing page.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary with the scraped job details.\n",
    "    \"\"\"\n",
    "    # Fetch the page content\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Error fetching the URL: {url} (Status code: {response.status_code})\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    \n",
    "    # Initialize lists to hold various benefit details\n",
    "    sections_benefit_list = []\n",
    "    aggregate_open_dictionary_model = []\n",
    "    aggregate_bullet_model_1 = []\n",
    "    aggregate_bullet_model_2 = []\n",
    "    \n",
    "    # Extract all \"aggregate-bullet-model\" lists (these may occur in multiple locations)\n",
    "    aggregate_bullet_models = soup.find_all('ul', {'data-test': 'aggregate-bullet-model'})\n",
    "    if len(aggregate_bullet_models) > 0:\n",
    "        aggregate_bullet_model_1 = [li.get_text(strip=True) for li in aggregate_bullet_models[0].find_all('li')]\n",
    "    if len(aggregate_bullet_models) > 1:\n",
    "        aggregate_bullet_model_2 = [li.get_text(strip=True) for li in aggregate_bullet_models[1].find_all('li')]\n",
    "    \n",
    "    # Extract additional lists: \"sections-benefit-list\" and \"aggregate-open-dictionary-model\"\n",
    "    data_lists = soup.find_all('ul', {'data-test': ['sections-benefit-list', 'aggregate-open-dictionary-model']})\n",
    "    for data_list in data_lists:\n",
    "        list_type = data_list.get('data-test')\n",
    "        items = [li.get_text(strip=True) for li in data_list.find_all('li')]\n",
    "        if list_type == 'sections-benefit-list':\n",
    "            sections_benefit_list.extend(items)\n",
    "        elif list_type == 'aggregate-open-dictionary-model':\n",
    "            aggregate_open_dictionary_model.extend(items)\n",
    "    \n",
    "    # Extract all links with the specific class \"b14qiyz3\"\n",
    "    links_list = [a.get('href') for a in soup.find_all('a', class_='b14qiyz3')]\n",
    "    \n",
    "    def parse_benefit_list(benefit_list):\n",
    "        \"\"\"\n",
    "        Parses a list of benefit strings and returns a dictionary with job attributes.\n",
    "        \n",
    "        The returned dictionary includes:\n",
    "            - work_location: Work or company location.\n",
    "            - validity: Validity period of the job offer.\n",
    "            - contract_type: Type of contract or agreement.\n",
    "            - employment_type: Employment type (e.g., full-time).\n",
    "            - position: Job position title.\n",
    "            - work_arrangement: Details on work arrangement (e.g., remote or hybrid).\n",
    "            - start: Immediate employment information.\n",
    "            - recruitment_method: Method of recruitment.\n",
    "            - additional_info: Any other information.\n",
    "        \"\"\"\n",
    "        parsed = {\n",
    "            \"work_location\": None,\n",
    "            \"validity\": None,\n",
    "            \"contract_type\": None,\n",
    "            \"employment_type\": None,\n",
    "            \"position\": None,\n",
    "            \"work_arrangement\": None,\n",
    "            \"start\": None,\n",
    "            \"recruitment_method\": None,\n",
    "            \"additional_info\": []\n",
    "        }\n",
    "        \n",
    "        for item in benefit_list:\n",
    "            lower_item = item.lower()\n",
    "            # Work location details\n",
    "            if (\"siedziba firmy\" in lower_item or \"company location\" in lower_item or \n",
    "                \"miejsce pracy\" in lower_item or (\"work location\" in lower_item and parsed[\"work_location\"] is None)):\n",
    "                parsed[\"work_location\"] = item\n",
    "            # Validity period\n",
    "            elif \"valid for\" in lower_item or \"ważna jeszcze\" in lower_item:\n",
    "                parsed[\"validity\"] = item\n",
    "            # Contract type\n",
    "            elif \"b2b\" in lower_item or \"kontrakt\" in lower_item or \"umowa\" in lower_item:\n",
    "                parsed[\"contract_type\"] = item\n",
    "            # Employment type\n",
    "            elif \"full-time\" in lower_item or \"pełny etat\" in lower_item:\n",
    "                parsed[\"employment_type\"] = item\n",
    "            # Job position title\n",
    "            elif \"specialist\" in lower_item or \"specjalista\" in lower_item:\n",
    "                parsed[\"position\"] = item\n",
    "            # Work arrangement (e.g., hybrid, remote)\n",
    "            elif (\"hybrid\" in lower_item or \"home office\" in lower_item or \n",
    "                  \"praca zdalna\" in lower_item or \"praca hybrydowa\" in lower_item):\n",
    "                parsed[\"work_arrangement\"] = item\n",
    "            # Immediate start information\n",
    "            elif \"immediate\" in lower_item or \"od zaraz\" in lower_item:\n",
    "                parsed[\"start\"] = item\n",
    "            # Recruitment method\n",
    "            elif \"rekrutacja\" in lower_item or \"recruitment\" in lower_item:\n",
    "                parsed[\"recruitment_method\"] = item\n",
    "            # Any additional information\n",
    "            else:\n",
    "                parsed[\"additional_info\"].append(item)\n",
    "        \n",
    "        if not parsed[\"position\"]:\n",
    "            parsed[\"position\"] = None\n",
    "        if not parsed[\"additional_info\"]:\n",
    "            parsed[\"additional_info\"] = None\n",
    "        \n",
    "        return parsed\n",
    "    \n",
    "    # Parse the benefit list to extract job attributes\n",
    "    job_data = parse_benefit_list(sections_benefit_list)\n",
    "    \n",
    "    # Add additional scraped data to the job_data dictionary\n",
    "    job_data[\"technologies\"] = aggregate_open_dictionary_model\n",
    "    job_data[\"responsibilities\"] = aggregate_bullet_model_1\n",
    "    job_data[\"requirements\"] = aggregate_bullet_model_2  # Use second bullet model if available\n",
    "    job_data[\"application_link\"] = links_list[0] if links_list else None\n",
    "    \n",
    "    # Extract the job title from the <h1> element with data-test \"text-positionName\"\n",
    "    job_title_element = soup.find('h1', {'data-test': 'text-positionName'})\n",
    "    job_data[\"title\"] = job_title_element.get_text(strip=True) if job_title_element else None\n",
    "    \n",
    "    # Include the URL in the job data dictionary\n",
    "    job_data[\"url\"] = url\n",
    "    \n",
    "    return job_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64264dd3-81ac-4836-afe9-a2c903b368e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db(db_name=\"jobs.db\"):\n",
    "    \"\"\"\n",
    "    Creates a SQLite database with a table for job records.\n",
    "    \n",
    "    The table has columns matching the job record dictionary keys.\n",
    "    List values are stored as JSON strings.\n",
    "    \n",
    "    Parameters:\n",
    "        db_name (str): Name of the SQLite database file.\n",
    "    \n",
    "    Returns:\n",
    "        sqlite3.Connection: The database connection.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS job_records (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            url TEXT,\n",
    "            title TEXT,\n",
    "            work_location TEXT,\n",
    "            validity TEXT,\n",
    "            contract_type TEXT,\n",
    "            employment_type TEXT,\n",
    "            position TEXT,\n",
    "            work_arrangement TEXT,\n",
    "            start TEXT,\n",
    "            recruitment_method TEXT,\n",
    "            additional_info TEXT,\n",
    "            technologies TEXT,\n",
    "            responsibilities TEXT,\n",
    "            requirements TEXT,\n",
    "            application_link TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "def insert_job_record(conn, record):\n",
    "    \"\"\"\n",
    "    Inserts a job record (as a dictionary) into the SQLite database.\n",
    "    \n",
    "    List values are converted to JSON strings.\n",
    "    \n",
    "    Parameters:\n",
    "        conn (sqlite3.Connection): The database connection.\n",
    "        record (dict): The job record dictionary.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Convert list values to JSON strings for storage.\n",
    "    for key in [\"additional_info\", \"technologies\", \"responsibilities\", \"requirements\"]:\n",
    "        if key in record and isinstance(record[key], list):\n",
    "            record[key] = json.dumps(record[key])\n",
    "    \n",
    "    columns = [\"url\", \"title\", \"work_location\", \"validity\", \"contract_type\", \n",
    "               \"employment_type\", \"position\", \"work_arrangement\", \"start\", \n",
    "               \"recruitment_method\", \"additional_info\", \"technologies\", \n",
    "               \"responsibilities\", \"requirements\", \"application_link\"]\n",
    "    \n",
    "    values = [record.get(col) for col in columns]\n",
    "    placeholders = ','.join(['?'] * len(columns))\n",
    "    query = f\"INSERT INTO job_records ({','.join(columns)}) VALUES ({placeholders})\"\n",
    "    cursor.execute(query, values)\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf86fa44-47bb-4690-8630-726300f5c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_db(db_name=\"jobs.db\"):\n",
    "    \"\"\"\n",
    "    Queries the job_records table in the SQLite database and returns a list of records.\n",
    "    \n",
    "    List-type fields stored as JSON strings are converted back into Python objects.\n",
    "    \n",
    "    Parameters:\n",
    "        db_name (str): The SQLite database file name.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of dictionaries representing job records.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Execute a query to select all records from the job_records table.\n",
    "    cursor.execute(\"SELECT * FROM job_records\")\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    # Get the column names from the cursor description.\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    \n",
    "    records = []\n",
    "    for row in rows:\n",
    "        record = dict(zip(columns, row))\n",
    "        # Convert JSON fields back to Python lists if they are not None.\n",
    "        for key in [\"additional_info\", \"technologies\", \"responsibilities\", \"requirements\"]:\n",
    "            if record.get(key):\n",
    "                try:\n",
    "                    record[key] = json.loads(record[key])\n",
    "                except json.JSONDecodeError:\n",
    "                    record[key] = record[key]\n",
    "        records.append(record)\n",
    "    \n",
    "    conn.close()\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b474258-7996-4915-9614-e993ff50474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(conn):\n",
    "    \"\"\"\n",
    "    Removes duplicates from the job_records table, leaving only one entry for each combination of title, URL and application link.\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    query = \"\"\"\n",
    "    DELETE FROM job_records\n",
    "    WHERE id NOT IN (\n",
    "        SELECT MIN(id) \n",
    "        FROM job_records \n",
    "        GROUP BY title, url, application_link\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        conn.commit()\n",
    "        print(\"Duplicate records removed successfully.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(\"Database error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "524bb9db-f5ee-40db-8f5f-b252fbd07d51",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     11\u001b[0m         result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m links_scrap(i)\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(results))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"    \u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    job_records = []\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;03m    conn.close()\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import requests\n",
    "    import sqlite3\n",
    "    import json\n",
    "    import time\n",
    "    from bs4 import BeautifulSoup  \n",
    "\n",
    "    result = []\n",
    "    for i in range(20):\n",
    "        time.sleep(0.2)\n",
    "        result += links_scrap(i)\n",
    "\n",
    "    print(len(results))\n",
    "\"\"\"    \n",
    "    job_records = []\n",
    "    \n",
    "    for i in range(len(result)):\n",
    "        time.sleep(0.2)\n",
    "        record = scrape_job_listing(result[i])\n",
    "        job_records.append(record)\n",
    "        print(f\"Record {i+1} added. Title: {record.get('title')}\")\n",
    "\n",
    "    # Save job records to the SQLite database\n",
    "    conn = sqlite3.connect(\"jobs.db\") # conn = create_db(\"jobs.db\")\n",
    "    for record in job_records:\n",
    "        insert_job_record(conn, record)\n",
    "    \n",
    "    # Usuwamy duplikaty\n",
    "    remove_duplicates(conn)\n",
    "\n",
    "    conn.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ed45122d-0dd3-48b8-9785-7075e4579dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Job Record 1 ---\n",
      "id: 1\n",
      "url: https://www.pracuj.pl/praca/head-of-data-engineering-warszawa,oferta,1003875039\n",
      "title: Head of Data Engineering\n",
      "work_location: None\n",
      "validity: valid for 3 daysto 01 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'director', 'Робота для іноземцівбез польської']\n",
      "technologies: ['data engineering', 'ETL', 'BI', 'Big Data', 'AWS', 'Azure Cosmos DB', 'Microsoft Azure', 'Databases', 'CI/CD', 'Python', 'SQL', 'Synapse/Redshift', 'Databricks/Spark', \"at the client's site\", 'you develop several projects simultaneously', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you develop the code \"from scratch\"', 'you focus on product development', 'agile']\n",
      "responsibilities: ['Team Leadership:', 'Oversee and manage a team of 10+ data engineering specialists', 'Conduct 1:1 meetings, plan career paths, and set and evaluate individual goals', 'Assign resources to projects and manage bench activities', 'Supervise and actively participate in recruitment processes', 'Onboard new employees effectively', 'Foster knowledge-sharing practices and ensure team skill development', 'Technical Expertise & Contributions:', 'Ensure the quality of deliverables produced by the team', 'Participate in selected projects as a Solution Architect', 'Represent the team as a subject matter expert in internal and external meetings', 'Build and maintain strong client relationships and support business development activities', 'Monitor market trends and propose innovative solutions in the Data Engineering domain', 'Strategic Collaboration:', 'Work closely with the Chief Delivery Officer to develop and implement strategic plans for the Data Engineering team, including team growth, specializations, technology partnerships, and R&D initiatives', 'Contribute to presales and sales processes by supporting the creation of proposals and showcasing team expertise']\n",
      "requirements: ['About 5 years of experience in leading teams in data engineering or data warehouse domains and managing specialist teams of over 10 members', 'Over 7 years of hands-on experience in designing and developing solutions in areas such as Business Intelligence, Advanced Analytics, ETL/ELT, Data Warehouse, Data Lake, or Big Data', 'More than 3 years of experience building and deploying cloud-based services, particularly with Azure or AWS data services', 'Comprehensive understanding of Data Governance, including data quality, master data management, data cataloging, and lineage', 'Proven ability to apply advanced team management techniques', 'Excellent organizational, communication, and collaboration skills', 'High proficiency in English (minimum C1 level)', 'Readiness to work in a hybrid model from the Warsaw']\n",
      "application_link: https://www.pracuj.pl/aplikuj/head-of-data-engineering-warszawa,oferta,1003875039?apid=66d885a4-190f-40c7-b945-bf09c8e2c7bf&oca=None&acv=0\n",
      "\n",
      "--- Job Record 2 ---\n",
      "id: 2\n",
      "url: https://www.pracuj.pl/praca/senior-software-engineer-python-genai-team-krakow-lubicz-23a,oferta,1003882259\n",
      "title: Senior Software Engineer (Python) - GenAI Team\n",
      "work_location: None\n",
      "validity: valid for 7 daysto 05 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Lubicz 23a, Grzegórzki, KrakówKraków, Lesser Poland', 'contract of employment']\n",
      "technologies: ['Python', 'JavaScript', 'Google Cloud Platform', 'AWS']\n",
      "responsibilities: ['Independent Execution:', 'Demonstrate the ability to execute tasks and projects independently, driving them from inception to completion with minimal supervision.', 'Exhibit self-reliance and resourcefulness in problem-solving, leveraging technical expertise to overcome challenges effectively.', 'Take ownership of assigned responsibilities and deliverables, ensuring they are completed accurately and on schedule.', 'Design and whiteboard solutions, collaborating with the team to develop and implement innovative approaches.', 'Generative AI Exploration:', \"Investigate and apply Generative AI methodologies to meet the company's specific needs and challenges.\", 'Collaborate with cross-functional teams to determine the best applications of Generative AI for the business.', 'Cloud Solutions Development:', 'Take an active role in the design and implementation of Cloud solutions, particularly on Google Cloud Platform (GCP) and Vertex AI, for assigned projects or components within the team.', 'Guide the full lifecycle of development, from initial design to production deployment, ensuring quality and efficiency.', 'Design and whiteboard solutions, collaborating with the team to develop and implement innovative approaches.', 'Continuous Learning and Improvement:', \"Stay abreast of emerging technologies and industry trends to evolve the team's capabilities continually.\", 'Foster a culture of curiosity and learning, encouraging experimentation and growth.']\n",
      "requirements: ['Bachelor’s degree in Computer Science, Engineering, or related field.', 'Minimum of 4-5 years of experience in full-stack software development, with a strong focus on Python, and development-to-production lifecycle. Experience with JavaScript (React or similar) is a plus.', 'Experience with Google Cloud Platform (GCP) or other major cloud platforms (e.g., AWS, Azure) is essential.', 'Excellent communication, collaboration, and problem-solving skills.', 'Passion for learning and applying innovative technologies, with the ability to inspire and drive innovation within the team.', 'Experience with Generative AI, deploying and supporting Large Language Models, and building/deploying Data Science related solutions is a major plus.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-software-engineer-python-genai-team-krakow-lubicz-23a,oferta,1003882259?apid=9ea9c10a-8388-4706-a70a-31422b5bd0c0&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 3 ---\n",
      "id: 3\n",
      "url: https://www.pracuj.pl/praca/assistant-vice-president-senior-engineer-ai-data-science-krakow-kapelanka-42a,oferta,1003878893\n",
      "title: Assistant Vice President Senior Engineer - AI & Data Science\n",
      "work_location: None\n",
      "validity: valid for 5 daysto 03 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'manager / supervisor', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Python', 'Django', 'Flask', 'FastAPI', 'Next.js', 'React', 'Angular', 'TypeScript']\n",
      "responsibilities: ['Support building and implementation of Python-based AI solutions for the AI modelling – frontend and backend.', 'Deliver both prototypes and production ready applications, ensuring compliance with HSBC Group standards.', 'Work closely with tech leads, business stakeholders, Quantitative Analysts and IT to create synergies across different functions and departments and contribute to the end-to-end design and planning.']\n",
      "requirements: ['Strong experience in frontend and/or full stack development of large scale web applications and systems, preferably in Python.', 'Experience in building and maintaining frontend solutions, e.g. using Next.js, React/Angular, and TypeScript.', 'Proficiency in backend development with at least two of: Python, Django, Flask, FastAPI.', '3+ years of relatable experience.', 'Experience in writing technical documentation and test-driven development.', 'Knowledge of finance, particularly quantitative finance and experience in AI/ML area is nice to have.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/assistant-vice-president-senior-engineer-ai-data-science-krakow-kapelanka-42a,oferta,1003878893?apid=79e21248-e110-4b66-8390-8530ae616c0f&oca=None&acv=0\n",
      "\n",
      "--- Job Record 4 ---\n",
      "id: 13\n",
      "url: https://www.pracuj.pl/praca/lead-ai-engineer-wroclaw-jaworska-11-13,oferta,1003909335\n",
      "title: Lead AI Engineer\n",
      "work_location: None\n",
      "validity: valid for 16 daysto 14 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Jaworska 11-13, Fabryczna, WrocławWrocław, Lower Silesia', 'contract of employment']\n",
      "technologies: ['SQL', 'Python', 'Snowflake Data Cloud', 'Microsoft Azure', 'Microsoft SQL Server']\n",
      "responsibilities: ['Lead AI development initiatives: Take ownership of technical AI development within your area of responsibility, ensuring alignment with strategic goals and business priorities', 'Design, deliver and operate AI solutions: Architect, develop, and implement scalable, maintainable, and high-performance AI and analytics solutions that address prioritized business objectives', 'Manage the full lifecycle of AI implementation: identify use cases, experiment, support in feasibility analyses and deploy models in collaboration with internal teams and third-party providers', 'Optimize Data Integration Pipelines: Develop, maintain, and optimize data pipelines and ETL processes to ensure seamless and efficient integration of data from diverse sources', 'Stay Ahead in AI Innovations: Continuously monitor advancements in AI technologies, integrating cutting-edge tools, methodologies, and best practices into ongoing and future initiatives']\n",
      "requirements: ['Proactive and Accountable: You are self-directed, responsible and action-oriented, with a strong customer service mindset and a focus on delivering substantial results', 'You are fluent in English and comfortable working in a global, multicultural environment; proficiency in additional languages is beneficial', 'Educational Background: You hold a Master’s Degree in Information Systems, Computer Science, Engineering, Economics, Management Science, or Mathematics, providing a solid foundation for technical AI development', 'Relevant Experience: You possess 3-5 years of experience in a similar role within a global organization', 'AI and Data Expertise: You have strong experience in designing and implementing AI use cases, machine learning, generative AI frameworks, Large Language Models (LLMs), and architectures, building data pipelines, and performing related data engineering activities; proficiency in programming languages like SQL and Python is essential', 'Cloud Proficiency: You bring deep knowledge of cloud platforms, tools, and architecture, including Snowflake, SQL Server, Microsoft Azure AI and Data services', 'API and Data Integration Skills: You are skilled in working with APIs and integrating data sources into advanced analytics solutions', 'Domain Knowledge: You are familiar with SAP ECC, S/4HANA, or the Ariba data model within the Source-to-Pay process area']\n",
      "application_link: https://www.pracuj.pl/aplikuj/lead-ai-engineer-wroclaw-jaworska-11-13,oferta,1003909335?apid=3f2812f8-cbd0-401f-9b73-56d58fd3ed68&oca=None&acv=0\n",
      "\n",
      "--- Job Record 5 ---\n",
      "id: 14\n",
      "url: https://www.pracuj.pl/praca/data-engineer-krakow,oferta,1003924182\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: valid for a monthto 25 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KrakówKraków, Lesser Poland', 'Запрошуємо працівників з України']\n",
      "technologies: ['SQL', 'Apache Airflow', 'Python', 'Java', 'Scala', 'AWS', 'Microsoft Azure', 'Google Cloud Platform', 'in house', \"at the client's site\", 'you focus on a single project at a time', 'you develop several projects simultaneously', 'you can change the project', 'you have influence on the product', 'you focus on product development', 'agile', 'scrum']\n",
      "responsibilities: [\"Your daily duties will include designing, implementing, and maintaining scalable data infrastructure, optimizing data pipelines, and ensuring data integrity across various platforms. You'll collaborate with cross-functional teams to drive data-driven decisions and contribute to the development of advanced analytics tools. If you enjoy creating impactful solutions and working in a collaborative environment, this role is for you.\"]\n",
      "requirements: [\"Bachelor's degree in Computer Science, Engineering, or a related field.\", 'Proven experience as a Data Engineer or in a similar role.', 'Strong knowledge of data processing technologies and database systems (SQL, NoSQL).', 'Experience with data pipeline and workflow management tools (e.g., Apache Airflow).', 'Proficiency in programming languages such as Python, Java, or Scala.', 'Familiarity with cloud services (AWS, GCP, Azure).', 'Strong problem-solving skills and attention to detail.', 'Excellent communication and teamwork abilities.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-krakow,oferta,1003924182?apid=ab1da3e9-5b4f-4336-b14b-1abb610b4b19&oca=None&acv=0\n",
      "\n",
      "--- Job Record 6 ---\n",
      "id: 15\n",
      "url: https://www.pracuj.pl/praca/sql-intern-krakow-cystersow-13,oferta,1003924223\n",
      "title: SQL Intern\n",
      "work_location: None\n",
      "validity: valid for a monthto 25 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: None\n",
      "start: Immediate employment\n",
      "recruitment_method: None\n",
      "additional_info: ['Cystersów 13, Grzegórzki, KrakówKraków, Lesser Poland', 'contract of mandate', 'trainee', 'full office work']\n",
      "technologies: ['Microsoft SQL Server', 'PL/SQL', 'in house', 'you develop several projects simultaneously', 'you can change the project', 'you have influence on the technological solutions applied', 'you have influence on the product', 'agile', 'kanban', 'waterfall', 'technical leader', 'architect', 'manual tester', 'support', 'project manager']\n",
      "responsibilities: ['Be a part of the Team and work together with developers and different Clients in an agile software-development environment;', 'Work in one of the following areas: MS SQL, Oracle, Apex, Power Platform, or Robotic Process Automation (UiPath), Power Automate, Python;', 'Design and build business apps using one of the above standards and tools;', 'Conduct analyses, and create technical documentation;', 'Participate in initiatives and internal projects;', 'Cooperate with development teams.']\n",
      "requirements: ['Knowledge of MS SQL (or PL/SQL);', 'General programming knowledge;', 'Willingness to develop as a Technical Consultant;', 'Polish level min. B2;', 'English level min. B2;', 'Proactive attitude;', 'Availability 40h per week;', 'Ability to work from the office for 6 months.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/sql-intern-krakow-cystersow-13,oferta,1003924223?apid=ce99adda-684c-4ec5-a0b6-72d36df4b091&oca=None&acv=0\n",
      "\n",
      "--- Job Record 7 ---\n",
      "id: 16\n",
      "url: https://www.pracuj.pl/praca/hadoop-developer-gdansk,oferta,1003932367\n",
      "title: Hadoop Developer\n",
      "work_location: None\n",
      "validity: valid for a monthto 28 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['GdańskGdańsk, Pomeranian']\n",
      "technologies: ['Hadoop', 'in house']\n",
      "responsibilities: ['Supporting CBA Downstream area (CBA Hadoop ecosystem)', 'Solving all Downstream related production incidents.']\n",
      "requirements: ['Hadoop experience and analytical thinking', 'Knowledge of the Hadoop ecosystem', 'Proficiency in programming languages', 'Ability to write efficient code', 'Strong analytical and problem-solving skills', 'Experience with data mining and machine learning', 'Hands-on experience with Hadoop', 'Knowledge of big data concepts', 'Cloud computing skills', 'Communication and teamwork skills', 'Continuous learning attitude']\n",
      "application_link: https://www.pracuj.pl/aplikuj/hadoop-developer-gdansk,oferta,1003932367?apid=bc443fbb-d39b-4a47-b033-00bea1b691ae&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 8 ---\n",
      "id: 17\n",
      "url: https://www.pracuj.pl/praca/data-and-business-analyst-warszawa-pulawska-2,oferta,1003897710\n",
      "title: Data and Business Analyst\n",
      "work_location: Company locationPuławska 2, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for 14 daysto 12 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'Робота для іноземцівбез польської']\n",
      "technologies: ['SQL', 'Python', 'Redshift', 'Azure Synapse', 'BigQuery', 'Data Vault', 'Snowflake', 'in house', 'agile']\n",
      "responsibilities: ['Translating business needs into data requirements', 'Analyzing data to improve quality and user experience', 'Creating simple user stories with clear acceptance criteria', 'Collaborating with teams to ensure data solutions meet business goals', 'Assisting in creating data verification tests', 'Working in Agile methodology, delivering value in iterations', 'Participating in gathering requirements from stakeholders']\n",
      "requirements: ['5 years of experience in systems, processes, and data', 'Strong analytical skills with the ability to translate business needs into data requirements', 'Familiarity with SQL and Python', 'Experience in a data product or business application role', 'Knowledge of Agile and Design Thinking methodologies', 'Excellent communication and ability to gather requirements effectively', 'Proactive, adaptable, and able to deliver value in iterative sprints']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-and-business-analyst-warszawa-pulawska-2,oferta,1003897710?apid=55e501d0-3c42-425a-a46c-741a22b2d876&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 9 ---\n",
      "id: 18\n",
      "url: https://www.pracuj.pl/praca/healthcare-analytics-intern-warszawa,oferta,1003932194\n",
      "title: Healthcare Analytics Intern\n",
      "work_location: None\n",
      "validity: valid for a monthto 28 March 2025\n",
      "contract_type: contract of employment, contract of mandate, B2B contract, internship / apprenticeship contract\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: full office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'trainee', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['SQL', 'Microsoft Power BI', 'agile', 'scrum']\n",
      "responsibilities: ['Perform strategic analyses to support consulting projects', 'Conduct quantitative and qualitative research on the pharmaceutical sector', 'Analyze market trends and key contributors in the healthcare industry', 'Prepare reports, presentations, and other client deliverables', 'Assist in formulating strategic recommendations based on data analysis', 'Utilize proprietary data sources to generate valuable insights', 'Support project teams in delivering high-quality consulting services', 'Develop knowledge of healthcare industry dynamics and emerging trends', 'Collaborate with team members to enhance analytical methodologies', 'Contribute to internal initiatives aimed at improving consulting capabilities']\n",
      "requirements: ['Strong analytical skills with a keen attention to detail', 'Willingness to learn about the pharmaceutical and healthcare industry', 'Developed interpersonal and communication skills', 'Fluency in Polish and English', 'Proficiency in MS Office, particularly Excel and PowerPoint', 'Ability to interpret and analyze large datasets', 'Strong problem-solving skills and critical thinking ability', 'Interest in market research and business intelligence', 'Ability to work effectively in a team-oriented environment', 'Strong organizational skills with the ability to manage multiple tasks']\n",
      "application_link: https://www.pracuj.pl/aplikuj/healthcare-analytics-intern-warszawa,oferta,1003932194?apid=ec95a48b-2a93-4f0e-a280-989de3aad7b4&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 10 ---\n",
      "id: 19\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-with-python-warszawa,oferta,1003881958\n",
      "title: Senior Data Engineer with Python\n",
      "work_location: None\n",
      "validity: valid for 7 daysto 05 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'More than one vacancy']\n",
      "technologies: ['Python', 'SQL', 'Oracle', 'Linux', 'Jira', 'Bitbucket', 'Bash', 'Apache Spark', 'Django', 'Flask', 'Nginx', 'HTML', 'JSP', 'ServiceNow', 'Remedy', 'Jenkins', 'in house', 'you focus on a single project at a time', 'you have influence on the product', 'you focus on product development', 'you focus on code maintenance']\n",
      "responsibilities: ['Develop and optimize Python scripts, focusing on SQL performance, caching, and data structures.', 'Design and maintain Oracle database structures.', 'Manage Linux infrastructure, including scripting, upgrades, and security enhancements.', 'Automate deployment processes and improve system scalability.', 'Utilize Jira and Bitbucket for project tracking and version control.', 'Work within an Agile Scrum environment.']\n",
      "requirements: ['At least 5 years of experience in data engineering or a related field.', 'Strong Python skills (SQL optimization, caching, data structures).', 'Experience with Oracle database design and Linux administration.', 'Proficiency in Jira, Bitbucket, and Bash scripting.', 'Familiarity with CI/CD pipelines (Jenkins preferred).', 'Knowledge of Apache Spark, Django, Flask, and Nginx.', 'Strong problem-solving and communication skills.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-with-python-warszawa,oferta,1003881958?apid=c2d4b1b6-84a3-4668-a213-0e3a57233582&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 11 ---\n",
      "id: 20\n",
      "url: https://www.pracuj.pl/praca/data-architect-krakow-zablocie-43a,oferta,1003874729\n",
      "title: Data Architect\n",
      "work_location: None\n",
      "validity: ważna jeszcze 3 dnido 01 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Zabłocie 43A, Podgórze, KrakówKraków, małopolskie']\n",
      "technologies: ['ETL', 'ELT', 'Splunk', 'Kafka', 'Grafana', 'wewnątrz organizacji', 'u klienta', 'koncentrujesz się na jednym projekcie', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'koncentrujesz się na utrzymaniu kodu', 'agile', 'scrum', 'kanban']\n",
      "responsibilities: ['Poszukujemy doświadczonego specjalisty na stanowisko Data Architect w zespole odpowiedzialnym za rozwój infrastruktury przechowywania i ochrony danych. Twoim głównym zadaniem będzie budowanie, rozbudowywanie i optymalizowanie naszych platform danych, wspierając podejście oparte na danych w organizacji.', \"Współpraca z SME/Storage Architect'em nad strategicznym rozwiązaniem, które poprawi nasze możliwości monitorowania i alarmowania na wyższy poziom dojrzałości, umożliwiając pełną obserwowalność.\", 'Projektowanie infrastruktury umożliwiającej optymalne ekstrakcje, transformacje i ładowanie danych z różnych źródeł, w tym przy użyciu API, SQL i technologii big data.', \"Tworzenie i utrzymywanie optymalnej architektury pipeline'ów danych. Zbieranie dużych, złożonych zbiorów danych spełniających wymagania funkcjonalne i niefunkcjonalne.\", 'Optymalizacja procesów wewnętrznych, automatyzowanie procesów manualnych, poprawa wydajności dostarczania danych i skalowalności infrastruktury.', 'Współpraca z stakeholders przy rozwiązywaniu technicznych problemów związanych z danymi.', 'Śledzenie nowoczesnych narzędzi i technologii oraz kwestii związanych z bezpieczeństwem cybernetycznym, ochroną danych, prywatnością, zgodnością z regulacjami (np. GDPR).', 'Praca z zespołami BI i analitycznymi w dynamicznym środowisku oraz w zgodzie z metodologią Agile (Scrum, Kanban).']\n",
      "requirements: ['Doświadczenie w prowadzeniu i rozwoju platform danych, a także praktyczne umiejętności w poniższych obszarach:', 'Praca z danymi surowymi, strukturalnymi, półstrukturalnymi i niestrukturalnymi.', 'Łączenie dużych, niepołączonych zbiorów danych z wykorzystaniem odpowiednich narzędzi i frameworków.', \"Budowanie i optymalizowanie pipeline'ów ETL / ELT.\", 'Znajomość narzędzi takich jak Splunk, Kafka, Grafana będzie dodatkowym atutem.', 'Doświadczenie w pracy z systemami BI i zespołami analitycznymi.', 'Znajomość metodologii Agile, Scrum, Kanban.', 'Doświadczenie w pracy z systemami Continuous Integration, Continuous Delivery i Deployment przy użyciu CI/CD Pipelines.', 'Umiejętność pracy w zespole, współpracy z innymi inżynierami i architektami.', 'Umiejętności analityczne i rozwiązywania problemów.', 'Gotowość do nauki i zdobywania nowych umiejętności w zakresie produktów i usług związanych z danymi.', 'Mile widziane doświadczenie w pracy w branży bankowo - finansowej.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-architect-krakow-zablocie-43a,oferta,1003874729?apid=1e9cdb64-6425-4324-80b4-10103150c2eb&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 12 ---\n",
      "id: 21\n",
      "url: https://www.pracuj.pl/praca/senior-net-developer-ai-wroclaw-robotnicza-42,oferta,1003893596\n",
      "title: Senior .NET Developer (AI)\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 11 dnido 09 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['.NET', 'C#', 'Microsoft SQL Server', 'Elasticsearch', 'Microsoft Azure', 'Git', 'wewnątrz organizacji', 'u klienta', 'agile', 'scrum']\n",
      "responsibilities: ['Tworzenie kodu we współpracy z zespołem projektowym i klientem', 'Tworzenie pomysłów usprawniających projekt', 'Aktywny udział w analizowaniu, estymowaniu i planowaniu projektu', 'Aktywny udział w Community (wewnętrzne spotkania wymiany wiedzy dla zespołów technicznych)', 'Posługiwanie się językiem angielskim w codziennej komunikacji']\n",
      "requirements: ['Minimum 7 lat komercyjnego doświadczenia w tworzeniu nowoczesnych aplikacji', 'Zaawansowana znajomość technologii .NET i języka C#', 'Praktyczne doświadczenie w projektach wykorzystujących sztuczną inteligencję (AI)', 'Doświadczenie w pracy z SQL Server i Elasticsearch', 'Biegłość w wykorzystaniu chmury Microsoft Azure', 'Znajomość GIT i Git Flow, CI/CD z zaawansowanymi pipelinami,', 'Język angielski na poziomie min. B2/C1', 'Umiejętność rozwiązywania problemów i skupienie na szczegółach, aby diagnozować problemy, sugerować rozwiązania i podejmować decyzje w oparciu o wymagania', 'Zdolność do sprawnego wdrożenia się do pracy, pozwalająca na dostarczenie wersji w ciągu pierwszych kilku tygodni', 'Brak obaw przed kwestionowaniem statusu quo, jeśli oznacza to osiągnięcie lepszych wyników dla klientów', 'Doskonałe umiejętności komunikacji pisemnej i werbalnej oraz zdolność do prezentowania złożonych informacji technicznych oraz wskazywania zalet/konsekwencji i ryzyka w jasny i zwięzły sposób różnym odbiorcom', 'Zdolność do współpracy na wszystkich poziomach (zespoły produktowe, handlowe i danych), aby zapewnić, że tworzymy wartościowe produkty', 'Umiejętność bycia dobrym graczem zespołowym', 'Zdolność do szybkiej nauki i dostosowywania się do zmieniających się wymagań', 'Otwartość na udzielanie/otrzymywanie informacji zwrotnych', 'Proaktywność, koncentracja na codziennym doskonaleniu produktu/projektu/zespołu']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-net-developer-ai-wroclaw-robotnicza-42,oferta,1003893596?apid=a7dc505a-68f5-4952-99a3-8962f880eb24&oca=None&acv=0\n",
      "\n",
      "--- Job Record 13 ---\n",
      "id: 22\n",
      "url: https://www.pracuj.pl/praca/sas-data-scientist-wroclaw-legnicka-70,oferta,1003914900\n",
      "title: SAS Data Scientist\n",
      "work_location: Company locationLegnicka 70, Fabryczna, WrocławWrocław, Lower Silesia\n",
      "validity: valid for 22 daysto 20 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['SAS', 'SQL', 'Microsoft Excel', 'Microsoft Power BI', 'R', 'agile', 'scrum']\n",
      "responsibilities: ['Oversee monthly forecasting, ensuring accuracy and integration of various data sources', 'Generate and review reports to improve and validate forecasting methods', 'Collaborate with market demand planners to refine and adopt statistical forecasts', 'Conduct experiments to enhance forecast accuracy and stability', 'Present forecasting strategies and results to senior leadership with confidence', 'Drive continuous improvement in forecasting techniques, including statistical and machine learning methods', 'Share best practices and innovations through workshops with regional forecasting teams', 'Monitor key forecasting metrics and demand trends, adjusting models as needed', 'Provide on-demand analysis to support client management and demand planning']\n",
      "requirements: ['Proficient in time series forecasting techniques & predictive modelling', 'Ability to efficiently gather information and generate analysis to support successful supply chain management and business success', 'Well Versed with SAS Viya (SAS Studio, SAS Model Studio, SAS Environment Manager, SAS Visual Analytics etc.), including ability to write 4GL Macros, use  DATA steps, and PROC steps for data preparation and statistical modelling (e.g. PROC FOREST, PROC FORECAST, PROC HPFDIAGNOSE, PROC ESM, PROC REG, PROC ARIMA, PROC LOESS, PROC HPFRECONCILE, PROC SQL, PROC EXPAND, PROC FREQ, PROC MEANS, PROC UNIVARIATE etc)', 'Proficient with SQL (CTAS)', 'Proficient with Excel (Conditional Statements, Logical Operators, Lookup & Reference Functions, Text Functions, Date & Time Functions, Conditional Formatting, Data Validations, Pivot Tables, Visualizations etc.) and Git (branch, checkout, fetch, add, status, commit, push, merge, pull request creation & feature branch creation on Git provider UI, merge conflict resolution)', 'Presentation skills - i.e. ability to describe complex analytical problems to business audience']\n",
      "application_link: https://www.pracuj.pl/aplikuj/sas-data-scientist-wroclaw-legnicka-70,oferta,1003914900?apid=ac02de99-84c4-4b11-91a5-621663129d65&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 14 ---\n",
      "id: 23\n",
      "url: https://www.pracuj.pl/praca/data-scientist-warszawa,oferta,1003881850\n",
      "title: Data Scientist\n",
      "work_location: None\n",
      "validity: ważna jeszcze 7 dnido 05 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL', 'Python', 'u klienta', 'testy jednostkowe', 'wymiana wiedzy technicznej w firmie']\n",
      "responsibilities: ['Opracowywanie i realizacja złożonych analiz statystycznych, ekonometrycznych i uczenia maszynowego w zakresie efektywności procesowej.', 'Projektowanie i implementacja nowych Data Martów (ABT) oraz procesów raportowych na platformie Hurtowni Danych/Business Intelligence.', \"Rozwój i utrzymanie procesów HD, w tym budowanie Data Mart's pod procesy modelowania i scoringu.\", 'Budowa inteligentnych rozwiązań służących automatyzacji i optymalizacji procesów obsługowych.', 'Analiza potrzeb biznesowych, projektowanie i testowanie rozwiązań biznesowych.']\n",
      "requirements: ['Wykształcenie wyższe w zakresie data science, ekonometrii, statystyki, matematyki, fizyki lub pokrewnych obszarów oraz minimum 3-letnie doświadczenie na stanowisku Data Scientist.', 'Chęć dzielenia się wiedzą, promowanie najlepszych praktyk oraz bardzo dobre zdolności komunikacyjne i umiejętność dekompozycji problemu.', 'Doświadczenie w pracy z modelami ekonometrycznymi, klasyfikacyjnymi oraz rachunkiem prawdopodobieństwa.', 'Doświadczenie w tworzeniu produkcyjnych rozwiązań ML, testowaniu danych i jednostkowym testowaniu.', 'Biegłość w Pythonie i/lub 4GL oraz podstawowa znajomość relacyjnych baz danych (SQL).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-warszawa,oferta,1003881850?apid=d204d262-6eea-4014-b3c3-7dbf960151a2&oca=None&acv=0\n",
      "\n",
      "--- Job Record 15 ---\n",
      "id: 24\n",
      "url: https://www.pracuj.pl/praca/mlops-with-kubeflow-aws-sagemaker-warszawa-pulawska-2,oferta,1003897552\n",
      "title: MLOps with Kubeflow - AWS Sagemaker\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 14 dnido 12 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior), ekspert\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Kubernetes', 'Bash', 'AWS', 'Python', 'kubeflow', 'sagemaker', 'Data Science', 'u klienta', 'agile']\n",
      "responsibilities: ['Badanie i wdrażanie narzędzi, frameworków oraz platform MLOps na potrzeby projektów Data Science', 'Realizacja backlogu działań w celu podniesienia dojrzałości MLOps w organizacji', 'Proaktywne wprowadzanie nowoczesnego, zwinnego i zautomatyzowanego podejścia do Data Science', 'Prowadzenie wewnętrznych szkoleń i prezentacji na temat korzyści i zastosowania narzędzi MLOps']\n",
      "requirements: ['Doświadczenie w pracy z Kubernetes', 'Doświadczenie w operacjonalizacji projektów Data Science (MLOps) przy użyciu co najmniej jednego z popularnych frameworków lub platform (np. Kubeflow, AWS Sagemaker)', 'Dobra znajomość koncepcji ML i AI oraz praktyczne doświadczenie w rozwijaniu modeli ML', 'Biegłość w Pythonie zarówno w kontekście ML, jak i automatyzacji. Dobra znajomość Bash oraz narzędzi wiersza poleceń systemu Unix', \"Doświadczenie w implementacji pipeline'ów CI/CD/CT\", 'Doświadczenie w pracy z AWS']\n",
      "application_link: https://www.pracuj.pl/aplikuj/mlops-with-kubeflow-aws-sagemaker-warszawa-pulawska-2,oferta,1003897552?apid=0c4dde46-f630-4474-afb6-e52025a959fb&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 16 ---\n",
      "id: 25\n",
      "url: https://www.pracuj.pl/praca/marketing-cloud-developer-warszawa,oferta,1003932000\n",
      "title: Marketing Cloud Developer\n",
      "work_location: Company locationWarszawaWarszawa, Masovian\n",
      "validity: valid for a monthto 28 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['HTML', 'CSS', 'ampscript', 'SSJS', 'JS']\n",
      "responsibilities: ['Maintaining and optimizing existing Marketing Cloud solutions to ensure system stability and performance.', 'Troubleshooting and resolving issues related to email campaigns, automation, and integrations.', 'Updating and enhancing email templates and landing pages using HTML, CSS, Ampscript, and SSJS.', 'Monitoring and improving automation workflows in Journey Builder and Automation Studio.', 'Collaborating with business teams to understand requirements and provide technical support.', 'Conducting performance analysis on email deliverability, campaign engagement, and automation efficiency.', 'Providing technical documentation and knowledge sharing with the team.', 'Participating in meetings and reporting updates on system maintenance and improvements.']\n",
      "requirements: ['Very good HTML and CSS skills', 'Attention to details', 'Good knowledge of Marketing Cloud Engagement', 'Good knowledge of modules: Content Builder, Email Studio, Web Studio, Journey Builder, Contact Builder, Automation Studio.', 'Solid skills in: Ampscript, SSJS, JS, HTML, SQL', 'Ability to create email templates in HTML and CSS from scratch', 'Ability to conduct trainings with Business Users', 'Ability to translate technological concepts to business use cases', 'Ability to control meeting agenda and lead the meeting']\n",
      "application_link: https://www.pracuj.pl/aplikuj/marketing-cloud-developer-warszawa,oferta,1003932000?apid=046dacfe-4dc2-4cf9-b439-7a932ea49b8e&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 17 ---\n",
      "id: 26\n",
      "url: https://www.pracuj.pl/praca/starszy-programista-etl-krakow,oferta,1003931992\n",
      "title: Starszy Programista ETL\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze miesiącdo 28 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: None\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['SQL Server Integration Services', 'Pentaho Data Integration', 'Oracle', 'PL/SQL', 'DAX', 'MDX', 'scrum']\n",
      "responsibilities: ['Praca w środowisku bazodanowym Oracle (PL/SQL) oraz narzędziami Apache Hadoop;', 'Projektowanie i implementowanie procesów i transformacji ETL (MS SSIS lub PDI);', 'Projektowanie i implementowanie narzędzi, procesów, wzorców i metodyk wspierających procesy kontroli i zarządzania jakością danych (Data Quality);', 'Monitorowanie i analizowanie wydajności procesów ETL oraz proponowanie usprawnień;', 'Projektowanie, tworzenie i rozwój hurtowni danych;', 'Tworzenie rozwiązań Big Data z wykorzystaniem Apache Hadoop;', 'Aktywne uczestnictwo w testach procesów ETL;', 'Optymalizacja procesów ETL;', 'Dokumentowanie procesów ETL;', 'Obsługa incydentów związanych z przetwarzaniem danych.']\n",
      "requirements: ['Praktyczna znajomość projektowania oraz implementacji procesów ETL w oparciu o SQL Server Integration Services (SSIS) oraz Pentaho Data Integration (PDI);', 'Bardzo dobra znajomość relacyjnych baz danych, szczególnie Oracle, poparta minimum 5-letnim doświadczeniem;', 'Dobra znajomość projektowania oraz implementacji rozwiązań z wykorzystaniem pakietów SSAS, SSRS (oraz pozostałych z pakietu SSDT);', 'Dobra znajomość języka DAX i/lub MDX;', 'Umiejętność programowania w języku PL/SQL na poziomie zaawansowanym;', 'Umiejętność tworzenia oprogramowania od opisu algorytmu do implementacji rozwiązania;', 'Mile widziana znajomość rozwiązań Big Data z wykorzystaniem Apache Hadoop & Family;', 'Umiejętność analizy wymagań;', 'Bardzo dobra znajomość języka polskiego (swobodna komunikacja werbalna i pisemna).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/starszy-programista-etl-krakow,oferta,1003931992?apid=b0f75a78-a3f0-4054-bf8b-fa185200340b&oca=None&acv=0\n",
      "\n",
      "--- Job Record 18 ---\n",
      "id: 27\n",
      "url: https://www.pracuj.pl/praca/data-analyst-scientist-warszawa-pulawska-2,oferta,1003871939\n",
      "title: Data Analyst - Scientist\n",
      "work_location: Company locationPuławska 2, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for 2 daysto 28 February 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['Python', 'Blockchain', 'ML', \"at the client's site\"]\n",
      "responsibilities: ['Identify and analyze discontinuities and critical points within the state space of blockchain systems', 'Employ igraph libraries to analyze the topological properties of blockchain networks and using techniques like mincut to identify vulnerabilities and potential points of failure', 'Apply advanced simulation techniques, such as Multi-Level Monte Carlo, to accelerate simulations and improve accuracy', 'Leverage appropriate Python libraries to implement e?cient and scalable simulation workflows', 'Prepare clear and concise written reports detailing simulation methodologies, findings, and recommendations', 'E?ectively communicate complex technical concepts to both technical and non-technical audiences']\n",
      "requirements: ['Strong proficiency in Python programming and data analysis tools (e.g., NumPy, Pandas, Matplotlib)', 'Experience with simulation frameworks (e.g., SimPy, AnyLogic)', 'Solid understanding of blockchain technology and its underlying principles', 'Familiarity with statistical analysis and machine learning concepts']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-analyst-scientist-warszawa-pulawska-2,oferta,1003871939?apid=c8932040-65d7-4435-be53-202a3f075289&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 19 ---\n",
      "id: 28\n",
      "url: https://www.pracuj.pl/praca/senior-c%2b%2b-computer-vision-developer-pl-ai-startup-krakow-czysta-10,oferta,1003893530\n",
      "title: Senior C++ & Computer Vision developer | PL AI startup\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 11 dnido 09 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['C++', 'WinAPI', 'Computer Vision', 'DirectX', 'OpenGL', 'Vulcan', 'JavaScript', 'wewnątrz organizacji', 'koncentrujesz się na jednym projekcie', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'tworzysz kod \"od zera\"', 'koncentrujesz się na rozwoju produktu', 'koncentrujesz się na utrzymaniu kodu', 'kanban', 'frontend developer', 'fullstack developer', 'lider techniczny', 'devOps', 'tester manualny', 'product owner']\n",
      "responsibilities: ['Praca nad projektem badawczym z zakresu rozszerzonej rzeczywistości.', 'Rozwijanie aplikacji desktopowej (Windows) działającej w trybie kamery wirtualnej.', 'Implementacja modeli uczenia maszynowego.', 'Opracowywanie algorytmów Computer Vision.']\n",
      "requirements: ['Solidna, seniorska znajomość C++.', 'Doświadczenie w tworzeniu aplikacji Desktopowych pod Windows (WinAPI).', 'Doświadczenie w pracy z algorytmami Computer Vision.', 'Znajomość zagadnień związanych z optymalizacją.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-c%2b%2b-computer-vision-developer-pl-ai-startup-krakow-czysta-10,oferta,1003893530?apid=3885af34-aa81-4fac-b386-7edf690e8c1e&oca=None&acv=0\n",
      "\n",
      "--- Job Record 20 ---\n",
      "id: 29\n",
      "url: https://www.pracuj.pl/praca/ai-ml-business-analyst-krakow,oferta,1003926877\n",
      "title: AI ML Business Analyst\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior), expert\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KrakówKraków, Lesser Poland', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['TensorFlow', 'PyTorch', 'Hugging Face', 'Scikit-learn', 'Python', 'C++', 'JavaScript', 'HTML', 'Microsoft Azure', 'agile', 'scrum', 'industry-specific e-learning platforms']\n",
      "responsibilities: ['Managing the full lifecycle of AI-driven projects from inception to deployment', 'Collaborating with business stakeholders to define and prioritize AI product features', 'Monitoring and evaluating AI model performance, identifying areas for improvement', 'Staying up-to-date with advancements in GenAI, analytics, and Prompt Engineering', 'Participating in regular audits and reviews of AI-driven processes', 'Ensuring compliance with data privacy and security standards', 'Establishing and maintaining strong relationships with internal and external partners', 'Developing and executing comprehensive system test plans for AI platforms', 'Driving innovation by implementing AI solutions in regulated environments', 'Communicating key insights and recommendations to senior management']\n",
      "requirements: ['Strong background in mathematical disciplines such as computer science, statistics, physics, or engineering', 'Extensive experience in AI model development and lifecycle management', 'Proven expertise in GenAI, Large Language Models (LLM), and Natural Language Processing (NLP)', 'Familiarity with AI frameworks like TensorFlow, PyTorch, Hugging Face, and Scikit-learn', 'Proficiency in programming languages such as Python, C++, JavaScript, and HTML', 'Experience with cloud-based AI platforms, particularly Microsoft Azure', 'Strong problem-solving skills with a proactive, results-driven mindset', 'Excellent verbal and written communication skills for engaging with stakeholders', 'Ability to work in a highly regulated environment with strict compliance requirements', 'Experience in the financial sector with knowledge of banking products and processes']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-ml-business-analyst-krakow,oferta,1003926877?apid=3a7920f4-a998-4bc1-8eae-dec605c12202&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 21 ---\n",
      "id: 30\n",
      "url: https://www.pracuj.pl/praca/power-bi-data-analyst-krakow,oferta,1003926861\n",
      "title: Power BI Data Analyst\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KrakówKraków, Lesser Poland', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Microsoft Power BI', 'Python', 'SQL', 'Apache Spark', 'PySpark', 'Databricks', 'agile', 'scrum', 'industry-specific e-learning platforms']\n",
      "responsibilities: ['Creating interactive dashboards and reports using Power BI', 'Publishing and managing dashboards in Power BI Service Workspace', 'Conducting regular check-ins with stakeholders to ensure dashboards meet their needs', 'Troubleshooting dashboard refresh issues and ensuring continuous data updates', 'Conducting analytics on IT network data to identify trends, anomalies, and patterns', 'Enhancing datasets by preprocessing data using Databricks', 'Collaborating with data engineers to gather, clean, and prepare data for the Azure Data Lakehouse', 'Engaging with business analysts and subject matter experts to define reporting requirements', 'Providing demonstrations, guidance, and training on dashboard usage', 'Maintaining a strong focus on data quality and security compliance']\n",
      "requirements: ['Proficiency in Power BI Desktop and Power BI Service', 'Strong experience with Python and/or SQL', 'Ability to analyze large datasets and derive insights', 'Knowledge of IT network data analytics', 'Experience working with structured and unstructured data', 'Strong problem-solving and troubleshooting skills', 'Excellent communication and stakeholder management abilities', 'Understanding of data visualization best practices', 'Experience working in a cybersecurity or financial services environment', 'Ability to work collaboratively in a fast-paced team environment']\n",
      "application_link: https://www.pracuj.pl/aplikuj/power-bi-data-analyst-krakow,oferta,1003926861?apid=96f67c29-85e1-4bb2-9b82-e550c0d6b790&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 22 ---\n",
      "id: 31\n",
      "url: https://www.pracuj.pl/praca/inzynier-programista-ai-i-ml-krakow-wadowicka-8a,oferta,1003931937\n",
      "title: Inżynier / Programista AI i ML\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 28 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Wadowicka 8A, Podgórze, KrakówKraków, małopolskie']\n",
      "technologies: ['Python', 'R', '.NET', 'C#', 'TensorFlow', 'PyTorch', 'SQL']\n",
      "responsibilities: ['projektowanie, rozwijanie i wdrażanie modeli AI i ML w ekosystemie ERP enova365,', 'integracja rozwiązań AI z istniejącymi modułami systemu ERP,', 'współpraca z zespołem programistów modułowych w celu implementacji i optymalizacji rozwiązań AI,', 'monitorowanie i utrzymanie wdrożonych modeli AI, zapewniając ich efektywność i dokładność,', 'badania i rozwój nowych rozwiązań związanych z AI i ML.']\n",
      "requirements: ['pasja do AI/ML – poszukujemy osób, które żyją i oddychają sztuczną inteligencją oraz aktywnie poszukują jej zastosowań w życiu codziennym,', 'doświadczenie w programowaniu i projektowaniu systemów AI/ML,', 'kreatywność i umiejętność myślenia poza schematami w celu znajdowania innowacyjnych rozwiązań,', 'zdolność do współpracy i dzielenia się wiedzą z zespołem.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/inzynier-programista-ai-i-ml-krakow-wadowicka-8a,oferta,1003931937?apid=655421cb-91d0-4937-8248-8b181e9c4e65&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 23 ---\n",
      "id: 32\n",
      "url: https://www.pracuj.pl/praca/senior-ml-engineer-warszawa-plac-europejski-1,oferta,1003900181\n",
      "title: Senior ML Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 15 dnido 13 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['plac Europejski 1, Wola, WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatówwakaty: 3']\n",
      "technologies: ['Python', 'AWS', 'GitHub Actions', 'Airflow', 'TensorFlow', 'PyTorch', 'Spark ML', 'Docker', 'Kubernetes', 'SQL', 'Github Actions', 'Snowflake Data Cloud', 'Kafka', 'Spark', 'Flink', 'MapRreduce', 'scikit-learn', 'Jupyter', 'wewnątrz organizacji', 'u klienta', 'koncentrujesz się na jednym projekcie', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'koncentrujesz się na utrzymaniu kodu', 'agile', 'scrum', 'data scientist']\n",
      "responsibilities: ['Opracowywanie, testowanie, wdrażanie i utrzymanie danych oraz skalowalnych produktów i potoków uczenia maszynowego wspierających produkty ML.', 'Walidacja wydajności modelu na nieznanych danych, zapewniając, że dobrze się generalizuje i nie nadmiernie dopasowuje do danych treningowych.', 'Projektowanie i rozwój nowej generacji platformy uczenia maszynowego, która będzie wspierać tysiące potoków szkoleniowych modeli jednocześnie i miliardy codziennych prognoz wsadowych.', 'Eksperymentowanie z nowymi platformami ML oraz tworzenie szybkich prototypów / PoC.', 'Usprawnianie wdrażania modeli, testów jednostkowych, testów integracyjnych i testów obciążeniowych oraz zapewnianie jakości inżynieryjnej.']\n",
      "requirements: ['Masz min. 3-letnie doświadczenie w pracy na stanowisku ML Engineera.', 'Posiadasz min. 7-letnie doświadczenie w branży IT (preferowane role: Python Developer, Data Scientist)', 'Bardzo dobrze znasz Pythona.', 'Pracowałeś z popularnymi bibliotekami ML (np. TensorFlow, PyTorch, Spark ML).', 'Masz doświadczenie w pracy z rozwiązaniami chmurowymi (preferowany AWS).', 'Znasz narzędzia CI/CD (np. GitHub Actions, Airflow).', 'Masz doświadczenie z ETL lub big data (np. MapReduce, Spark, Flink, Kafka, Docker, Kubernetes).', 'Posiadasz umiejętność optymalizacji zapytań SQL.', 'Bardzo dobra znajomość języka angielskiego.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-ml-engineer-warszawa-plac-europejski-1,oferta,1003900181?apid=1c5d2834-a9aa-40a1-a77c-c56c62e9b06b&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 24 ---\n",
      "id: 33\n",
      "url: https://www.pracuj.pl/praca/data-analyst-m-f-d-szczecin,oferta,1003890333\n",
      "title: Data Analyst (m/f/d)\n",
      "work_location: None\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['SzczecinSzczecin, West Pomeranian', 'contract of employment']\n",
      "technologies: ['SQL', 'Python', 'Git', 'R', 'Jira', 'Confluence']\n",
      "responsibilities: ['Develop SQL scripts for key metrics.', 'Transform data findings into insights; use data visualization tools.', 'Design and optimize reports, dashboards, use Power BI.', 'Analyze data using Python (pandas, BeautifulSoup).', 'Calculate metrics for statistical significance.', 'Collaborate with business, analysts, data scientists, data engineers.', 'Present results to business clients.', 'Use Jira/Confluence for project management.']\n",
      "requirements: ['Proficient in Python, SQL (ClickHouse preferred).', 'Skilled in Power BI/BI tools.', 'Intermediate statistics knowledge.', 'Handle large datasets effectively.', 'Experience with ClickHouse, certifications, e-commerce/automotive, R, Git, PySpark, ML.', 'At least a B2 level proficiency with English and Russian', \"3 years' experience in data and product analysis.\", 'Exceptional communication abilities.', 'Proficient in analyzing extensive datasets and deriving insights and suggestions.', 'Capable of handling large data volumes effectively.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-analyst-m-f-d-szczecin,oferta,1003890333?apid=89234b4f-cfad-446f-b17f-80d0c0dc3635&oca=None&acv=0\n",
      "\n",
      "--- Job Record 25 ---\n",
      "id: 34\n",
      "url: https://www.pracuj.pl/praca/stanowisko-ds-administracji-systemami-big-data-warszawa,oferta,1003874486\n",
      "title: Stanowisko ds. Administracji Systemami Big Data\n",
      "work_location: None\n",
      "validity: ważna jeszcze 3 dnido 01 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Linux', 'Red Hat', 'Oracle']\n",
      "responsibilities: ['Administracja oraz utrzymanie oprogramowania BigData/hadoop;', 'Wykonywanie instalacji i re-instalacji oprogramowania BigData/hadoop / działających;', 'Rozwiązywanie problemów technicznych, wsparcie użytkowników w celu zapewnienia wysokiej dostępności świadczonych usług informatycznych;', 'Współpraca z serwisami firm zewnętrznych w celu utrzymania w pełnej sprawności oprogramowania BigData/hadoop;', 'Nadzorowanie prawidłowej realizacji umów serwisowych i utrzymanie ciągłości wsparcia technicznego dla oprogramowania BigData/hadoop;', 'Uczestnictwo w uruchamianiu i realizacji postępowań o zamówienie publiczne w celu wyłonienia dostawcy usługi informatycznej lub oprogramowania dla potrzeb BigData/hadoop.']\n",
      "requirements: ['Wykształcenie: co najmniej średnie;', 'Doświadczenie zawodowe: minimum 2 lata doświadczenia;', 'Znajomość systemów operacyjnych Linux (RedHat, SUSE, Oracle Linux);', 'Znajomość funkcjonowania sieci komputerowych LAN, WAN;', 'Znajomość funkcjonowania systemów klastrowych;', 'Znajomość języka angielskiego na poziomie umożliwiającym czytanie dokumentacji technicznej.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/stanowisko-ds-administracji-systemami-big-data-warszawa,oferta,1003874486?apid=b8031f95-c3a8-4940-8803-bd5301c18e65&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 26 ---\n",
      "id: 35\n",
      "url: https://www.pracuj.pl/praca/innovation-lead-ai-driven-solutions-wroclaw-swietego-mikolaja-7,oferta,1003893426\n",
      "title: Innovation Lead (AI-Driven Solutions)\n",
      "work_location: None\n",
      "validity: valid for 11 daysto 09 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: None\n",
      "additional_info: ['Świętego Mikołaja 7, Stare Miasto, WrocławWrocław, Lower Silesia', 'manager / supervisor']\n",
      "technologies: ['AI', 'Data Analytics', 'SEO', 'in house', 'you develop several projects simultaneously', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product']\n",
      "responsibilities: ['Keeping abreast of trends in AI and emerging technologies – Collaborating with the team to develop and launch innovative services that can support customers in achieving their goals,', 'Supporting clients in identifying opportunities to use AI to optimize processes and generate new value,', 'Proactively identifying potential customers and analyzing their needs,', 'Building relationships with clients and proposing new and creative solutions based on a deep understanding of their business and aligned with their strategy.']\n",
      "requirements: ['You follow trends in digital technologies and translate them into specific recommendations for clients,', 'You are able to operate at the intersection of sales and innovation, delivering solutions that realistically support clients’ business,', 'You have excellent communication skills in Polish and English,', 'You are proactive, setting and consistently achieving goals,', 'You are bold in putting ideas into practice, looking for unconventional solutions,', 'You are able to listen, ask the right questions and translate customer needs into effective solutions,', 'You have experience working with corporate clients in the areas of AI, SEO, process automation and data analytics.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/innovation-lead-ai-driven-solutions-wroclaw-swietego-mikolaja-7,oferta,1003893426?apid=54cf05b0-17c5-4b7f-ad4a-4b8ac1ea4ddc&oca=None&acv=0\n",
      "\n",
      "--- Job Record 27 ---\n",
      "id: 36\n",
      "url: https://www.pracuj.pl/praca/database-administrator-wroclaw,oferta,1003884581\n",
      "title: Database Administrator\n",
      "work_location: Company locationWrocławWrocław, Lower Silesia\n",
      "validity: valid for 8 daysto 06 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['Oracle', 'Linux', 'Windows Server', 'PowerShell', 'Bash', 'SQL', 'Ansible', 'Kubernetes', 'agile', 'scrum']\n",
      "responsibilities: ['IT infrastructure support and Oracle RDBMS database administration.', 'Performing tasks related to data masking, data virtualization, and synthetic data generation.', 'Managing Linux and Windows operating systems in the context of database support.', 'Creating and maintaining scripts to automate administrative tasks (PowerShell, Bash).', 'MS SQL administration (optional).', 'Implementing and maintaining automation using Ansible.', 'Participating in projects related to containerization and Kubernetes environment management (nice to have).', 'Monitoring performance and ensuring the security of database environments.']\n",
      "requirements: ['Experience in Oracle RDBMS database administration.', 'Strong knowledge of Linux and Windows systems in IT administration contexts.', 'Proficiency in PowerShell and Bash for process automation.', 'Knowledge of data masking and data virtualization concepts.', 'Experience in MS SQL administration (optional).', 'Familiarity with automation tools (Ansible).', 'Analytical thinking and IT process optimization skills.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/database-administrator-wroclaw,oferta,1003884581?apid=99c5a81c-e601-4ece-aca4-42dddcc57045&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 28 ---\n",
      "id: 37\n",
      "url: https://www.pracuj.pl/praca/data-engineer-analyst-warszawa-dluga-29,oferta,1003900086\n",
      "title: Data Engineer / Analyst\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 15 dnido 13 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['AWS', 'Snowflake', 'Python', 'Airflow', 'Tableau', 'Thoughtspot', 'u klienta', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu']\n",
      "responsibilities: ['Praca przy wizualizacji i analizie danych', 'Projektowanie i wdrażanie rozwiązań do zarządzania danymi', 'Tworzenie i konfigurowanie data pipelinów', 'Budowanie rozwiązań danych w celu tworzenia produktów danych', 'Uczestniczenie w okazjonalnych spotkaniach online po godzinie 17']\n",
      "requirements: ['Co najmniej 4 lata doświadczenia w pracy na podobnych stanowiskach', 'Doświadczenie w budowaniu złożonych rozwiązań przy użyciu technologii chmurowych (AWS, Snowflake)', 'Umiejętność tworzenia data pipelinów przy użyciu Python, Airflow', 'Znajomość narzędzi do orkiestracji przepływu pracy i zarządzania danymi', 'Doświadczenie w pracy z narzędziami do wizualizacji danych, takimi jak Tableau (preferowane) lub Thoughtspot', 'Język angielski (min B2/C1)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-analyst-warszawa-dluga-29,oferta,1003900086?apid=0b8e1dbb-b9c4-4033-9687-33da4907ac9c&oca=None&acv=0\n",
      "\n",
      "--- Job Record 29 ---\n",
      "id: 38\n",
      "url: https://www.pracuj.pl/praca/inzynier-big-data-warszawa-zubra-1,oferta,1003900087\n",
      "title: Inżynier Big Data\n",
      "work_location: None\n",
      "validity: ważna jeszcze 15 dnido 13 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Żubra 1, Wola, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Python', 'pyspark', 'SQL', 'Jira']\n",
      "responsibilities: ['Analiza i opracowywanie wymagań systemowych w oparciu o wymagania biznesowe i pryncypia integracji systemów Big Data', 'Analiza dużych wolumenów danych, w tym użycie metod sztucznej inteligencji (AI/Machine Learning)', 'Projektowanie zbiorów danych, partycjonowanie danych dla potrzeb on-premise i chmurowych (rozwiązania wsadowe/batch i strumieniowe/online)', 'Tworzenie specyfikacji wymagań funkcjonalnych i niefunkcjonalnych dla rozwiązań IT z obszaru Big Data oraz odpowiadać za ich jakość i spójność', 'Identyfikacja oraz rozwiązywanie problemów pojawiających się w bieżącej pracy systemów Big Data']\n",
      "requirements: ['Posiadasz ukończone studia wyższe (preferowane kierunki: informatyka, telekomunikacja, analiza danych, Big Data i inne pokrewne)', 'Masz znajomość języków programowania: Python, pyspark, SQL, mile widziana Scala', 'Masz znajomość pisania skryptów powłoki (shell scripts)', 'Masz doświadczenie w zakresie implementacji i integracji systemów IT', 'Znasz narzędzia: Jira, Wiki, repozytoria kodu (Git/GitHub/Gerrit itp.), IntelliJ/PyCharm itp.', 'Znasz systemy operacyjne (preferowany Linux)', 'Jesteś zainteresowany rozwojem kompetencji chmurowych (Cloud)', 'Posiadasz umiejętności interpersonalne (chęć rozwoju, zaangażowanie, inicjatywę, zdolność uczenia się, umiejętności komunikacyjne)', 'Znasz język angielski na poziomie B1', 'Cechuje Cię samodzielność w działaniu i dobra organizację pracy', 'Umiesz pracować w zespole']\n",
      "application_link: https://www.pracuj.pl/aplikuj/inzynier-big-data-warszawa-zubra-1,oferta,1003900087?apid=d001be44-70d0-47a3-a58d-40b959de40b5&oca=None&acv=0\n",
      "\n",
      "--- Job Record 30 ---\n",
      "id: 39\n",
      "url: https://www.pracuj.pl/praca/data-engineer-databricks-warszawa-opatowska-2,oferta,1003890238\n",
      "title: Data Engineer (Databricks)\n",
      "work_location: None\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: None\n",
      "additional_info: ['Opatowska 2, Bielany, WarszawaWarszawa, Masovian']\n",
      "technologies: ['Databricks', 'Python', 'SQL', 'Microsoft Azure', 'Hadoop', 'Kafka', 'Apache Flink', 'Apache Hive']\n",
      "responsibilities: ['Design, build, and optimize scalable data pipelines in Databricks platform using SQL/Python/Spark', 'Developing existing projects in the Microsoft Azure environment to gain valuable insights for the business.', 'Ensuring that data is modelled and processed according to the architecture and both functional and non-functional requirements', 'Planning and implementing processing pipelines for structured and unstructured data (e.g. video and images).', 'Working to automate and optimize internal processes in Azure.', 'Collaborating with cross-functional and international teams both internally and externally.']\n",
      "requirements: [\"Minimum 3 years' experience working with Databricks platform (Delta Lake, Workflows/Jobs, DLT, Unity Catalog)\", 'Strong experience with programming in Python', 'Solid understanding of SQL and relational databases.', 'Knowledge of Data Warehouse, Business Intelligence and ETL/ELT data processing', 'Familiarity with Medallion Architecture', 'Very good knowledge of English (particular emphasis on written English)', 'Proactive approach to tasks, problem-solving attitude and critical thinking skills.', 'Flexibility, independence and responsibility for assigned tasks.', 'A constant desire to improve your skills and learn new technologies.', 'Knowledge of Azure cloud components for data storage and processing: Azure Data Lake Storage, Azure Event Hubs, Azure Data Factory, SQL Database, Synapse, Fabric', 'Experience with other big data technologies such as Hadoop, Hive, Kafka, and Flink would be an asset.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-databricks-warszawa-opatowska-2,oferta,1003890238?apid=9adcef3e-c423-4ea4-b486-d5e3230cce00&oca=None&acv=0\n",
      "\n",
      "--- Job Record 31 ---\n",
      "id: 40\n",
      "url: https://www.pracuj.pl/praca/lead-data-engineer-warszawa-opatowska-2,oferta,1003890231\n",
      "title: Lead Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: None\n",
      "additional_info: ['Opatowska 2, Bielany, WarszawaWarszawa, Masovian']\n",
      "technologies: ['AWS', 'Microsoft Azure', 'Databricks', 'Python', 'SQL']\n",
      "responsibilities: ['Team Leadership & Development – Leading a dynamic team of Data Engineers by conducting regular 1:1 meetings, mentoring team members, planning development paths, setting and reviewing individual goals, and managing project assignments. Supporting recruitment, onboarding, and fostering a knowledge-sharing culture within the team to enhance competencies.', 'Technical Expertise & Contributions – Ensuring the high quality of team-delivered solutions by actively contributing as a Solution Architect on selected projects. Promoting best practices in Data Engineering, participating in internal and external expert meetings, and strengthening client relationships. Supporting pre-sales and sales efforts, including gathering customer requirements, preparing work estimations, and assisting in proposal creation.', 'Monitoring the market for interesting solutions in the Data Engineering area,', 'Directly report and actively collaborate with a board member (Chief Delivery Officer) on the strategy for the Data Engineering team (development plans, team offerings/specializations, technology partnerships, R&D initiatives, ...),', 'Main technology areas: data services in Azure/AWS cloud and Databricks platform; competency areas: SQL, Python, ETL/ELT, QA, Data Governance.']\n",
      "requirements: ['At least 5 years of experience managing professional teams (including groups of more than 10 people) as a line manager or project supervisor,', 'More than 3 years of experience in managing teams in the area of data engineering/data warehousing,', 'Very good knowledge and ability to apply team management methods in practice,', 'At least 7 years of hands-on experience delivering projects focused on the design and development of Business Intelligence, Advanced Analytics, ETL/ELT, Data Warehousing, Data Lakes, and Big Data solutions (in roles such as Project Manager, Solution Architect, Data Engineer, or BI Consultant),', 'More than 3 years of experience in building and implementing cloud services, in particular, discernment and experience in data services offered by Azure and/or AWS platforms,', 'Understanding of Data Governance (including Data Quality, Master Data Management, Data Catalog, Data Lineage, Data Lifecycle Management, ...),', 'Open-minded, motivated, and curious, with a readiness to learn cutting-edge technologies and integrate them into hands-on projects,', 'Proficiency in English at a minimum C1 level,', 'Regular presence in the Warsaw office (at least three times per month, ideally more frequently).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/lead-data-engineer-warszawa-opatowska-2,oferta,1003890231?apid=5a84d3b1-6927-495d-bfec-644da9cd8d6a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 32 ---\n",
      "id: 41\n",
      "url: https://www.pracuj.pl/praca/data-engineer-with-aws-glue-gdynia,oferta,1003897289\n",
      "title: Data Engineer with AWS Glue\n",
      "work_location: Company locationGdyniaGdynia, Pomeranian\n",
      "validity: valid for 14 daysto 12 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'More than one vacancy']\n",
      "technologies: ['AWS Glue', 'Python', 'PySpark', 'Iceberg', 'in house', 'agile']\n",
      "responsibilities: ['Designing, building, and maintaining robust, scalable and efficient ETL pipelines using Python and Spark.', 'Willingness to work from the office (one day per week or once a month for a few days).', 'Developing workflows leveraging AWS services such as EMR Serverless, Glue, Glue Data Catalog, Lambda and S3.', 'Implementing data quality frameworks to ensure reliable data processing.', 'Optimizing existing workflows and driving migration to a modern data lakehouse architecture, integrating Apache Iceberg.', 'Monitoring system performance and ensuring data reliability through proactive optimizations.']\n",
      "requirements: ['At least 5 years of practice in related roles.', 'Expertise in Python and Spark, with proven experience in designing and implementing data workflows.', 'Hands-on experience with AWS EMR Serverless, Glue, Glue Data Catalog, Lambda, S3, and EMR.', 'Strong understanding of data quality frameworks, governance practices and scalable architectures.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-with-aws-glue-gdynia,oferta,1003897289?apid=1f74699f-538a-4afa-8f03-dbaaafd899d4&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 33 ---\n",
      "id: 42\n",
      "url: https://www.pracuj.pl/praca/machine-learning-engineer-wroclaw-strzegomska-2,oferta,1003931862\n",
      "title: Machine Learning Engineer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze miesiącdo 28 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Python', 'SQL', 'Docker', 'Kubernetes', 'wewnątrz organizacji']\n",
      "responsibilities: ['Projektowanie i implementacja modeli uczenia maszynowego', 'Rozwój i utrzymanie backendu dla aplikacji wykorzystujących ML', \"Tworzenie i optymalizacja pipeline'ów danych\", 'Wdrażanie rozwiązań opartych na Large Language Models (LLM), w tym praca z produktami OpenAI', 'Integracja z bazami wektorowymi i zarządzanie danymi za pomocą SQL/noSQL']\n",
      "requirements: ['Masz doświadczenie w Data Science i uczeniu maszynowym, zwłaszcza w LLM (large language models), NLP, OpenAI, huggingface', 'Znasz Python (z Pandas) na zaawansowanym poziomie', 'Znasz biegle Pythona i biblioteki ML (np. Transformers, PyTorch, TensorFlow)', 'Znasz relacyjne (SQL) i nierelacyjne baz danych (noSQL) oraz bazy wektorowe', 'Masz doświadczenie w web developmencie backendowym (Flask, FastAPI, Django)', 'Posiadasz praktyczną wiedzę o Dockerze, Kubernetesie i MLOps', 'Znasz język angielski na poziomie przynajmniej B2']\n",
      "application_link: https://www.pracuj.pl/aplikuj/machine-learning-engineer-wroclaw-strzegomska-2,oferta,1003931862?apid=6f46def9-9e1e-4dc7-9ad8-2d91eb8110b9&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 34 ---\n",
      "id: 43\n",
      "url: https://www.pracuj.pl/praca/ai-engineer-wroclaw-marszalka-jozefa-pilsudskiego-101,oferta,1003917451\n",
      "title: AI Engineer\n",
      "work_location: None\n",
      "validity: valid for 23 daysto 21 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Marszałka Józefa Piłsudskiego 101, Stare Miasto, WrocławWrocław, Lower Silesia', 'contract of employment']\n",
      "technologies: ['Python', 'PyTorch', 'TensorFlow']\n",
      "responsibilities: ['Design and develop AI-driven agent to enhance Android experiences using LLM, machine learning, or other AI technologies', 'Stay at the forefront of AI research by exploring and experimenting with new approaches in AI', 'Evaluate new tools, frameworks, and methodologies, ensuring that the team uses the best technology available', 'Plan, execute, and analyze complex research experiments', 'Collaborate with cross-functional teams to translate research outcomes into practical applications']\n",
      "requirements: ['Diploma in Computer Science, Artificial Intelligence, or a closely related field', 'Demonstrated expertise in programming, with proficiency in Python', 'Background in machine learning frameworks such as PyTorch or TensorFlow', 'A self-starter with the ability to work independently while also guiding and mentoring', 'Minimum 2 years of experience in research or a comparable role, preferably in an industry or academic setting', 'Excellent problem-solving skills with a track record of innovative solutions', 'Strong communication and interpersonal skills, with the ability to work effectively in a diverse, international team environment', 'Experience with Android software development and Android framework is a plus']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-engineer-wroclaw-marszalka-jozefa-pilsudskiego-101,oferta,1003917451?apid=ac57864a-0dbc-4c87-8a9f-ba944cd5e7a1&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 35 ---\n",
      "id: 44\n",
      "url: https://www.pracuj.pl/praca/data-engineer-warszawa,oferta,1003893321\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 11 daysto 09 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'More than one vacancyvacancies: 4']\n",
      "technologies: ['Python', 'Microsoft Azure', 'Azure DevOps', 'Azure Data Factory', 'Azure Databricks', 'Apache Spark', 'Spark SQL', 'in house', 'technical knowledge exchange within the company']\n",
      "responsibilities: ['Collaborate with cross-functional teams to design, develop and maintain data pipelines and analytics solutions.', 'Design and build a foundational platform for a modern data lake architecture, optimizing it for scalability, flexibility, and performance.', 'Develop automated test to ensure data accuracy and quality.', 'You will assist with planning and maintaining the Azure architectural runway and pipeline for multiple products, ensuring their stability and efficient operation.', 'Continuously secure improvement that can make developers on the platform work even more efficiently and act as a sparring partner on use of Azure services for the organisation.', 'Leverage your expertise in cloud development to design and implement innovative digital solutions focused on delivering business insights and patient care in real time.', 'Overall, our goal is to improve the clinical experience for patients, doctors and nurses world-wide, and your role will support this journey.']\n",
      "requirements: ['We are seeking a candidate with an educational background in Computer Science and Software Development, as well as experience in some of the following areas:', 'Strong proficiency in Python programming', 'Extensive experience with Azure, including Azure Data Factory and Azure Databricks, and a deep understanding of Azure architecture and services', 'Experience in using Spark, including Spark SQL and understanding of how to optimize Spark performance.', 'Automated unit testing and code quality inspection', 'CI/CD Pipelines using Azure DevOps (or similar)', 'Working in pharma domain or other regulated area is considered an advantage', 'Good English knowledge']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-warszawa,oferta,1003893321?apid=7ef3f97e-976f-4e6c-9944-d7bfddb486ec&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 36 ---\n",
      "id: 45\n",
      "url: https://www.pracuj.pl/praca/senior-technology-specialist-software-engineer-krakow-aleja-jana-pawla-ii-43a,oferta,1003931839\n",
      "title: Senior Technology Specialist - Software Engineer\n",
      "work_location: None\n",
      "validity: valid for a monthto 28 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['aleja Jana Pawła II 43a, Czyżyny, KrakówKraków, Lesser Poland', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['AWS', 'Git', 'Terraform', 'Kubernetes']\n",
      "responsibilities: ['collaborating closely with cross-functional teams to design, develop, and maintain high-performing applications', 'writing clean, efficient, and maintainable code to meet project requirements', 'Iidentifying and promptly resolve software issues to ensure continuous operation', 'participating in thorough code reviews and contribute to refining coding standards for improved efficiency', 'staying abreast of the latest methodologies and emerging technologies', 'contributing to projects driving real-time, data-centric initiatives forward', 'advocating for a culture of process and data quality within development teams', \"gathering requirements and assist in building roadmaps and architectures to support the Product Team's objectives\", 'ensuring solution quality and proactively address technical debt to ensure long-term sustainability and scalability', 'following agile development methodologies']\n",
      "requirements: ['6-8 years of experience with modern data architectures and relevant technologies, desirable strong experience in GO programming language or extensive experience in any contemporary programming language is welcome', 'strong problem-solving skills, particularly in Golang development, enabling you to effectively tackle complex challenges', 'decision-making abilities to balance technical feasibility, business requirements, and resource constraints', 'proven ability to communicate effectively with stakeholders at various levels, explaining technical concepts and aligning on strategic objectives', 'experience with AWS or other public cloud providers', 'expertise in data modelling, design patterns, and building highly scalable and secure analytical solutions', 'strong experience in Relational Databases', 'proven experience in code testing and building high quality software', 'ability to work independently and Commitment to taking ownership of your work']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-technology-specialist-software-engineer-krakow-aleja-jana-pawla-ii-43a,oferta,1003931839?apid=4743631d-26aa-41ba-9c14-8714283e6e90&oca=None&acv=0\n",
      "\n",
      "--- Job Record 37 ---\n",
      "id: 46\n",
      "url: https://www.pracuj.pl/praca/advanced-data-analytics-manager-warszawa-adama-branickiego-17,oferta,1003900044\n",
      "title: Advanced Data Analytics Manager\n",
      "work_location: None\n",
      "validity: ważna jeszcze 15 dnido 13 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: None\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Adama Branickiego 17, Wilanów, WarszawaWarszawa, mazowieckie', 'menedżer']\n",
      "technologies: ['Microsoft Power BI', 'SQL', 'wewnątrz organizacji']\n",
      "responsibilities: ['Zarządzanie zespołem Big Data, koordynacja działań, motywowanie oraz delegowanie zadań.', 'Nadzór nad przestrzenią Maczfit Official (Snowflake) – tworzenie koncepcji nowych widoków, utrzymywanie dokumentacji, ustalanie standardów współpracy z BI DEV', 'Przygotowywanie analiz (transformacja, modelowanie i wizualizacja informacji pochodzących z dużych zbiorów danych) na potrzeby działów biznesowych oraz kluczowych projektów, pozwalających na monitorowanie inicjatyw i efektywne komunikowanie postępów', 'Projektowanie raportów i dashboardów, w oparciu o zasady efektywnej wizualizacji danych oraz specyficzne potrzeby biznesowe', 'Wykorzystanie języka programowania Python do tworzenia skryptów analitycznych i automatyzacji procesów', 'Koordynacja projektów standaryzacji oraz automatyzacji procesów analitycznych – w szczególności w zakresie optymalizacji przygotowania danych, kalkulacji wskaźników i najlepszych praktyk prezentacji informacji zarządczej', 'Współpraca z interesariuszami w zakresie wyznaczenia i zastosowania krótko i długo-terminowych KPI spójnych z finansowymi celami firmy i strategią', 'Współpraca przy tworzeniu strategii rozwoju narzędzi analitycznych/BI w organizacji']\n",
      "requirements: ['Doświadczony analityk, który lubi wgłębić się w dane, ustrukturyzować problemy i przeprowadzać analizy przekładające się na biznesowe rekomendacje (doświadczenie w Programach lojalnościowych/ Retail/ eCommerce/ FMCG mile widziane)', 'Minimum 3 lata doświadczenia na podobnym stanowisku', 'Wykształcenie wyższe (preferowane ekonomiczne, informatyczne lub pokrewne)', 'Biegła znajomość MS Excel (mile widziana znajomość PowerPivot, VBA, DAX, PowerQuery)', 'Doświadczenie w zakresie projektowania raportów i dashboardów przy użyciu nowoczesnych narzędzi BI (znajomość MS PowerBI)', 'Doświadczenie w zakresie transformacji dużych zbiorów danych oraz pracy z hurtowniami danych (dobra znajomość języka SQL)', 'Silna orientacja biznesowa i koncentracja na wyznaczonych celach', 'Umiejętności komunikacyjne, pozwalające na przełożenie potrzeb biznesowych na konkretne rozwiązania analityczne', 'Samodzielności i kreatywności w poszukaniu rozwiązań problemów biznesowych', 'Zaangażowanie i dobra organizacja pracy własnej; umiejętność budowy solidnych relacji z interesariuszami i członkami zespołu']\n",
      "application_link: https://www.pracuj.pl/aplikuj/advanced-data-analytics-manager-warszawa-adama-branickiego-17,oferta,1003900044?apid=941e4aae-f813-406a-ab0c-d4a285f94c77&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 38 ---\n",
      "id: 47\n",
      "url: https://www.pracuj.pl/praca/principal-lead-data-science-wroclaw-powstancow-slaskich-9,oferta,1003893287\n",
      "title: Principal Lead Data Science\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 11 dnido 09 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Python', 'SQL', 'Google Cloud Platform', 'BigQuery', 'Vertex AI', 'Azure', 'Docker', 'Kubernetes', 'Spark', 'BPMN', 'Bash', 'R']\n",
      "responsibilities: ['aktywny udział w budowie nowego ekosystemu technologicznego w domenie finansowej,', 'tworzenie i rozwijanie modeli statystycznych oraz algorytmów ML na platformach GCP (BigQuery, Vertex AI) i/lub Azure (Azure ML, Databricks),', 'projektowanie i wdrażanie rozwiązań AI oraz Feature Store w środowisku chmurowym,', 'budowa, monitorowanie i zarządzanie procesami ML w oparciu o MLflow lub Kubeflow,', 'tworzenie pipeline’ów ETL oraz integracja rozwiązań ML z innymi systemami,', 'tworzenie i zarządzanie REST API.']\n",
      "requirements: ['posiadasz min. 5 lat doświadczenia w Data Science i ML,', 'pracujesz w Pythonie, R (mile widziane) oraz SQL,', 'znasz techniki modelowania statystycznego i algorytmy ML (xgboost, sieci neuronowe, regresja logistyczna, SVM, itp.)', 'masz doświadczenie w pracy z GCP (BigQuery, Vertex AI) lub Azure (Azure ML, Databricks)', 'wiesz, jak tworzyć pipeline’y ML oraz zarządzać ich cyklem życia,', 'znasz technologie Docker/Kubernetes, Spark oraz rozwiązania BPMN,', 'masz doświadczenie w wersjonowaniu kodu, CI/CD i pisaniu skryptów w bash.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/principal-lead-data-science-wroclaw-powstancow-slaskich-9,oferta,1003893287?apid=a5eccfb8-ce94-4584-bc1c-749ebc906fe8&oca=None&acv=0\n",
      "\n",
      "--- Job Record 39 ---\n",
      "id: 48\n",
      "url: https://www.pracuj.pl/praca/t-sql-engineer-warszawa-aleja-niepodleglosci-18,oferta,1003897237\n",
      "title: T-SQL Engineer\n",
      "work_location: Company locationAleja Niepodległości 18, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for 14 daysto 12 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['T-SQL', 'PL/SQL', 'Oracle', 'in house', 'agile']\n",
      "responsibilities: ['Optimizing existing code and performance in SQL.', 'Working with recursion and stored procedures.', 'Writing and optimizing T-SQL queries, stored procedures, functions and views to support efficient data retrieval and processing.', 'Monitoring and tuning database performance and troubleshooting any performance issues.', 'Collaborating with the Scandinavian team and (occasionally) managers from the Balkan region.', '(in the future) Working with PostgreSQL and AWS.']\n",
      "requirements: ['3+ years experience in a related field.', 'Comprehensive understanding of the Transact-SQL.', 'Practice with Oracle and PL/SQL.', \"Bachelor's Degree in Computer Science, Math, or a related field.\"]\n",
      "application_link: https://www.pracuj.pl/aplikuj/t-sql-engineer-warszawa-aleja-niepodleglosci-18,oferta,1003897237?apid=647ffec7-0607-4e5f-822c-2414c54676e1&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 40 ---\n",
      "id: 49\n",
      "url: https://www.pracuj.pl/praca/oracle-administrator-warszawa-bukowinska-22b,oferta,1003931802\n",
      "title: Oracle Administrator\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze miesiącdo 28 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Java', 'Ansible', 'AWS', 'Terraform', 'Oracle']\n",
      "responsibilities: ['Administracja i optymalizacja baz danych Oracle w środowiskach on-premise oraz chmurowych.', 'Automatyzacja procesów zarządzania infrastrukturą IT przy użyciu Ansible, Terraform, Pulumi.', 'Wdrażanie i utrzymanie procesów CI/CD.']\n",
      "requirements: ['Doświadczenie w administracji bazami danych Oracle.', 'Znajomość systemów operacyjnych Linux i Windows.', 'Umiejętność automatyzacji z wykorzystaniem Ansible.', 'Doświadczenie w pracy z chmurą (AWS) oraz narzędziami IaC (Pulumi/Terraform).', 'Znajomość procesów CI/CD i narzędzi do monitorowania systemów.', 'Wiedza z zakresu bezpieczeństwa systemów i baz danych.', 'Umiejętność analitycznego myślenia i rozwiązywania problemów.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/oracle-administrator-warszawa-bukowinska-22b,oferta,1003931802?apid=d13a1903-fd39-47cc-a247-7dba30b2e05e&oca=None&acv=0\n",
      "\n",
      "--- Job Record 41 ---\n",
      "id: 50\n",
      "url: https://www.pracuj.pl/praca/programista-programistka-ai-w-obszarze-analizy-obrazow-przemyslowych-i-medycznyc-warszawa-aleja-niepodleglosci-188b,oferta,1003900001\n",
      "title: Programista / Programistka AI w obszarze analizy obrazów przemysłowych i medycznych (z doktoratem / doktorant/tka)\n",
      "work_location: None\n",
      "validity: ważna jeszcze 15 dnido 13 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat, część etatu\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Aleja Niepodległości 188B, Śródmieście, WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatówwakaty: 2']\n",
      "technologies: ['ML/DL', 'Pandas/Numpy/SciPy', 'R', 'SQL', 'Python']\n",
      "responsibilities: ['opracowywanie modeli ML/DL analizy obrazów przemysłowego CT (dla danych rzeczywistych i syntetycznych);', 'optymalizowanie i ewaluowanie modeli, wybór odpowiednich metryk, raportowanie efektywności modeli;', 'przygotowywanie dokumentacji technicznej, raportów, analiz;', 'opracowywanie zbiorów treningowych/walidacyjnych/testowych dla rozwoju i optymalizacji modeli;', 'udział w projektach B+R realizowanych w CIMC, prowadzenie badań z zakresu informatyki medycznej / komputerowej wizji z uwzględnieniem obszarów strategicznych dla prac badawczych CIMC.']\n",
      "requirements: ['stopień naukowy doktora w dziedzinie nauk inżynieryjno-technicznych, dyscyplina: informatyka techniczna i telekomunikacja lub inżynieria biomedyczna (lub odpowiedniki według starszych klasyfikacji nauk); możliwy także stopień naukowy magistra inżynieryjno-technicznych, dyscyplina: informatyka techniczna i telekomunikacja lub inżynieria biomedyczna (lub odpowiedniki według starszych klasyfikacji nauk) pod warunkiem rozpoczętego doktoratu;', 'doświadczenie zawodowe w obszarze analizy i rozpoznawania obrazów oraz systemów komputerowej wizji;', 'doświadczenie zawodowe w realizacji projektów B+R lub naukowych,', 'doświadczenie w opracowywaniu raportów z badań lub w prowadzeniu prac wdrożeniowych;', 'wiedza z zakresu algorytmów ML/DL,  znajomość zagadnień i technik z obszaru komputerowej wizji i analizy obrazów, doświadczenie w statystycznej analizie wyników;', 'praktyczna znajomość wybranych środowisk ML/DL (PyTorch, TensorFlow, Keras,) oraz pakietów  Pandas/Numpy/SciPy;', 'umiejętność analizy danych ilościowych z wykorzystaniem języka programowania R, SQL lub Python;', 'bardzo dobra znajomość języka angielskiego, w tym umiejętność opracowania artykułu naukowego w tym języku oraz prezentacji wyników badań naukowych po angielsku;', 'pożądane doświadczenie w obszarze informatyki medycznej: analizy i rozpoznawania obrazów medycznych oraz wspomagania decyzji klinicznych;', 'pożądane doświadczenie zarządzaniu bazami i systemami gromadzenia medycznych danych obrazowych klasy PACS w zastosowaniach naukowo-badawczych oraz wykorzystaniu przeglądarek obrazów medycznych klasy DICOM (np. OHIF Viewer).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/programista-programistka-ai-w-obszarze-analizy-obrazow-przemyslowych-i-medycznyc-warszawa-aleja-niepodleglosci-188b,oferta,1003900001?apid=1ed6289d-d890-449d-a1c3-54f9c668986e&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 42 ---\n",
      "id: 51\n",
      "url: https://www.pracuj.pl/praca/programista-sql-warszawa-aleje-jerozolimskie-212a,oferta,1003923053\n",
      "title: Programista SQL\n",
      "work_location: None\n",
      "validity: ważna jeszcze 25 dnido 23 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['aleje Jerozolimskie 212A, Włochy, WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatów']\n",
      "technologies: ['PostgreSQL', 'MS SQL', 'SQL Serwer', 'ETL/SSIS', 'Git', 'GitLab', 'PowerShell', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'koncentrujesz się na rozwoju produktu', 'agile', 'scrum']\n",
      "responsibilities: ['Projektowanie i tworzenie rozwiązań bazodanowych dedykowanego oprogramowania na podstawie analizy procesów biznesowych,', 'Modelowanie struktur danych,', 'Implementacja procesów bazodanowych,', 'Testowanie oraz wdrażanie tworzonych rozwiązań,', 'Rozwijanie i optymalizacja wydajności już działających procesów,', 'Administracja i monitoring serwerów.']\n",
      "requirements: ['Rozumienie specyfiki OLTP i systemów transakcyjnych,', 'Doskonała znajomość założeń ACID, transakcji oraz poziomów izolacji,', 'Umiejętność znajdowania rozsądnych rozwiązań na dowolne problemy związane z istniejącym kodem SQL,', 'Umiejętność formułowania logiki biznesowej w działający kod SQL,', 'Doświadczenie ze skuteczną implementacją technik optymalizacyjnych.', 'Wykształcenie wyższe informatyczne lub pokrewne,', '2-letnie doświadczenie w pracy na podobnym stanowisku,', 'Samodzielność w działaniu, proaktywność, otwartość na nowe rozwiązania,', 'Komunikatywność w języku polskim na poziomie bardzo dobrym - Warunek Konieczny!']\n",
      "application_link: https://www.pracuj.pl/aplikuj/programista-sql-warszawa-aleje-jerozolimskie-212a,oferta,1003923053?apid=9be16e7b-524d-42ef-930b-7f9336cf1e09&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 43 ---\n",
      "id: 52\n",
      "url: https://www.pracuj.pl/praca/data-engineer-swarzedz-rabowicka-13,oferta,1003899975\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 15 daysto 13 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Rabowicka 13, SwarzędzSwarzędz, Greater Poland', 'contract of employment']\n",
      "technologies: ['Python', 'Scala', 'SQL', 'Git', 'GitLab', 'TensorFlow', 'Databricks', 'Machine Learning', 'Azure']\n",
      "responsibilities: ['Design, implement, and maintain scalable and efficient data pipelines using Databricks', 'Align to the global BI Team to implement data platform / data lake solutions and workflows based on the Group cloud technology', 'Collaborate with data scientists and analysts to integrate machine learning models into the data pipeline.', 'Implement and optimize machine learning workflows and processes.', 'Design and implement data models to support analytics and reporting requirements.', 'Ensure data models are scalable, efficient, and aligned with business objectives.', 'Serve as a subject matter expert on Databricks, providing guidance on best practices and optimizations.', 'Work on performance tuning and optimization of Databricks clusters.', 'Implement data quality checks and governance processes to ensure data accuracy and reliability.']\n",
      "requirements: [\"Bachelor's/Master’s degree in Computer Science, Engineering, or a related field\", 'Proven experience as a Data Engineer with a focus on Databricks and machine learning', 'Strong proficiency in programming languages such as Python or Scala (PySpark preferred)', 'Proficient in SQL with a solid understanding of data modeling concepts and best practices', 'Experience building data pipelines using orchestration tools like Azure Data Factory and Databricks workflows', 'Familiarity with version control systems such as Git, GitHub, or GitLab', 'Hands-on experience with machine learning frameworks (e.g., TensorFlow, PyTorch), including GenAI applications and Large Language Models (LLMs)', 'Extensive experience working with cloud platforms (Azure)', 'Databricks Associate Certification or Azure Data Engineer Associate(DP-203) required', 'Knowledge and experience in using the MS Azure Analytics Services, especially Azure Databricks, Azure Data Factory, FiveTran as well as MS Purview', 'Excellent problem-solving skills and attention to detail', 'Strong communication and collaboration skills', 'Fluent English – additional languages are a bonus, in particular German.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-swarzedz-rabowicka-13,oferta,1003899975?apid=09348787-1f1d-4a0d-9fe7-318b2768a5de&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 44 ---\n",
      "id: 53\n",
      "url: https://www.pracuj.pl/praca/solution-information-architect-snowflake-amazon-eks-warszawa-pulawska-2,oferta,1003884353\n",
      "title: Solution & Information Architect - Snowflake/Amazon EKS\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 8 dnido 06 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior), ekspert\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Amazon DocumentDB', 'AWS', 'Python', 'Snowflake Data Cloud', 'Spring Boot', 'AIRFLOW', 'Apache Airflow', 'u klienta']\n",
      "responsibilities: ['Kierowanie zespołami deweloperów i DevOps, zapewniając wsparcie techniczne oraz pełniąc rolę głównego punktu kontaktowego w kwestiach projektowych', 'Tworzenie wysokiej jakości rozwiązań architektonicznych spełniających wymagania biznesowe', 'Projektowanie i zarządzanie strukturami danych oraz przepływami danych w systemach i aplikacjach', 'Wsparcie w identyfikacji i rozwiązywaniu skomplikowanych problemów technicznych']\n",
      "requirements: ['Minimum 8 lat doświadczenia na stanowisku architekta rozwiązań', 'Wiedza na temat przepływu danych klinicznych w PD Data Sciences (PDD), w szczególności przepływu danych non-CRF (preferowane)', 'Doświadczenie w zarządzaniu zespołem i zdolności przywódcze', 'Znajomość procesów walidacji systemów komputerowych', 'Umiejętność komunikacji i pracy zespołowej', 'Python Snowflake Airflow Amazon EMR Amazon EKS Spark (PySpark, Data Partitioning) Java SpringBoot, SpringCloud Apache Iceberg Lakehouse Architecture', 'Doświadczenie w pracy z chmurami obliczeniowymi (AWS, Google Cloud)', 'Znajomość Terraform oraz zasad CI/CD', 'Doświadczenie w obszarze architektury informacji oraz znajomość koncepcji Data Mesh']\n",
      "application_link: https://www.pracuj.pl/aplikuj/solution-information-architect-snowflake-amazon-eks-warszawa-pulawska-2,oferta,1003884353?apid=1a686a33-e11d-4a99-937e-e3d873c1d3a3&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 45 ---\n",
      "id: 98\n",
      "url: https://www.pracuj.pl/praca/analityk-biznesowy-ds-sztucznej-inteligencji-ai-warszawa,oferta,1003899887\n",
      "title: Analityk Biznesowy ds. Sztucznej Inteligencji (AI)\n",
      "work_location: None\n",
      "validity: ważna jeszcze 15 dnido 13 marca 2025\n",
      "contract_type: umowa o pracę, umowa zlecenie, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['BPMN', 'UML']\n",
      "responsibilities: ['Analiza procesów pod kątem możliwości optymalizacji z wykorzystaniem nowych technologii', 'Dokumentowanie procesów biznesowych', 'Współpraca z zespołami operacyjnymi, architektami i programistami AI/ML', 'Współtworzenie analiz opłacalności inwestycji', 'Testowanie rozwiązań tworzonych przez zespół (testy odbiorcze)']\n",
      "requirements: ['Wykształcenie wyższe (preferowane kierunki: analityka biznesowa, ekonomia, informatyka lub pokrewne)', 'Umiejętność analizy, modelowania i dokumentowania procesów biznesowych', 'Znajomość notacji (np. BPMN, UML) i narzędzi modelujących (Visio, Igrafix, Visual Paradigm)', 'Znajomość funkcjonalności Generative AI i Machine Learning', 'Doświadczenie w zarządzaniu projektami, znajomość zwinnych metodyk (Agile/Scrum)', 'Samodzielność, dobra organizacja i kreatywność']\n",
      "application_link: https://www.pracuj.pl/aplikuj/analityk-biznesowy-ds-sztucznej-inteligencji-ai-warszawa,oferta,1003899887?apid=488a1663-948b-496e-ad39-907e4f0e344c&oca=None&acv=0\n",
      "\n",
      "--- Job Record 46 ---\n",
      "id: 99\n",
      "url: https://www.pracuj.pl/praca/administrator-hurtowni-danych-warszawa,oferta,1003926439\n",
      "title: Administrator Hurtowni Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SAS', 'SAS Viya', 'Oracle', 'SQL']\n",
      "responsibilities: ['Utrzymanie i rozwój procesów zasilania oraz raportowania Hurtowni Danych', 'Nadzór nad kontrolą jakości danych w ramach procesów zasilania i przetwarzania', 'Realizacja wdrożeń w obszarze Hurtowni Danych', 'Wsparcie użytkowników w zakresie identyfikacji sytuacji nadzwyczajnych i komunikacji z zespołami wsparcia technicznego', 'Tworzenie narzędzi wspomagających pracę developerów – m.in. wspólne, re-używalne komponenty (SAS/SAS Viya/Oracle/inne technologie)', 'Optymalizacja przetwarzania danych oraz analiza i usprawnianie wydajności systemów', 'Wsparcie projektów strategicznych pod kątem technologii i procesów']\n",
      "requirements: ['Co najmniej 3 lata doświadczenia w administracji Hurtowniami Danych oraz procesami ETL', 'Znajomość technologii: SAS/SAS Viya, Oracle, SQL oraz innych narzędzi do przetwarzania danych', 'Umiejętność analizy i optymalizacji procesów przetwarzania danych', 'Doświadczenie w budowie narzędzi i komponentów wspierających pracę zespołów developerskich', 'Umiejętność pracy w modelu hybrydowym i komunikacji z różnymi zespołami']\n",
      "application_link: https://www.pracuj.pl/aplikuj/administrator-hurtowni-danych-warszawa,oferta,1003926439?apid=576c0f18-adac-4c1d-accd-97f885af9e49&oca=None&acv=0\n",
      "\n",
      "--- Job Record 47 ---\n",
      "id: 100\n",
      "url: https://www.pracuj.pl/praca/data-scientist-warszawa,oferta,1003893036\n",
      "title: Data Scientist\n",
      "work_location: None\n",
      "validity: ważna jeszcze 11 dnido 09 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Python', '4GL', 'SQL', 'SAS Viya', 'wewnątrz organizacji', 'koncentrujesz się na jednym projekcie', 'agile', 'scrum']\n",
      "responsibilities: ['Tworzenie inteligentnych systemów do automatyzacji i usprawniania procesów obsługi', 'Przeprowadzanie zaawansowanych analiz statystycznych, ekonometrycznych i z zakresu machine learning w kontekście efektywności procesów', 'Projektowanie oraz wdrażanie nowych struktur danych i procesów raportowych na platformie BI/DW', 'Identyfikacja potrzeb biznesowych oraz projektowanie i testowanie rozwiązań', 'Rozwój i utrzymanie procesów związanych z hurtowniami danych, w tym budowa struktur danych na potrzeby modelowania i oceny']\n",
      "requirements: ['Wykształcenie wyższe w dziedzinie data science, ekonometrii, statystyki, matematyki, fizyki lub pokrewnych', 'Co najmniej 2 lata doświadczenia na stanowisku Data Scientist', 'Biegłość w metodach ilościowych: zaawansowane modele ekonometryczne, klasyfikacyjne, probabilistyczne', 'Znajomość Pythona i/lub 4GL oraz podstaw SQL', 'Doświadczenie w pisaniu produkcyjnych rozwiązań ML, testowaniu danych oraz tworzeniu testów jednostkowych']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-warszawa,oferta,1003893036?apid=4b132573-e7d3-469a-baf8-87089ae82d7a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 48 ---\n",
      "id: 101\n",
      "url: https://www.pracuj.pl/praca/data-engineer-gcp-warszawa-postepu-15,oferta,1003874135\n",
      "title: Data Engineer (GCP)\n",
      "work_location: Company locationPostępu 15, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for 3 daysto 01 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['SQL', 'Google Cloud Platform', 'Python', 'Java', 'Looker', 'Tableau', 'Microsoft Power BI', 'Azure DevOps', 'in house', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'agile']\n",
      "responsibilities: ['Design and implement data warehouse solutions.', 'Build and manage data ingestion and transformation pipelines.', 'Design BI dashboards.', 'Contribute to the technology choices and architecture of new components and services.', 'Collaborate with the Product Owner and Feature Engineer to refine requirements and translate them into working software.', 'Deliver high-quality, working software that meets customer needs.', 'Stay current with the latest developments in technical and financial services.', 'Learn and apply financial markets concepts.', 'Discuss features with stakeholders and translate them into valuable software.', 'Demonstrate a curiosity to learn fundamental software engineering concepts (e.g., serialization, threading, transactions, functional programming).', 'Actively listen to, learn from, and share knowledge with international colleagues.']\n",
      "requirements: ['3+ years of data engineering experience.', 'Strong analytical skills, a proactive approach, and the ability to work effectively in cross-border international teams.', 'Proficiency in designing and implementing data warehouse solutions.', 'Experience building and managing data ingestion and transformation solutions.', 'Experience designing BI dashboards.', 'Fundamental knowledge of at least one programming language (Python or Java).', 'Fluency in SQL and expertise in query optimization.', 'Experience with Continuous Integration & Continuous Delivery practices and tooling.', 'Experience or interest in IT security concepts.', 'Fluency in English.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-gcp-warszawa-postepu-15,oferta,1003874135?apid=e0fc0cd8-dd03-485d-bbda-1b5e2b231e4d&oca=None&acv=0\n",
      "\n",
      "--- Job Record 49 ---\n",
      "id: 102\n",
      "url: https://www.pracuj.pl/praca/oracle-data-platform-engineer-krakow-szlak-49,oferta,1003892970\n",
      "title: Oracle Data Platform Engineer\n",
      "work_location: None\n",
      "validity: valid for 11 daysto 09 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Szlak 49, Stare Miasto, KrakówKraków, Lesser Poland', 'Робота для іноземцівбез польської']\n",
      "technologies: ['PL/SQL', 'Oracle Database', 'CI/CD', 'Power BI', 'REST API', 'Oracle REST Data Services', 'Data Lineage', 'Data Migration']\n",
      "responsibilities: ['Hands-on expertise with cloud solutions (Oracle Cloud or similar)', 'Ability to dive into any new language or technology if needed', '5+ years of professional experience', 'Good English skills (B2/C1) for communication in an international environment', 'Openness for business travel to Malmö (once a quarter)']\n",
      "requirements: []\n",
      "application_link: https://www.pracuj.pl/aplikuj/oracle-data-platform-engineer-krakow-szlak-49,oferta,1003892970?apid=f1c0bd45-eb99-492f-bcdd-db11fb35b6ca&oca=None&acv=0\n",
      "\n",
      "--- Job Record 50 ---\n",
      "id: 103\n",
      "url: https://www.pracuj.pl/praca/pl-sql-developer-warszawa,oferta,1003917194\n",
      "title: PL/SQL Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 23 dnido 21 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL', 'PL/SQL', 'Oracle Partitioning', 'Oracle Data Guard', 'Oracle Sharding', 'SQL Data Modeler', 'TKPROF']\n",
      "responsibilities: ['Projektowania efektywnych i skalowalnych modeli danych z zastosowaniem dobrych praktyk', 'Programowanie w języku SQL oraz PL/SQL', 'Analiz działania systemu oraz identyfikacja wąskich gardeł co umożliwi mu efektywne zarządzanie wydajnością bazy bez bezpośredniego zaangażowania administratora baz danych (DBA)', 'Pisania testów automatycznych PL/SQL', 'Tworzenia zrozumiałej i czytelnej dokumentacji technicznej wytwarzanego oprogramowania']\n",
      "requirements: ['Minimum 5 lat doświadczenia w pracy na podobnym stanowisku', 'Znajomość zasad modelowania danych oraz dobrych praktyk w tym zakresie', 'Znakomita znajomość składni i konstrukcji języka SQL i PL/SQL', 'Zaawansowana optymalizacja zapytań SQL i PL/SQL', 'Umiejętność wstecznej analizy kodu', 'Znajomość zaawansowanych elementów bazy danych Oracle (np.DBMS_ , UTL_ ; USER_/ALL_/DBA_/v$)', 'Znajomość bazy danych Oracle 19c+', 'Znajomość Oracle Partitioning, Oracle Data Guard, Oracle Sharding', 'Doświadczenie z bardzo dużymi zbiorami danych', 'Znajomość pisania testów automatycznych PL/SQL', 'Podstawowa znajomość systemów operacyjnych klasy UNIX oraz skryptów bash']\n",
      "application_link: https://www.pracuj.pl/aplikuj/pl-sql-developer-warszawa,oferta,1003917194?apid=10286826-3b29-4e27-b2d4-91e805c0520a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 51 ---\n",
      "id: 104\n",
      "url: https://www.pracuj.pl/praca/bi-developer-poznan,oferta,1003922811\n",
      "title: BI Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 25 dnido 23 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['PoznańPoznań, wielkopolskie']\n",
      "technologies: ['SQL', 'Microsoft Power BI']\n",
      "responsibilities: ['Udział w zespole projektowym celem stworzenia modelu danych biznesowych oraz połączeń pomiędzy hurtownią danych i systemami / aplikacjami biznesowymi', 'Integrowanie danych z wielu źródeł, opisywanie i unifikowanie w ramach spójnego modelu biznesowego danych', 'Monitorowanie procesów zarządzania danymi oraz ich modyfikowanie pod kątem wymagań klienta biznesowego', 'Projektowanie i tworzenie zaawansowanych rozwiązań analitycznych, raportów, dashboardów w obrębie środowiska Business Intelligence / data mining', 'Zarządzanie modelem danych podstawowych spółki', 'Opieka i rozwój nad środowiskiem BI', 'Tworzenie i wdrażanie skryptów SQL oraz ich testowanie', 'Optymalizacja kodu i konfiguracji w celu zapewnienia optymalnej wydajności środowiska', 'Monitorowanie i rozwiązywanie problemów związanych z bazami danych', 'Zapewnienie zgodności tworzonych rozwiązań z wymogami w zakresie cyber security']\n",
      "requirements: ['Wykształcenie informatyczne / matematyka / statystyka / ekonometria', 'Umiejętność projektowania i optymalizacji modeli danych w SQL', 'Dobra znajomość SQL (MS SQL, DB2, ORACLE), relacyjnych i nierelacyjnych baz danych', 'Praktyczna znajomość narzędzi BI / data mining', 'Biegła znajomość MS Excel', 'Znajomość języka angielskiego umożliwiającą swobodną komunikację i czytanie dokumentacji technicznej', 'Analityczne myślenie, dokładność, szybkie wyciąganie wniosków i rozwiązywanie problemów', 'Komunikatywność i umiejętność pracy w zespole, inicjatywa w działaniu', 'Samodzielność i umiejętność pracy z harmonogramem']\n",
      "application_link: https://www.pracuj.pl/aplikuj/bi-developer-poznan,oferta,1003922811?apid=629e37c4-5bc2-4333-bbf9-a473438a26dc&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 52 ---\n",
      "id: 105\n",
      "url: https://www.pracuj.pl/praca/administrator-baz-danych-wroclaw,oferta,1003922795\n",
      "title: Administrator Baz Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 25 dnido 23 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['WrocławWrocław, dolnośląskie']\n",
      "technologies: ['Oracle', 'PL/SQL', 'Linux', 'Jenkins']\n",
      "responsibilities: ['Na tym stanowisku administrujesz bankowymi bazami danych na środowiskach produkcyjnym, testowych i developerskich. Ponadto:', 'dbasz o aktualność wersji silnika baz danych', 'monitorujesz, analizujesz i nadzorujesz dostępność i wydajność baz danych', 'realizujesz usługi informatyczne dla jednostek biznesowych banku w zakresie dostępu do baz danych', 'rozwiązujesz problemy/incydenty zgłaszane przez jednostki banku', 'prowadzisz dokumentację oraz współpracujesz z dostawcami oprogramowania w zakresie obsługi błędów i problemów eksploatacyjnych na bazach danych', 'bierzesz udział w pracach zespołów projektowych wdrażających usługi IT', 'wdrażasz nowe technologie']\n",
      "requirements: ['Jesteśmy otwarci zarówno na osoby o podstawowych kompetencjach (które chcą się rozwijać w zakresie w/w technologii) jak i specjalistów, którzy swoim doświadczeniem z innych organizacji wspomogą nasz zespół administratorów. Szczególnie istotne jest dla nas:', 'znajomość technologii: Oracle, PLSQL, unix, linux, jenkins', 'umiejętności analityczne w zakresie rozwiązywania problemów w aplikacjach korzystających z baz danych', 'chęć do pracy w zespole', 'kreatywność, samodzielność, otwartość, komunikatywność', 'zdolność szybkiego uczenia się', 'sumienność w  realizacji powierzonych zadań', 'umiejętność pracy w warunkach zmiennych priorytetów', 'znajomość języka angielskiego w stopniu umożliwiającym swobodną pracę z dokumentacją techniczną oraz komunikację z producentem oprogramowania']\n",
      "application_link: https://www.pracuj.pl/aplikuj/administrator-baz-danych-wroclaw,oferta,1003922795?apid=355146f6-d2ef-4c45-ab4c-f7dd3ff98e90&oca=None&acv=0\n",
      "\n",
      "--- Job Record 53 ---\n",
      "id: 106\n",
      "url: https://www.pracuj.pl/praca/database-infrastructure-engineer-warszawa-tasmowa-10,oferta,1003931522\n",
      "title: Database Infrastructure Engineer\n",
      "work_location: None\n",
      "validity: valid for a monthto 28 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Taśmowa 10, Mokotów, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['Microsoft SQL Server', 'Oracle', 'DB2', 'MySQL', 'PostgreSQL', 'MongoDB', 'Ansible', 'VMware', 'OpenShift', 'Kubernetes', 'in house']\n",
      "responsibilities: ['Develop and implement database infrastructure strategies that support business goals and ensure high availability, scalability, and performance', 'Design and architect database solutions, including data modeling, schema design, and integration with other systems', 'Evaluate and recommend new database technologies and tools to enhance our infrastructure', 'Implement automation for database deployment, management, and monitoring using tools like Ansible to improve efficiency and reduce manual intervention', 'Utilize virtualization technologies such as VMware, OpenShift, and Kubernetes to manage and deploy containerized database applications', 'Identify and address database vulnerabilities, ensuring timely application of patches and updates to maintain security and compliance', 'Ensure database security by designing and implementing robust security measures, including access controls and encryption', 'Develop and maintain disaster recovery plans to ensure data integrity and availability in case of failures', 'Work with development and operations teams to support database-related projects and initiatives', 'Create and maintain detailed documentation of database architecture, configurations, and procedures', 'Participate in guard duty rotations to provide 24/7 support and monitoring of database systems']\n",
      "requirements: ['Bachelor’s degree in computer science, Information Technology, or a related field', 'Minimum of 5 years of experience in database infrastructure engineering and architecture', 'Proficiency in database management systems (e.g., SQL Server, Oracle, DB2, MySQL, PostgreSQL, MongoDB), data modeling, infrastructure automation tools (e.g., Ansible), and virtualization technologies (e.g., VMware, OpenShift, Kubernetes) Strong architecture skills are essential', 'Strong analytical and problem-solving skills with the ability to design and implement complex database solutions', 'Excellent communication and teamwork skills', 'Relevant certifications (e.g., Microsoft Certified: Azure Solutions Architect, Oracle Certified Master, AWS Certified Solutions Architect) are a plus']\n",
      "application_link: https://www.pracuj.pl/aplikuj/database-infrastructure-engineer-warszawa-tasmowa-10,oferta,1003931522?apid=c1110cf0-7632-4acc-89bd-358a98b4ff4a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 54 ---\n",
      "id: 107\n",
      "url: https://www.pracuj.pl/praca/senior-data-scientist-warszawa,oferta,1003917063\n",
      "title: Senior Data Scientist\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 23 dnido 21 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['ETL', 'Python', 'wewnątrz organizacji', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt']\n",
      "responsibilities: ['Analiza rynku oraz identyfikacja kluczowych danych niezbędnych do tworzenia wiarygodnych rankingów.', 'Pozyskiwanie i przetwarzanie najlepszych dostępnych źródeł danych przy użyciu nowoczesnych narzędzi, w tym zaawansowanych skryptów w Python.', 'Projektowanie przejrzystych, logicznych i skalowalnych metodologii rankingowych.', 'Tworzenie stabilnych prototypów rankingów oraz ich produkcyjnych odpowiedników, ze szczególnym naciskiem na utrzymanie wysokiej higieny projektu poprzez czysty kod i uporządkowaną strukturę.', 'Współpraca z zespołem w celu ciągłej optymalizacji procesów analitycznych.']\n",
      "requirements: ['Umiejętność zarządzania bazami danych oraz integrowania informacji z różnych źródeł (ETL) w celu tworzenia kompleksowych raportów.', 'Bardzo dobra znajomość Python, umożliwiająca automatyzację i zaawansowaną analizę danych.', 'Znajomość popularnych narzędzi business intelligence oraz biegłość w pracy z narzędziami analitycznymi.', 'Doskonałe umiejętności przekształcania „surowych” danych w wartościowe informacje, ich wizualizacji oraz prezentacji wyników w klarowny sposób.', 'Doświadczenie w przetwarzaniu dużych wolumenów danych i budowaniu stabilnych prototypów systemów rankingowych.', 'Umiejętność tworzenia publikacji i artykułów opartych na analizie danych.', 'Zdolność do identyfikowania problemów i proponowania efektywnych rozwiązań przy wykorzystaniu nowoczesnych technologii.', 'Dbałość o higienę projektu – utrzymywanie wysokich standardów kodowania oraz organizacji pracy.', 'Wyróżnianie się skrupulatnością, samodzielnością, dociekliwością oraz efektywną komunikacją.', 'Znajomość języka angielskiego na poziomie minimum B2.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-scientist-warszawa,oferta,1003917063?apid=11233e94-4bd5-4ad9-8347-2cdf103ea475&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 55 ---\n",
      "id: 108\n",
      "url: https://www.pracuj.pl/praca/clinical-informaticist-rwd-bioinformatics-warszawa-pulawska-2,oferta,1003907772\n",
      "title: Clinical Informaticist - RWD Bioinformatics\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 18 dnido 16 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: None\n",
      "position: starszy specjalista (Senior), ekspert\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['część etatu, dodatkowa / tymczasowa']\n",
      "technologies: ['RWD', 'OMOP', 'Python', 'R', 'FHIR', 'SNOMED', 'Neo4j', 'Athena', 'u klienta']\n",
      "responsibilities: ['W ramach tego projektu będziesz pełnił/a rolę pomostu między zespołami IT a działami medycznymi, zajmując się analizą danych rzeczywistych (RWD) oraz integracją narzędzi informatycznych z systemami medycznymi. Twoja praca będzie miała kluczowe znaczenie dla wdrażania nowych rozwiązań w obszarze zdrowia cyfrowego oraz współpracy z zespołami terapeutycznymi w celu rozwoju przyszłych strategii.']\n",
      "requirements: ['5+ lat doświadczenia w informatyce klinicznej lub pokrewnych obszarach.', 'Doświadczenie z danymi rzeczywistymi (RWD) – w tym z danymi z EHR, roszczeniami, rejestrami, itp.', 'Praktyczna znajomość OMOP, integracji terminologii (np. OHDSI Athena, UMLS), oraz pracy z narzędziami takimi jak Python (numpy, pandas), R, SQL, CI/CD, Git.', 'Doświadczenie z bazami danych grafowymi (np. Neo4j, GraphDB) oraz bazami danych w chmurze (np. Snowflake).', 'Znajomość standardów i terminologii zdrowotnych (np. FHIR, SNOMED).', 'Umiejętność pracy w interdyscyplinarnych zespołach oraz komunikacja z interesariuszami z różnych działów, w tym z zespołami technicznymi i medycznymi.', 'Dobre umiejętności komunikacyjne w języku angielskim – zarówno w mowie, jak i piśmie.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/clinical-informaticist-rwd-bioinformatics-warszawa-pulawska-2,oferta,1003907772?apid=0e950f3d-0cb1-4094-8f5b-8e35498693fe&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 56 ---\n",
      "id: 109\n",
      "url: https://www.pracuj.pl/praca/administrator-baz-danych-katowice-mikolowska-100,oferta,1003873973\n",
      "title: Administrator Baz Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 3 dnido 01 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Mikołowska 100, KatowiceKatowice, śląskie']\n",
      "technologies: ['Oracle', 'PostgreSQL', 'MySQL', 'MS SQL Server', 'SQL', 'Oracle Certified Professional', 'Windows Server', 'Bash', 'Python', 'Perl', 'PowerShell', 'Puppet', 'RH Satelite', 'SVN', 'Commvault', 'Veeam']\n",
      "responsibilities: ['zarządzanie bazami danych Oracle, MS SQL Server, PostgreSQL, mySQL,', 'utrzymanie ciągłości działania baz danych,', 'tworzenie, konfigurowanie i utrzymanie środowisk wysokiej dostępności,', 'instalowanie i patchowanie baz danych,', 'zarządzanie bezpieczeństwem baz danych,', 'backupowanie, klonowanie i odtwarzanie baz danych,', 'utrzymanie środowisk deweloperskich oraz produkcyjnych,', 'rozwiązywanie bieżących problemów,', 'dokumentowanie prowadzonych działań,', 'śledzenie nowości i trendów.']\n",
      "requirements: ['wykształcenie średnie techniczne lub wyższe informatyczne lub pokrewne,', 'znajomość administrowania jedną z baz spośród: Oracle, PostgreSQL, mySQL, MS SQL Server,', 'znajomość języka SQL,', 'znajomość podstawowych zagadnień administracyjnych systemu Linux lub Unix.', 'znajomość języka angielskiego w stopniu umożliwiającym czytanie dokumentacji technicznej,', 'zdolność analitycznego myślenia i szybkiego rozwiązywania problemów,', 'umiejętność pracy zespołowej jak również samodzielnej,', 'kreatywność w wykonywaniu zadań,', 'wysoka kultura osobista,', 'prawo jazdy kategorii B.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/administrator-baz-danych-katowice-mikolowska-100,oferta,1003873973?apid=32c59194-ead2-421f-b6bf-86d8f280d205&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 57 ---\n",
      "id: 110\n",
      "url: https://www.pracuj.pl/praca/databricks-engineer-warszawa-bukowinska-22b,oferta,1003931333\n",
      "title: Databricks Engineer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze miesiącdo 28 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Databricks', 'ETL', 'Python', 'Azure DevOps', 'SQL', 'PL/SQL', 'u klienta', 'scrum', 'fullstack developer', 'lider techniczny', 'big data developer', 'data scientist', 'product owner', 'project manager']\n",
      "responsibilities: ['Projektowanie, budowanie i zarządzanie platformą Data Bricks', 'Optymalizacja i skalowanie infrastruktury', 'Tworzenie i utrzymywanie skryptów ETL oraz modeli danych', 'Wdrażanie rozwiązań Big Data i Machine Learning', 'Nadzorowanie procesu integracji danych oraz analizy danych', 'Utrzymywanie wysokiego poziomu bezpieczeństwa danych', 'Współpraca z zespołem programistów, analityków danych i inżynierów oprogramowania']\n",
      "requirements: ['Min. 4 lata doświadczenia', 'Databricks + Azure', 'PL/SQL', 'Python', 'ETL', 'Bazy danych SQL']\n",
      "application_link: https://www.pracuj.pl/aplikuj/databricks-engineer-warszawa-bukowinska-22b,oferta,1003931333?apid=2c9be6a0-ab79-4b4a-b90e-a18f645d1b65&oca=None&acv=0\n",
      "\n",
      "--- Job Record 58 ---\n",
      "id: 111\n",
      "url: https://www.pracuj.pl/praca/treasury-data-analyst-warszawa-tadeusza-rejtana-17,oferta,1003873912\n",
      "title: Treasury Data Analyst\n",
      "work_location: None\n",
      "validity: valid for 3 daysto 01 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Tadeusza Rejtana 17, Mokotów, WarszawaWarszawa, Masovian']\n",
      "technologies: ['Data Analysis', 'Banking', 'English', \"at the client's site\", 'you have influence on the product', 'you focus on product development', 'agile']\n",
      "responsibilities: ['Gather and document business needs for data integration between financial systems and SAP.', 'Work with IT and business teams to ensure smooth data flow and consistency.', 'Develop and maintain data mappings and transformation rules.', 'Support troubleshooting and manage defects during new feature rollouts.', 'Help users and team members by sharing technical expertise.', 'Validate and confirm the golden source of required data elements.', 'Review and challenge taxonomy requirements to ensure accuracy.', 'Apply knowledge of balance sheet management, capital, and liquidity reporting to improve processes.']\n",
      "requirements: ['5+ years of work experience, with at least 2+ years in data management.', 'Experience with data models and transformation projects.', 'Strong knowledge of financial markets, balance sheets, liquidity, and capital reporting.', 'Ability to map financial systems, identify issues, and find solutions.', 'Excellent communication and stakeholder management skills.', 'High energy, initiative, and a problem-solving mindset.', 'Previous experience in financial services or banking.', 'Familiarity with business architecture principles and regulatory requirements.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/treasury-data-analyst-warszawa-tadeusza-rejtana-17,oferta,1003873912?apid=e790d32b-e10f-40bc-8548-8b8947e413b1&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 59 ---\n",
      "id: 112\n",
      "url: https://www.pracuj.pl/praca/senior-power-bi-analyst-developer-krakow,oferta,1003892638\n",
      "title: Senior Power BI Analyst & Developer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 11 dnido 09 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Microsoft Power BI', 'SQL', 'Azure Synapse Analytics', 'Azure Data Factory', 'Azure Data Factory', 'Databricks', 'Azure Integration', 'Python']\n",
      "responsibilities: ['projektowanie i implementacja zaawansowanych raportów w Power BI,', 'tworzenie i optymalizacja zapytań SQL oraz procesów ETL,', 'budowa i utrzymanie modeli danych w Azure Synapse Analytics,', 'implementacja przepływów danych i integracji w Azure Data Factory,', 'tworzenie miar i kalkulacji w DAX,', 'zapewnienie integralności danych i optymalizacja wydajności,', 'współpraca z interesariuszami i zespołami biznesowymi.']\n",
      "requirements: ['masz min. 3 lata doświadczenia w Power BI oraz SQL (E2E),', 'masz praktyczne doświadczenie w pracy z Azure Synapse Analytics,', 'potrafisz projektować i implementować przepływy danych w Azure Data Factory,', 'masz silne umiejętności w zakresie DAX i modelowania danych,', 'umiesz współpracować z interesariuszami biznesowymi,', 'znasz język angielski na poziomie min. B2+.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-power-bi-analyst-developer-krakow,oferta,1003892638?apid=aa91af28-6182-443d-ba4e-eccacd16ada5&oca=None&acv=0\n",
      "\n",
      "--- Job Record 60 ---\n",
      "id: 113\n",
      "url: https://www.pracuj.pl/praca/lead-cybersecurity-data-engineer-krakow-kapelanka-42a,oferta,1003931297\n",
      "title: Lead Cybersecurity Data Engineer\n",
      "work_location: None\n",
      "validity: valid for a monthto 28 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['Microsoft Azure', 'Python', 'Java', 'C#']\n",
      "responsibilities: ['Ingestion and provisioning of raw datasets, enriched tables, and/or curated, re-usable data assets to enable Cybersecurity use cases.', 'Driving improvements in the reliability and frequency of data ingestion including increasing real-time coverage.', 'Identifying and onboarding data sources using existing schemas and, where required, conducting exploratory data analysis to investigate and determine new schemas.', 'Extract Translate and Load (ETL) workflows, using both advanced data manipulation tools and programmatically manipulating data throughout our data flows, ensuring data is available at each stage in the data flow, and in the form needed for each system, service, and customer along said data flow.', 'Designing and implementing data pipelines that will collect data from disparate sources across the enterprise, and from external sources, transport said data, and deliver it to our data platform.']\n",
      "requirements: ['Demonstrable experience of Linux administration and scripting (preferably Red Hat Systems).', 'Experience with Python, Spark, SQL, Databricks, Azure Function App, Azure DevOps and Terraform.', 'Understanding of hardware and software principles and storage technologies (SSD, HDD, NVMe), CPU architectures, and Memory & Operating system principles (especially network stack fundamentals).', 'Understanding of network protocols and network design.', 'Designing, building, and maintaining data pipelines and ETL workflows across disparate datasets.', 'Experience in applying data engineering methods to the cyber security domain.', 'Bachelor’s degree in any Science, Technology, Engineering field accompanied with data, technology and/or programming experience also considered.', 'Proficiency in programming languages such as Python, Java, C#, or similar preferred.', 'Experience with server, operating system, and infrastructure technologies such as Nginx/Apache, CosmosDB, Linux, Bash, PowerShell, Prometheus, Grafana, Elasticsearch).', 'Experience operating in highly regulated industry, e.g., Financial Services advantageous.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/lead-cybersecurity-data-engineer-krakow-kapelanka-42a,oferta,1003931297?apid=999d8954-bd43-4fcc-9bbf-1155a457d713&oca=None&acv=0\n",
      "\n",
      "--- Job Record 61 ---\n",
      "id: 114\n",
      "url: https://www.pracuj.pl/praca/lead-cybersecurity-data-acquisition-engineer-krakow-kapelanka-42a,oferta,1003931296\n",
      "title: Lead Cybersecurity Data Acquisition Engineer\n",
      "work_location: None\n",
      "validity: valid for a monthto 28 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['PowerShell', 'Terraform', 'Python']\n",
      "responsibilities: ['Ingestion and provisioning of raw datasets, enriched tables, and/or curated, re-usable data assets to enable Cybersecurity use cases.', 'Driving improvements in the reliability and frequency of data ingestion including increasing real-time coverage.', 'Designing and implementing data pipelines that will collect data from disparate sources across the enterprise, and from external sources, transport said data, and deliver it to our data platform.', 'Extract Translate and Load (ETL) workflows, using both advanced data manipulation tools and programmatically manipulating data throughout our data flows, ensuring data is available at each stage in the data flow, and in the form needed for each system, service, and customer along said data flow.', 'Identifying and onboarding data sources using existing schemas and, where required, conducting exploratory data analysis to investigate and determine new schemas.']\n",
      "requirements: ['Programming experience in the following languages: PowerShell, Terraform, Python Windows command prompt and object orientated programming languages.', 'Demonstrable experience of Linux administration and scripting (preferably Red Hat Systems).', 'Understanding of hardware and software principles and storage technologies (SSD, HDD, NVMe), CPU architectures, and Memory & Operating system principles (especially network stack fundamentals).', 'Technical knowledge and breadth of Azure technology services (Identity, Networking, Compute, Storage, Web, Containers, Databases).', 'Experience with Security Information & Event Management (SIEM) and Security Orchestration, Automation & Response (SOAR) technologies, especially cloud based, is a significant asset.', 'Bachelor’s degree in any Science, Technology, Engineering field accompanied with data, technology and/or programming experience also considered.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/lead-cybersecurity-data-acquisition-engineer-krakow-kapelanka-42a,oferta,1003931296?apid=cf5b9da1-5475-41f6-8dd2-53091a7f84bc&oca=None&acv=0\n",
      "\n",
      "--- Job Record 62 ---\n",
      "id: 115\n",
      "url: https://www.pracuj.pl/praca/datastage-administrator-wroclaw,oferta,1003883887\n",
      "title: Datastage Administrator\n",
      "work_location: Company locationWrocławWrocław, Lower Silesia\n",
      "validity: valid for 8 daysto 06 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'More than one vacancyvacancies: 2']\n",
      "technologies: ['IBM InfoSphere DataStage', 'SQL', 'ETL']\n",
      "responsibilities: ['Install, configure, and manage IBM InfoSphere DataStage components, including servers, engine tiers, clients, and supporting infrastructure, ensuring seamless integration with existing systems and databases', 'Handle daily administrative tasks such as user management, security configurations, role-based access control, and permissions to maintain data confidentiality and integrity', 'Monitor system performance, diagnose issues, and fine-tune configurations to enhance the efficiency and scalability of DataStage jobs and workflows', 'Oversee software updates, patches, and version upgrades for DataStage components, ensuring alignment with vendor guidelines and security best practices', 'Conduct capacity planning to anticipate future infrastructure needs and support the growing demands of data integration', 'Maintain metadata repositories and track data lineage for DataStage jobs and assets, ensuring comprehensive impact analysis capabilities', 'Ensure compliance with regulatory standards and industry best practices for data security, privacy, and access control, implementing necessary safeguards', 'Work closely with IT teams, database administrators, and business stakeholders to integrate DataStage with enterprise systems and data sources, enabling efficient data exchange', 'Provide technical support to end-users by troubleshooting issues, addressing queries, and escalating complex problems to vendor support or senior IT teams when needed', 'Perform administrative functions such as project and user creation, role management, project import/export, database configuration, and dataset management', 'Develop and enforce DataStage standards, policies, and procedures to ensure consistency and best practices']\n",
      "requirements: ['Bachelor’s degree in Computer Science, Information Technology, or a related discipline', 'Demonstrated experience as a DataStage Administrator or in a similar role within enterprise environments', 'Deep understanding of IBM InfoSphere DataStage architecture, components, and best practices', 'Expertise in installing, configuring, and managing DataStage software, including server administration, security settings, and performance optimization', 'Solid knowledge of ETL processes, data warehousing, and data integration methodologies', 'Experience working with database concepts and proficiency in SQL.', 'Familiarity with scripting languages (e.g., Unix shell scripting) for automation and job scheduling', 'Strong analytical and problem-solving skills with keen attention to detail', 'Excellent communication and interpersonal abilities, enabling effective collaboration with cross-functional teams', 'Relevant certifications (e.g., IBM Certified Administrator - InfoSphere DataStage) are an advantage']\n",
      "application_link: https://www.pracuj.pl/aplikuj/datastage-administrator-wroclaw,oferta,1003883887?apid=b758a1fa-f4b4-49c6-94f5-ea798cfb0223&oca=None&acv=0\n",
      "\n",
      "--- Job Record 63 ---\n",
      "id: 116\n",
      "url: https://www.pracuj.pl/praca/migration-technical-leader-with-snowflake-dbt-warszawa-pulawska-2,oferta,1003883779\n",
      "title: Migration Technical Leader with Snowflake/DBT\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 8 dnido 06 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Snowflake Data Cloud', 'Informatica PowerCenter', 'Talend', 'DBT', 'u klienta']\n",
      "responsibilities: ['Doświadczenie z rozproszonymi bazami danych, w szczególności Snowflake.', 'Zrozumienie optymalizacji Snowflake / najlepszych praktyk, aby móc oceniać i kierować decyzjami w odpowiednim kierunku.', 'Wysokie doświadczenie w DBT: to jest rdzeń kodu, w którym odbywają się skomplikowane transformacje, dlatego kandydat musi doskonale rozumieć Jinja i najlepsze praktyki przy pracy z DBT na Snowflake.', 'Doświadczenie w GitLab / CICD. Doświadczenie w DevOps / DataOps jest kluczowe. Zaletą będzie znajomość istniejących pipeline’ów CI/CD.', 'Doświadczenie z modelowaniem wymiarowym i migracją/konwersją do Data Vault, co jest istotne w kontekście wsparcia nowych produktów danych (nasze podejście do wewnętrznej siatki danych).', 'Doświadczenie z Informatica PowerCenter: ważne, aby rozumieć i dyskutować z dostawcami przy ocenie złożoności naszego starszego kodu. Idealny kandydat musi mieć doświadczenie z parametrami (zarówno w plikach, jak i zmiennych środowiskowych).', 'Doświadczenie z SAP BW i łącznością S3 jest dodatkowym atutem, podobnie jak doświadczenie z łącznością API z PowerCenter.', 'Doświadczenie w Talend: używamy Talend wyłącznie do onboardingu danych, więc ważne jest doświadczenie w konfiguracji różnych źródeł (używamy plików CSV, TXT, RDBMS (ODBC, JDBC), API, SAP BW oraz S3). Doświadczenie w najlepszych praktykach oraz uruchamianiu/zlecanie zadań Talend z innych aplikacji jest konieczne (używamy AutomateNow jako harmonogramera).', 'Doświadczenie w AutomateNow lub innych harmonogramerach korporacyjnych, takich jak UC4, Control-M lub podobne. Technologie te są bardzo podobne, więc doświadczenie w pracy z jednym z harmonogramerów korporacyjnych wystarczy, aby kandydat miał odpowiednie doświadczenie.', 'Doświadczenie w migracjach do chmury.']\n",
      "requirements: []\n",
      "application_link: https://www.pracuj.pl/aplikuj/migration-technical-leader-with-snowflake-dbt-warszawa-pulawska-2,oferta,1003883779?apid=2831c851-7f21-4a62-9c8c-524223f9847a&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 64 ---\n",
      "id: 117\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-warszawa-pulawska-2,oferta,1003907521\n",
      "title: Senior Data Engineer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 18 dnido 16 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['AWS', 'Snowflake Data Cloud', 'Python', 'SQL', 'Tableau', 'Airflow', 'Java', 'Scala', 'Docker', 'Jenkins', 'Terraform', 'u klienta']\n",
      "responsibilities: ['Projektowanie i wdrażanie rozwiązań do zarządzania danymi w przestrzeni medycznej.', 'Konfigurowanie i optymalizacja data pipelines.', 'Budowanie rozwiązań do analizy i wizualizacji danych w celu tworzenia produktów danych.', 'Współpraca z zespołami analitycznymi i inżynieryjnymi w celu dostarczania wartościowych produktów opartych na danych.', 'Implementacja najlepszych praktyk w zakresie zarządzania danymi, jakości danych i bezpieczeństwa.', 'Analiza dużych zbiorów danych medycznych, ich przetwarzanie i strukturalizacja.', 'Monitorowanie i poprawa wydajności systemów danych.', 'Praca z narzędziami i technologiami do przetwarzania i wizualizacji danych, takimi jak Tableau, Power BI, czy Python.', 'Udział w tworzeniu i rozwoju architektury danych w organizacji.', 'Wspieranie zespołów w kwestiach związanych z integracją danych i rozwiązywaniem problemów związanych z danymi.']\n",
      "requirements: [\"4+ lata doświadczenia w pracy z językami programowania do pipeline'ów danych (np. Python, R).\", '4+ lata doświadczenia w pracy z SQL.', \"3+ lata doświadczenia w utrzymaniu pipeline'ów danych.\", '3+ lata doświadczenia w pracy z różnymi typami magazynów danych (np. systemy plików, bazy relacyjne, NoSQL).', '3+ lata doświadczenia w pracy z koncepcjami architektury danych (np. ETL/ELT, transmisje w czasie rzeczywistym, jakość danych).', '3+ lata doświadczenia z technologiami chmurowymi (Airflow, Glue, Dataflow, Redshift, S3, Lambda itp.).', '1+ rok doświadczenia w Java i/lub Scala.', 'Bardzo dobra znajomość systemu Git, Gitflow oraz narzędzi DevOps (np. Docker, Jenkins, Terraform).', 'Doskonała znajomość systemu Unix.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-warszawa-pulawska-2,oferta,1003907521?apid=9bd344b1-efef-470f-8330-081322b38472&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 65 ---\n",
      "id: 118\n",
      "url: https://www.pracuj.pl/praca/data-solution-architect-snowflake-aws-platforms-warszawa-pulawska-2,oferta,1003883776\n",
      "title: Data Solution Architect (Snowflake/AWS Platforms)\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 8 dnido 06 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior), ekspert\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Snowflake Data Cloud', 'AWS', 'Data Vault', 'etl', 'Python', 'SQL', 'u klienta']\n",
      "responsibilities: [\"Prowadzenie projektowania i architektury technicznej rozwiązań dla produktów danych i pipeline'ów danych na platformach Snowflake i AWS, zapewniając skalowalność, jakość danych, bezpieczeństwo i wydajność.\"]\n",
      "requirements: [\"Co najmniej 5 lat doświadczenia w roli inżyniera danych lub analityka danych oraz dodatkowo 2+ lata doświadczenia w roli Architekta Rozwiązań Danych w dużych inicjatywach z zakresu hurtowni danych lub data lake, obejmujących pipeline'y danych i procesy ETL w kontekście rozwiązań Snowflake i AWS.\", \"Doświadczenie w pełnieniu roli lidera technicznego, koncentrującego się na projektowaniu i budowaniu produktów danych oraz rozwiązań pipeline'ów danych.\", 'Doświadczenie w zakresie architektury danych, w tym zastosowanie zasad Data Vault 2.0 w projektowaniu i architekturze.', \"Praktyczna znajomość optymalizacji i dostrajania pipeline'ów danych pod kątem wydajności i skalowalności.\", 'Doświadczenie w pracy z SQL, w tym zapytania i manipulacja danymi.', 'Praktyczna znajomość programowania w Pythonie.', 'Znajomość narzędzi DevOps, CI/CD oraz systemów integracji i ingercji danych.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-solution-architect-snowflake-aws-platforms-warszawa-pulawska-2,oferta,1003883776?apid=e0b709cc-a4eb-458e-90f5-e4ba64bdd7e3&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 66 ---\n",
      "id: 119\n",
      "url: https://www.pracuj.pl/praca/administrator-infrastruktury-i-bazy-danych-gdynia,oferta,1003892453\n",
      "title: Administrator Infrastruktury i Bazy Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 11 dnido 09 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior), ekspert\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['GdyniaGdynia, pomorskie']\n",
      "technologies: ['Oracle', 'Linux', 'PowerShell', 'Windows Server', 'Ansible', 'Bash', 'Kubernetes', 'u klienta']\n",
      "responsibilities: ['Wsparcie infrastruktury IT oraz administracja bazami danych w ramach rozwiązań do maskowania danych, wirtualizacji danych oraz generowania danych syntetycznych.', 'Zarządzanie środowiskiem Oracle RDBMS.', 'Administracja systemami Linux oraz Windows.', 'Tworzenie i zarządzanie skryptami w Powershell oraz Bash.', 'Współpraca przy automatyzacji procesów z wykorzystaniem Ansible.', 'Wsparcie w rozwoju i zarządzaniu środowiskiem Kubernetes (mile widziane doświadczenie)']\n",
      "requirements: ['Doświadczenie w administracji bazą danych Oracle RDBMS.', 'Znajomość administracji systemami Linux i Windows.', 'Umiejętność tworzenia skryptów w Powershell i Bash.', 'Doświadczenie w pracy z Ansible.', 'Dodatkowym atutem będzie znajomość MS SQL oraz Kubernetes.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/administrator-infrastruktury-i-bazy-danych-gdynia,oferta,1003892453?apid=96b6e16a-971f-4faa-aad2-6eb00b145a7c&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 67 ---\n",
      "id: 120\n",
      "url: https://www.pracuj.pl/praca/machine-learning-engineer-with-computer-vision-krakow-barska-61,oferta,1003880770\n",
      "title: Machine Learning Engineer with Computer Vision\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 7 dnido 05 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Szukamy wielu kandydatówwakaty: 3']\n",
      "technologies: ['Python', 'SQL', 'Computer Vision', 'Infrastructure as Code', 'Data as Code', 'AWS', 'Azure', 'GCP', 'wewnątrz organizacji', 'koncentrujesz się na jednym projekcie', 'rozwijasz kilka projektów jednocześnie', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'agile', 'scrum']\n",
      "responsibilities: ['Budowanie procesów akwizycji i przetwarzania danych wspierających modele AI/ML, z naciskiem na jakość i skalowalność.', 'Tworzenie modeli machine learning w środowiskach chmurowych, takich jak AWS lub GCP.', 'Finetuning i optymalizacja dużych modeli językowych (LLM) do różnych zastosowań biznesowych.', 'Współpraca z zespołem inżynierów chmurowych w celu integracji i wdrożenia modeli.', 'Monitorowanie wydajności oraz optymalizacja wdrożonych rozwiązań AI/ML.', 'Regularne raportowanie postępów i wyników projektów do zespołu oraz interesariuszy.', 'Utrzymanie najwyższych standardów etycznych i promowanie odpowiedzialnych praktyk w rozwoju AI.']\n",
      "requirements: ['Doświadczenie komercyjne w pracy z Computer Vision - warunek konieczny.', '3-letnie doświadczenie w branży.', 'Zaawansowana znajomość Pythona do przetwarzania danych i skryptowania.', 'Biegłość w zarządzaniu bazami danych SQL i ich odpowiednim zastosowaniu.', 'Silne umiejętności rozwiązywania problemów i dbałość o szczegóły.', 'Biegła znajomość języka angielskiego.', 'Komunikatywny język polski.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/machine-learning-engineer-with-computer-vision-krakow-barska-61,oferta,1003880770?apid=d7ad5f6f-3499-4169-8269-dba886c077a3&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 68 ---\n",
      "id: 121\n",
      "url: https://www.pracuj.pl/praca/machine-learning-engineer-with-llm-krakow-barska-61,oferta,1003880767\n",
      "title: Machine Learning Engineer with LLM\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 7 dnido 05 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Szukamy wielu kandydatówwakaty: 3']\n",
      "technologies: ['Python', 'SQL', 'Large Language Models', 'Infrastructure as Code', 'Data as Code', 'AWS', 'Azure', 'GCP', 'wewnątrz organizacji', 'koncentrujesz się na jednym projekcie', 'rozwijasz kilka projektów jednocześnie', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'agile', 'scrum']\n",
      "responsibilities: ['Budowanie procesów akwizycji i przetwarzania danych wspierających modele AI/ML, z naciskiem na jakość i skalowalność.', 'Tworzenie modeli machine learning w środowiskach chmurowych, takich jak AWS lub GCP.', 'Finetuning i optymalizacja dużych modeli językowych (LLM) do różnych zastosowań biznesowych.', 'Współpraca z zespołem inżynierów chmurowych w celu integracji i wdrożenia modeli.', 'Monitorowanie wydajności oraz optymalizacja wdrożonych rozwiązań AI/ML.', 'Regularne raportowanie postępów i wyników projektów do zespołu oraz interesariuszy.', 'Utrzymanie najwyższych standardów etycznych i promowanie odpowiedzialnych praktyk w rozwoju AI.']\n",
      "requirements: ['Doświadczenie komercyjne w pracy z Large Language Models - warunek konieczny.', '3-letnie doświadczenie w branży.', 'Zaawansowana znajomość Pythona do przetwarzania danych i skryptowania.', 'Biegłość w zarządzaniu bazami danych SQL i ich odpowiednim zastosowaniu.', 'Silne umiejętności rozwiązywania problemów i dbałość o szczegóły.', 'Biegła znajomość języka angielskiego.', 'Komunikatywny język polski.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/machine-learning-engineer-with-llm-krakow-barska-61,oferta,1003880767?apid=2745a3fe-b3a8-49ec-a32a-64a91b87db90&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 69 ---\n",
      "id: 122\n",
      "url: https://www.pracuj.pl/praca/it-solution-architect-warszawa,oferta,1003916761\n",
      "title: IT Solution Architect\n",
      "work_location: None\n",
      "validity: ważna jeszcze 23 dnido 21 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatów']\n",
      "technologies: ['AWS', 'SQL', 'u klienta', 'koncentrujesz się na jednym projekcie', 'możesz zmienić projekt', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'agile', 'data scientist', 'scrum master', 'UI designer', 'UX designer']\n",
      "responsibilities: ['Przejmowanie, rozwijanie i zarządzanie architekturą tworzonych rozwiązań', 'Tworzenie nowych architektur systemowych, uwzględniających wymagania biznesowe, istniejące systemy IT oraz standardy firmy', 'Zapewnianie wsparcia technicznego dla zespołów projektowych w zakresie: Integracji systemów, przetwarzania i modelowania danych oraz analizy i mapowania danych', 'Współpraca z zespołami biznesowymi (Product Owner, eksperci merytoryczni), technicznymi (architekci, managerowie, inżynierowie) oraz zespołami dostarczającymi rozwiązania (Scrum Leader, UI/UX, data engineers)']\n",
      "requirements: ['8+ lat doświadczenia w IT, w tym 3+ lata doświadczenia na stanowisku Solution Architect, Tech Lead, Lead Consultant lub podobnym', 'Zrozumienie zasad Data Governance i Data Quality', 'Znajomość integracji danych transakcyjnych i podstawowych', 'Modelowanie danych w relacyjnych bazach danych (normalizacja, indeksowanie, relacje)', 'Znajomość AWS oraz SQL', 'Język angielski na poziomie C1', 'Doświadczenie w branży produkcyjnej']\n",
      "application_link: https://www.pracuj.pl/aplikuj/it-solution-architect-warszawa,oferta,1003916761?apid=20fbb690-f7a3-4bb9-92ba-d3ebcbd53ca2&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 70 ---\n",
      "id: 123\n",
      "url: https://www.pracuj.pl/praca/data-solution-architect-snowflake-aws-platforms-warszawa-pulawska-2,oferta,1003871084\n",
      "title: Data Solution Architect (Snowflake/AWS Platforms)\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 2 dnido 28 lutego 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior), ekspert\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Snowflake Data Cloud', 'AWS', 'Data Vault', 'etl', 'Python', 'SQL', 'u klienta']\n",
      "responsibilities: [\"Prowadzenie projektowania i architektury technicznej rozwiązań dla produktów danych i pipeline'ów danych na platformach Snowflake i AWS, zapewniając skalowalność, jakość danych, bezpieczeństwo i wydajność.\"]\n",
      "requirements: [\"Co najmniej 5 lat doświadczenia w roli inżyniera danych lub analityka danych oraz dodatkowo 2+ lata doświadczenia w roli Architekta Rozwiązań Danych w dużych inicjatywach z zakresu hurtowni danych lub data lake, obejmujących pipeline'y danych i procesy ETL w kontekście rozwiązań Snowflake i AWS.\", \"Doświadczenie w pełnieniu roli lidera technicznego, koncentrującego się na projektowaniu i budowaniu produktów danych oraz rozwiązań pipeline'ów danych.\", 'Doświadczenie w zakresie architektury danych, w tym zastosowanie zasad Data Vault 2.0 w projektowaniu i architekturze.', \"Praktyczna znajomość optymalizacji i dostrajania pipeline'ów danych pod kątem wydajności i skalowalności.\", 'Doświadczenie w pracy z SQL, w tym zapytania i manipulacja danymi.', 'Praktyczna znajomość programowania w Pythonie.', 'Znajomość narzędzi DevOps, CI/CD oraz systemów integracji i ingercji danych.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-solution-architect-snowflake-aws-platforms-warszawa-pulawska-2,oferta,1003871084?apid=cfb1d183-ae1b-4928-bcc9-6c8d81db9253&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 71 ---\n",
      "id: 124\n",
      "url: https://www.pracuj.pl/praca/commercial-data-manager-szczecin-jacka-malczewskiego-26,oferta,1003931141\n",
      "title: Commercial Data Manager\n",
      "work_location: None\n",
      "validity: valid for a monthto 28 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: full office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Jacka Malczewskiego 26, SzczecinSzczecin, West Pomeranian', 'contract of employment']\n",
      "technologies: ['Salesforce']\n",
      "responsibilities: ['Establishing and enforcing data governance policies to uphold high data quality standards across various markets', 'Ensuring data is updated and well-structured by engaging with local teams and stakeholders, providing guidance and training as needed', 'Driving data ownership, accountability, and transparency within the commercial organization', 'Monitoring and auditing commercial data for accuracy, completeness, and compliance with internal and external requirements', 'Collaborating with cross-functional teams (Sales, Finance, IT, etc.) to align data standards and improve data consistency', 'Identifying and implementing process improvements for data management, including workflows and documentation', 'Supporting and leading data-related projects, from scoping and requirements gathering to execution and follow-up']\n",
      "requirements: ['3–5 years of experience in data management, data governance, or a similar field', 'A relevant educational background in Data Science, Statistics, Economics, Business, or a related discipline', 'Familiarity with master data management tools or CRM systems (e.g., SAP, Salesforce)', 'Strong analytical skills and proficiency in software such as Excel (advanced functions, pivot tables, etc.)', 'Excellent communication skills to collaborate effectively with global stakeholders and local market teams', 'Professional English (both written and spoken)', 'A detail-oriented mindset and the ability to adapt quickly to changing priorities']\n",
      "application_link: https://www.pracuj.pl/aplikuj/commercial-data-manager-szczecin-jacka-malczewskiego-26,oferta,1003931141?apid=3b5d822b-2d1f-478b-8b99-62a9cb5dd1ba&oca=None&acv=0\n",
      "\n",
      "--- Job Record 72 ---\n",
      "id: 125\n",
      "url: https://www.pracuj.pl/praca/architekt-danych-inzynier-danych-snowflake-warszawa-pulawska-2,oferta,1003871078\n",
      "title: Architekt Danych - Inżynier Danych (Snowflake)\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 2 dnido 28 lutego 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior), ekspert\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Snowflake Data Cloud', 'colibra', 'Immuta', 'data mesh', 'data vault', 'Python', 'u klienta']\n",
      "responsibilities: ['Opracowanie i ocena opcji architektonicznych dla różnych przypadków użycia oraz ich stałe dopasowywanie do technologicznego krajobrazu organizacji.', 'Przekształcanie wymagań biznesowych w produkty danych, zapewniając deweloperom jasne wytyczne do realizacji.', 'Wspieranie i kierowanie zespołem deweloperskim w implementacji architektonicznych rozwiązań oraz dostarczaniu insightów.', 'Wprowadzenie ram skutecznego zarządzania technicznego, zapewniających integralność i bezpieczeństwo danych.', 'Prowadzenie dyskusji z interesariuszami wewnętrznymi w celu zapewnienia zgodności architektury i produktów danych z wymaganiami dotyczącymi monitorowania umów oraz uzyskiwania insightów.', 'Definiowanie, egzekwowanie i stosowanie praktyk inżynierii danych oraz standardów w tworzeniu solidnych i łatwych do utrzymania pipeline’ów danych.', 'Organizowanie pipeline’ów do ładowania surowych danych oraz wspieranie interesariuszy biznesowych w definiowaniu nowych przypadków użycia produktów danych.', 'Przejęcie odpowiedzialności za pipeline’y produktów danych oraz ich utrzymanie, zapewniając niezawodność i wydajność.']\n",
      "requirements: ['Silne umiejętności organizacyjne, analityczne oraz doradcze.', 'Solidna znajomość koncepcji Data Mesh i Data Vault.', 'Doświadczenie z Data One Platform (Snowflake).', 'Zdolność do konwersji istniejących systemów legacy (PULSE) na projekt Data Vault z wykorzystaniem Snowflake, Immuta, Collibra oraz Cloud Security.', 'Zdolność do rozumienia i proponowania rozwiązań architektonicznych w istniejącej infrastrukturze danych.', 'Znajomość różnych technologii i ich możliwości analitycznych w złożonych środowiskach danych.', 'Dobra znajomość procesów biznesowych w DIA oraz danych związanych z tymi procesami (będzie to atut).', 'Doświadczenie na stanowisku Inżyniera Danych lub Architekta Danych.', 'Biegłość w transformacji danych (SQL, Python, Snowflake, dbt).', 'Doświadczenie w pracy w środowisku CI/CD (preferowany GitLab).', 'Wiedza w zakresie produktów danych, Data Mesh, architektury informacji oraz modelowania.', 'Znajomość formatów danych (API, ekstrakty, dane wprowadzane przez użytkowników itp.).', 'Dobra znajomość procesów pobierania, przechowywania i manipulacji danymi.', 'Doświadczenie w ustalaniu umów o udostępnianiu danych oraz umów SLA.', 'Doświadczenie w definiowaniu zasad obserwowalności oraz mechanizmów monitorowania produktów danych.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/architekt-danych-inzynier-danych-snowflake-warszawa-pulawska-2,oferta,1003871078?apid=a5e14fb7-8113-475f-abdd-34e71a9a082b&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 73 ---\n",
      "id: 126\n",
      "url: https://www.pracuj.pl/praca/migration-technical-leader-with-snowflake-dbt-warszawa-pulawska-2,oferta,1003871074\n",
      "title: Migration Technical Leader with Snowflake/DBT\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 2 dnido 28 lutego 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Snowflake Data Cloud', 'Informatica PowerCenter', 'Talend', 'DBT', 'u klienta']\n",
      "responsibilities: ['Bardzo dobre umiejętności komunikacyjne i prezentacyjne. Konieczność dostosowania przekazu do odbiorcy: członków zespołu, partnerów oraz interesariuszy. Lider techniczny pełni kluczową rolę w bardzo widocznym projekcie i oczekuje aktywnego udziału w spotkaniach z szeroką grupą odbiorców']\n",
      "requirements: ['Doświadczenie z rozproszonymi bazami danych, w szczególności Snowflake.', 'Zrozumienie optymalizacji Snowflake / najlepszych praktyk, aby móc oceniać i kierować decyzjami w odpowiednim kierunku.', 'Wysokie doświadczenie w DBT: to jest rdzeń kodu, w którym odbywają się skomplikowane transformacje, dlatego kandydat musi doskonale rozumieć Jinja i najlepsze praktyki przy pracy z DBT na Snowflake.', 'Doświadczenie w GitLab / CICD. Doświadczenie w DevOps / DataOps jest kluczowe. Zaletą będzie znajomość istniejących pipeline’ów CI/CD.', 'Doświadczenie z modelowaniem wymiarowym i migracją/konwersją do Data Vault, co jest istotne w kontekście wsparcia nowych produktów danych (nasze podejście do wewnętrznej siatki danych).', 'Doświadczenie z Informatica PowerCenter: ważne, aby rozumieć i dyskutować z dostawcami przy ocenie złożoności naszego starszego kodu. Idealny kandydat musi mieć doświadczenie z parametrami (zarówno w plikach, jak i zmiennych środowiskowych).', 'Doświadczenie z SAP BW i łącznością S3 jest dodatkowym atutem, podobnie jak doświadczenie z łącznością API z PowerCenter.', 'Doświadczenie w Talend: używamy Talend wyłącznie do onboardingu danych, więc ważne jest doświadczenie w konfiguracji różnych źródeł (używamy plików CSV, TXT, RDBMS (ODBC, JDBC), API, SAP BW oraz S3). Doświadczenie w najlepszych praktykach oraz uruchamianiu/zlecanie zadań Talend z innych aplikacji jest konieczne (używamy AutomateNow jako harmonogramera).', 'Doświadczenie w AutomateNow lub innych harmonogramerach korporacyjnych, takich jak UC4, Control-M lub podobne. Technologie te są bardzo podobne, więc doświadczenie w pracy z jednym z harmonogramerów korporacyjnych wystarczy, aby kandydat miał odpowiednie doświadczenie.', 'Doświadczenie w migracjach do chmury.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/migration-technical-leader-with-snowflake-dbt-warszawa-pulawska-2,oferta,1003871074?apid=896d12d5-515a-466b-a6e2-1f10c9ffe346&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 74 ---\n",
      "id: 127\n",
      "url: https://www.pracuj.pl/praca/data-engineer-with-gcp-warszawa-pulawska-2,oferta,1003871069\n",
      "title: Data Engineer with GCP\n",
      "work_location: None\n",
      "validity: valid for 2 daysto 28 February 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Puławska 2, Mokotów, WarszawaWarszawa, Masovian']\n",
      "technologies: ['ETL', 'Informatica PowerCenter', 'Python', 'Google Cloud Platform', 'Rust', \"at the client's site\"]\n",
      "responsibilities: ['Design and optimize ETL processes for on-prem and GCP environments', 'Develop and expose REST APIs for seamless data access', 'Manage data pipelines and scheduling with tools like Informatica Power Center and Apache AirFlow']\n",
      "requirements: ['5+ years of experience in ETL development for data integration on on-premise systems', 'Expertise with relational databases', 'Ability to develop and expose RESTful APIs', '3+ years of experience as a Data Engineer', 'Proven experience with Google Cloud Platform', 'Advanced proficiency in Python for data processing and analysis']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-with-gcp-warszawa-pulawska-2,oferta,1003871069?apid=cdc9d940-9a0a-4075-8dee-bbbaf5093fca&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 75 ---\n",
      "id: 128\n",
      "url: https://www.pracuj.pl/praca/web-analytics-expert-warszawa,oferta,1003873456\n",
      "title: Web Analytics Expert\n",
      "work_location: Company locationWarszawaWarszawa, Masovian\n",
      "validity: valid for 3 daysto 01 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior), expert\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['Google Analytics', 'Google Tag Manager', 'Snowplow', 'BigQuery', 'Kafka', 'JavaScript', \"at the client's site\", 'you focus on a single project at a time', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you focus on product development', 'agile', 'code review']\n",
      "responsibilities: ['Usage of Google Tag Manager (copying of Google Analytics 4 tags and triggers) for collecting the data with Snowplow', 'Creating GA4 events in Server-Side Google Tag Manager for Google Ads on the basis of Snowplow events is needed', 'Enriching of Snowplow online event data with Cart- and Order -service data from a webshop backend']\n",
      "requirements: ['Expertise in Google Analytics', 'Knowledge of Google Tag Manager', 'Knowledge of Snowplow', 'Knowledge of BigQuery', 'Knowledge of Kafka backend data', 'Fluent English']\n",
      "application_link: https://www.pracuj.pl/aplikuj/web-analytics-expert-warszawa,oferta,1003873456?apid=dfc4362d-eded-47aa-b87a-b60e7f5341e8&oca=None&acv=0\n",
      "\n",
      "--- Job Record 76 ---\n",
      "id: 129\n",
      "url: https://www.pracuj.pl/praca/administrator-hurtowni-danych-warszawa,oferta,1003930969\n",
      "title: Administrator Hurtowni Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 28 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SAS', 'Shell', 'SQL', 'u klienta', 'agile', 'scrum', 'kanban']\n",
      "responsibilities: ['Utrzymywanie i rozwijanie procesów integracji oraz raportowania w Hurtowni Danych', 'Monitorowanie jakości danych i zapewnienie ich poprawności', 'Tworzenie narzędzi usprawniających pracę zespołu, w tym modułów wielokrotnego użytku (SAS, SAS Viya, Oracle i in.)', 'Wdrażanie nowych rozwiązań, optymalizacja procesów przetwarzania danych', 'Wspieranie użytkowników w diagnozowaniu problemów i ich zgłaszaniu do odpowiednich zespołów']\n",
      "requirements: ['Min. 3-letnie doświadczenie komercyjne w roli Administratora Hurtowni Danych/pokrewnej', 'Doświadczenie w pracy z technologiami SAS Institute (SAS Data Integration Studio, SAS Management Console, SAS Viya)', 'Dobra znajomość Unix/Linux,', 'Praktyczna znajomość Shell (BASH/SH) oraz SQL', 'Wykształcenie wyższe (informatyka lub pokrewne) albo status studenta ostatnich lat', 'Komunikatywność i umiejętność współpracy z użytkownikami biznesowymi']\n",
      "application_link: https://www.pracuj.pl/aplikuj/administrator-hurtowni-danych-warszawa,oferta,1003930969?apid=2c54e552-9730-4062-9c8c-9df61d89c392&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 77 ---\n",
      "id: 130\n",
      "url: https://www.pracuj.pl/praca/data-solution-architect-warszawa-dluga-29,oferta,1003873349\n",
      "title: Data Solution Architect\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 3 dnido 01 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Snowflake', 'AWS', 'SQL', 'Python', 'u klienta', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu']\n",
      "responsibilities: ['Kierowanie architekturą techniczną i architekturą danych, a także projektowanie produktów danych i rozwiązań potoków danych na platformach Snowflake i AWS, zapewniając skalowalność, jakość danych, bezpieczeństwo i wydajność', 'Prowadzenie modelowania jednostek i relacji, zapewniając dokładną reprezentację i integrację w ekosystemie danych', 'Zapewnianie wskazówek technicznych i mentoringu zespołowi', 'Współpraca z zespołami wielofunkcyjnymi, w tym z właścicielem produktu danych i interesariuszami, w celu dostosowania rozwiązań technicznych do wymagań dotyczących jakości danych i wzorców danych użytkowych', 'Kierowanie rozwojem i realizacją technicznych roadmap dla produktów danych opartych na AWS i Snowflake', 'Nadzorowanie procesu zapewniania jakości danych, w tym przeglądów kodu, dostępności, testów i dokumentacji, w aby zapewnić solidność i niezawodność produktów opartych na danych']\n",
      "requirements: ['Co najmniej 5 lat doświadczenia w pracy z danymi, w tym minimum 2 jako Data Solution Architect', 'Bardzo dobra wiedza z obszarów: budowania architektury i produktów danych, a także rozwiązań pipelinów w Snowflake i/lub AWS', \"Doświadczenie w optymalizacji i dostrajaniu pipeline'ów danych pod kątem wydajności i skalowalności\", 'Znajomość SQL, Python, zasad Data Vault 2.0 (w projektowaniu i architekturze)', 'Język angielski umożliwiający swobodną komunikację']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-solution-architect-warszawa-dluga-29,oferta,1003873349?apid=a13de357-7d5d-42d0-a224-57e60f3d1f5d&oca=None&acv=0\n",
      "\n",
      "--- Job Record 78 ---\n",
      "id: 131\n",
      "url: https://www.pracuj.pl/praca/splunk-sme-krakow,oferta,1003895960\n",
      "title: Splunk SME\n",
      "work_location: None\n",
      "validity: valid for 14 daysto 12 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KrakówKraków, Lesser Poland', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Splunk', 'Python', 'Shell', 'agile', 'industry-specific e-learning platforms']\n",
      "responsibilities: ['Designing and developing advanced, multi-layer Splunk dashboards tailored to business needs', 'Creating interactive visualizations that enhance data-driven decision-making', 'Implementing advanced Splunk knowledge objects such as reports, searches, and alerts', 'Troubleshooting and resolving performance issues in Splunk dashboards and queries', 'Optimizing complex SPL queries for efficiency and scalability', 'Integrating multiple data sources and APIs for seamless data aggregation in Splunk', 'Ensuring accurate data ingestion, parsing, and transformation for dashboards', 'Collaborating with stakeholders to gather requirements and deliver effective solutions', 'Conducting user training and creating documentation for proper dashboard usage', 'Acting as the primary expert for Splunk-related inquiries and challenges']\n",
      "requirements: ['Extensive experience with Splunk Enterprise and Cloud, including dashboard development and administration', 'Deep understanding of SPL (Search Processing Language) and advanced search techniques', 'Proven expertise in designing multi-layered, interactive dashboards', 'Strong experience in troubleshooting and optimizing Splunk dashboards and queries', 'Proficiency in integrating multiple data sources and endpoints into Splunk', 'Familiarity with data ingestion, parsing, and indexing processes', 'Excellent problem-solving skills and attention to detail', 'Strong communication and collaboration abilities', 'Experience with scripting languages such as Python or Shell, and REST API integration']\n",
      "application_link: https://www.pracuj.pl/aplikuj/splunk-sme-krakow,oferta,1003895960?apid=b100c19b-1111-49f4-9e8d-3b1512f57e7c&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 79 ---\n",
      "id: 132\n",
      "url: https://www.pracuj.pl/praca/data-analyst-uk-cmb-transformation-krakow-kapelanka-42a,oferta,1003899087\n",
      "title: Data Analyst (UK CMB Transformation)\n",
      "work_location: None\n",
      "validity: valid for 15 daysto 13 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['Jira', 'Confluence']\n",
      "responsibilities: ['Influence and collaborate with stakeholders and business partners, building strong relationships to ensure consensus and influence change outcomes. Foster open and honest communication which anticipates stakeholder expectations.', 'Promote adherence to project/programme governance model and use defined controls and standards as outlined in Change Framework. Follow the pace and operating rhythm, driving a culture of high performance, and ensure pace by identifying and removing barriers to programme success. Be responsible for ensuring end to end delivery of requirements and agreed acceptance criteria on behalf of the business.', 'Develop and implement plans to drive digital adoption of our products/services; collaborate with key stakeholders to create adoption campaigns, materials, and resources; conduct regular analysis of adoption metrics to identify trends, opportunities, and areas of improvement; take ownership around governance activities, supporting the digital programme with mandatory reporting items as part of effective governance; generate reports and dashboards to communicate adoption metrics and insights to stakeholders; identify barriers to adoption and propose solutions to overcome them; work closely with internal frontline teams on adoption best practices and metric tracking tools.', 'Analyse current state legacy solutions/operating models against target model to identify gaps and opportunities to support an effective transition towards the target state. Conduct analysis using proven tools and techniques to plan, facilitate, validate, prioritise and otherwise lead problem statement definition, solution design, elaboration of requirements, etc.', 'Support Technology Delivery and IT Operations with the translation of business requirements and designs into more detailed functional requirements and designs, often representing the voice of the customer and business in these discussions, bridging the gap between product, technology, propositions, markets, business, and delivery teams.', 'Conduct early change planning and audience analysis through to designing and delivering change interventions (e.g. communications, training, support, organisation alignment); and tracking and taking actions on change readiness, adoption, and feedback. Source and interpret data and insights to underpin the feasibility and design of digital features and functions.', 'Act as a role model to foster a collaborative team environment which supports and encourages professionalism and development. Continuously support capability build of the Business Analysis Practice i.e., through advocating community events, sharing relevant articles / studies / ideas, ensuring regular development, and experience exchange with Transformation community.', 'Promote the value of challenging the status quo, seeking to improve ways of working and having a forward-thinking mind-set, in line with the Group’s values and strategy.']\n",
      "requirements: ['Excellent understanding of the change lifecycles, including 3+ years of working knowledge of Waterfall and Agile Project Management methodologies, ability to apply best practices. Excellent understanding of Change Framework and best practice techniques, ability to apply Change Management and Implementation Management techniques and approaches.', 'Knowledge of data analysis techniques and experience of using tools to create insightful reports and dashboards and support the delivery of customer centric digital products and features. Anticipation and identification of risks, ensuring appropriate steps are taken to mitigate and manage them.', 'Proven track record in engaging with the frontline to adopt change. Extensive business knowledge or banking experience, knowledge of the marketplace and the competition (advantage). Working experience with business stakeholders and product managers on product backlogs. Strong experience of managing senior stakeholders. Track record of building sustainable relationship with diverse stakeholder groups.', 'Skills: Excellent written and spoken communication skills; ability to communicate with impact, ensuring complex information is articulated in a meaningful way to wide and varied audiences. Influencing skills and ability to build positive working relationships with colleagues and stakeholders. Strong facilitation and public speaking skills.', 'Commercial and critical thinking with ability and confidence to challenge status quo; strong analytical and problem-solving skills, with the ability to interpret data and draw actionable insights and proposing solutions when escalating.', 'Excellent organizational skills to manage the unexpected as well as anticipated issues or events, so that success can still be achieved; strong time management, ability to prioritise and manage multiple tasks, working independently. A track record of constantly looking for improvement, with a ‘can do’ proactive approach.', 'Capabilities (critical):  Change & Implementation, Customer centricity, Stakeholder management, Data analysis & insight, MS Office (Excel, Power Point, Word); Confluence; Jira.', 'Qualifications and Accreditations: (i) Master’s degree (e.g., MBA) in related field or commensurate practical experience; (ii) CBAP, Change & Implementation, Lean Six Sigma (desirable); (iii) Agile BA, SAFe, TOGAF (optional).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-analyst-uk-cmb-transformation-krakow-kapelanka-42a,oferta,1003899087?apid=16b283e4-acc9-474c-a850-b43bfaaf1bcd&oca=None&acv=0\n",
      "\n",
      "--- Job Record 80 ---\n",
      "id: 133\n",
      "url: https://www.pracuj.pl/praca/data-engineer-krakow-aleja-29-listopada-20,oferta,1003918981\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['aleja 29 Listopada 20, Stare Miasto, KrakówKraków, Lesser Poland', 'contract of employment']\n",
      "technologies: ['Python', 'SQL', 'Microsoft Azure', 'Apache Airflow', 'Kafka', 'Java', 'Snowflake Data Cloud']\n",
      "responsibilities: ['Create data pipelines to ingest data from dissimilar sources (performance, reliability, monitoring).', 'Server data models as a product to the entire organisation through implementation and debugging.', 'Collaborate with the other teams to address data sourcing and provision requirements.', 'Coordinate with the Product & Technology teams to ensure all platforms collect and provide appropriate data.', 'Liaise with the other Data & Analytics teams to ensure reporting and analytics needs can be addressed by the central data lake.', 'Support the Data Quality and Security initiatives by building into the architecture the necessary data access, integrity, and accuracy controls']\n",
      "requirements: ['3+ years of experience in Data Engineering', '3+ years of experience in ETL/ELT development in cloud env.', 'Degree in Computer Science, Software Development, Engineering , or a related technical field.', 'Proficient in Python and SQL', 'Understanding of RDBMS, Columnar and NoSQL engines & performance', 'Experience with cloud architecture and tools: Microsoft Azure, Amazon or GCP', 'Experience with orchestration tools such as Apache AirFlow or UC4', 'Understanding of distributed logging platforms', 'Familiarity with DWH modeling will be considered an advantage.', 'Familiarity with DevOps methodologies and concepts will be considered an advantage', 'Background in stream data processing technologies such as NiFi, Kinesis, Kafka will be considered an advantage', 'Exposure to Java will be considered an advantage', 'Prior exposure to the Snowflake ecosystem will be considered an advantage']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-krakow-aleja-29-listopada-20,oferta,1003918981?apid=87f09f7f-b28f-4086-a0b8-797f8efcd05e&oca=None&acv=0\n",
      "\n",
      "--- Job Record 81 ---\n",
      "id: 134\n",
      "url: https://www.pracuj.pl/praca/solutions-architect-ai-warszawa,oferta,1003916379\n",
      "title: Solutions Architect (AI)\n",
      "work_location: None\n",
      "validity: valid for 23 daysto 21 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'More than one vacancy']\n",
      "technologies: ['Snowflake', 'SQL', 'Microsoft Power BI', 'AWS', 'Matillion', \"at the client's site\", 'you focus on a single project at a time', 'you can change the project', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'agile']\n",
      "responsibilities: ['Design and architect end-to-end data and AI platforms, integrating data lakes, warehouses, and advanced analytics', 'Develop and maintain reference architectures, blueprints, and best practices for scalable data solutions', 'Implement ML and Gen AI platforms, enabling end-to-end AI model lifecycles (from development to deployment and monitoring)', 'Ensure compliance with security, governance, and IT policies in all AI and data platform solutions', 'Lead proof-of-concept initiatives to assess emerging data and AI technologies']\n",
      "requirements: ['8+ years of experience in Big Data, Distributed Systems, and Cloud Architectures (preferably AWS)', '3+ years of experience with Databricks and strong knowledge of Snowflake, dbt, and Matillion (preferably)', 'Proven experience designing AI/ML solutions, MLOps, and Gen AI architectures', 'Deep understanding of SQL and NoSQL databases, data integration, and modern cloud infrastructure', 'Experience with data lake, data mesh, and data fabric architectures', 'Strong communication skills to bridge technical and business teams', 'Fluent English - at least B2+/C1']\n",
      "application_link: https://www.pracuj.pl/aplikuj/solutions-architect-ai-warszawa,oferta,1003916379?apid=57af2a26-7ec3-4bb8-9c96-f66cd4e04974&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 82 ---\n",
      "id: 135\n",
      "url: https://www.pracuj.pl/praca/paid-internship-electronic-data-interchange-edi-processes-support-with-german-warszawa-jutrzenki-105,oferta,1003930826\n",
      "title: Paid Internship Electronic Data Interchange (EDI) processes support (with German)\n",
      "work_location: None\n",
      "validity: valid for a monthto 28 March 2025\n",
      "contract_type: None\n",
      "employment_type: None\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Jutrzenki 105, Włochy, WarszawaWarszawa, Masovian', 'contract of mandate', 'part time', 'trainee', 'Запрошуємо працівників з України']\n",
      "technologies: ['SAP']\n",
      "responsibilities: ['Taking ownership of EDI customer requirements including the implementation of new IT requirements, change requests, testing, consulting on the EDI invoicing process, especially self-billing process', 'Maintain EDI technical documentation and mapping specifications (e.g., VDA, EDIFACT) for the EDI integration between SAP, EDI Manager and trade partners', 'Cooperate with process specialists to understand the business processes and define IT-related business requirements', 'Provide EDI support to internal departments, including verifying transmissions, troubleshooting issues, and conducting root cause analysis', 'Communicate with external customers regarding EDI technical topics', 'Managing technical setups in ERP']\n",
      "requirements: ['Student of IT related fields;', 'Avaiability of min. 35h per week', 'Interest in learning about SAP Sales and Distribution (SD) and Electronic Data Interchange (EDI) processes', 'Eagerness to gain knowledge of data exchange standards such as VDA, ODETTE, EDIFACT, and SAP IDOCs', 'Detail-oriented with a commitment to quality', 'Strong analytical and problem-solving skills', 'Fluent Polish & English, German language is a strong advantage']\n",
      "application_link: https://www.pracuj.pl/aplikuj/paid-internship-electronic-data-interchange-edi-processes-support-with-german-warszawa-jutrzenki-105,oferta,1003930826?apid=594c4787-b446-42cc-97c5-c1721ed46ef9&oca=None&acv=0\n",
      "\n",
      "--- Job Record 83 ---\n",
      "id: 136\n",
      "url: https://www.pracuj.pl/praca/data-qa-automation-krakow-aleja-29-listopada-20,oferta,1003918834\n",
      "title: Data QA Automation\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['aleja 29 Listopada 20, Stare Miasto, KrakówKraków, Lesser Poland', 'contract of employment']\n",
      "technologies: ['Apache Airflow', 'Snowflake Data Cloud', 'Kafka', 'Python', 'Ansible', 'SQL', 'PostgreSQL', 'Tableau']\n",
      "responsibilities: ['Establish Data Quality automation frameworks, tools, and processes', 'Design and implement scalable and robust data-focused test frameworks and tools across all the stages of the data lifecycle – from ingestion through transformation to consumption', 'Develop high-quality, automated data-centric tests using Airflow and dbt', 'Creating thorough test strategies, test plans, and test cases for all Data features and products', 'Participate in the data development lifecycle from research, technical design to implementation and maintenance', 'Collaborate with the DevOps team to integrate automation test frameworks and tools into CI/CD', 'Collaborate with the other development and product teams to define high-value test strategies and test cases on large datasets and transformations to determine data quality and data integrity', 'Rigorously test data infrastructure components to comply with privacy regulations including the EU GDPR, PCI, etc.', 'Participate in security audits and implement remedial actions', 'Learn to identify opportunities to compound the growth and efficiency in testing and automation', 'Drive innovation by constantly learning about new automation technologies and methodologies, along with experimentation']\n",
      "requirements: ['3+ years of work experience in testing automation - preferably in a role related to data warehouses, databases, ETL/ELT and/or data visualisation', 'Experience in testing applications using frameworks like Cypress, Selenium, and Appium', 'Prior use of Apache Airflow, Data Build Tool, Snowflake, Kafka, Ansible and other data specific tools and applications', 'Proficient in Advanced SQL and Python', 'Experienced with data warehouses like Snowflake, BigQuery, RedShift or equivalent', 'Experienced with databases like Microsoft SQL, PostgreSQL', 'Experienced with data visualization tools like Power BI, Metabase, QlikSense, Tableau, SIsense or equivalent', 'Understanding of software application testing best practices and philosophies with an emphasis around data integrity and data quality', 'Familiarity with data streaming and/or event-driven data pipelines will be considered an asset', 'Clear English written and spoken business communication skills']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-qa-automation-krakow-aleja-29-listopada-20,oferta,1003918834?apid=c5b24da9-8aa6-4276-9795-3f2fcdc68010&oca=None&acv=0\n",
      "\n",
      "--- Job Record 84 ---\n",
      "id: 137\n",
      "url: https://www.pracuj.pl/praca/business-analyst-data-oriented-krakow-kapelanka-42a,oferta,1003921914\n",
      "title: Business Analyst (Data Oriented)\n",
      "work_location: None\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['Jira', 'Confluence']\n",
      "responsibilities: ['Ability to understand data dictionaries, data models and data integration techniques to undertake complex data analysis across multiple large data sets, leveraging data analysis software, and building into concise findings for the ability to play back to senior leadership.', 'Defines requirements management processes, designs detailed business solutions and/or structures based on business requirements, and ensures requirements traceability through end-to-end process, challenges business intent in a constructive manner. Supports reengineering of processes, where process is one of the many parts of the change, and provides detailed guidance on process design (considering risk, end-to-end and cost).', 'Employs an improvement mindset to identify and define issues or problems that are less obvious; participates actively and constructively in brainstorming meetings where problems are discussed and / or resolved; Uses systemic thinking and creativity in devising solution options', 'Articulates or translates complex information in clear, meaningful and structured way to suit audience. Anticipates issues and risks and acts to mitigate these quickly, any unforeseen roadblocks are handled swiftly and effectively; manages review processes to identify quality issues early.', 'Understands the Group’s priorities, business drivers, competitors and competitive strategy to help drive strategically aligned solutions, considering aspects of risk/reward. Questions small-scale business decisions that do not demonstrate alignment to the Group’s commercial strategy. (Evaluates relative costs, benefits and obstacles of potential solutions before implementing.', 'Identifies areas of impact on the target operating model and designs activities to mitigate impact. Utilises financial skills to develop a high-level business case, considering investment and high level benefits.', 'Manages the change implementation activities, including business readiness assessment (i.e. avoiding multiple change clashes), implementation pilot (including approach and checklists) and actual implementation (including reporting status and issues).', 'Manages the change audience through the commitment curve with communications, training and development. Supports the programme management with impact assessment for change requests on design, scope, time, budget or effort on programme plan and provides accurate estimates for handling new requirements, design changes and scope additionally supports resourcing activities.']\n",
      "requirements: ['Knowledge', 'Good knowledge of HSBC Change Frameworks and delivery methodology including Agile techniques. Outstanding understanding of HSBC Group structures, processes, and objectives.', 'Strong knowledge of the external environment – regulatory, political, competitors etc.', 'Process management and re-engineering knowledge. Change management and implementation management techniques and approaches.', 'Business analysis, requirements gathering and design techniques. Strong understanding of data analysis methodologies, including data dictionaries, models and integration.', 'Experience', 'Proven track record as an analyst. Experience gathering requirements and conducting design activity, with a mix of business, operations and technology focused projects.', 'Experience with data dictionaries, data models and integration techniques. Proven ability at analysing complex, large data sets to draw clear, concise conclusions for play back to senior leadership.', 'Exposure to business case development and a sound understanding of how design enablers underpin business benefits.', 'Strong experience of delivering change into different audiences and managing implementation in banking environments (branch, contact centre, trading floor, operations, head office etc).', 'Excellent decision making and problem-solving ability', 'Proven ability to work across regions whilst maintaining a global perspective with senior stakeholders and business sponsors', 'Capabilities', 'Agile expertise', 'Business Analysis & Design', 'Data analysis on large, complex data sets', 'Business Case and Benefits Realisation', 'Delivery at Pace', 'Managing Change and Implementation', 'Process Re-engineering', 'Problem Solving and Critical Thinking', 'Engaging with Customers, Stakeholders and Colleagues', 'Working in a Dynamic Environment', 'Working Responsibly']\n",
      "application_link: https://www.pracuj.pl/aplikuj/business-analyst-data-oriented-krakow-kapelanka-42a,oferta,1003921914?apid=850c3fa4-bc54-41c9-85f5-06783d942a64&oca=None&acv=0\n",
      "\n",
      "--- Job Record 85 ---\n",
      "id: 138\n",
      "url: https://www.pracuj.pl/praca/ai-solutions-developer-net-or-python-lodz,oferta,1003918818\n",
      "title: AI Solutions Developer (.NET or Python)\n",
      "work_location: Company locationŁódźŁódź, Łódź\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['.NET', 'Python', 'Microsoft Azure']\n",
      "responsibilities: ['Develop AI-driven solutions using Microsoft Azure, specifically leveraging AI services such as Document Intelligence Studio and Computer Vision', 'Design and implement machine learning models to enhance automation and data processing capabilities', 'Ensure seamless integration of AI solutions within the existing banking infrastructure', 'Write clean, efficient, and maintainable code in .NET or Python', 'Conduct unit testing and automate testing processes to ensure software reliability', 'Analyze user requirements and translate them into scalable AI solutions', 'Collaborate with cross-functional teams to propose and implement AI-driven improvements', 'Maintain up-to-date knowledge of AI and machine learning trends to enhance solution capabilities', 'Optimize system performance and ensure compliance with security best practices', 'Provide documentation and training to internal teams on newly developed AI functionalities']\n",
      "requirements: ['Strong experience in software development using .NET or Python', 'Hands-on experience with Microsoft Azure AI services, including Document Intelligence Studio and Computer Vision', 'Basic understanding of machine learning and AI principles', 'Ability to work independently and propose AI solutions tailored to project needs', 'Experience in modular testing and test automation', 'Knowledge of user requirement analysis methodologies', 'Familiarity with system design methodologies', 'Strong problem-solving and analytical skills', 'Ability to work in a fast-paced environment and manage multiple tasks', 'Excellent communication and collaboration skills']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-solutions-developer-net-or-python-lodz,oferta,1003918818?apid=b4b0b8ed-6e56-4caa-afa1-a39b9d1f79f4&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 86 ---\n",
      "id: 139\n",
      "url: https://www.pracuj.pl/praca/database-developer-krakow-aleja-29-listopada-20,oferta,1003918824\n",
      "title: Database Developer\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['aleja 29 Listopada 20, Stare Miasto, KrakówKraków, Lesser Poland', 'contract of employment']\n",
      "technologies: ['SQL', 'Git']\n",
      "responsibilities: ['Design stable, reliable and effective databases', 'Develop database structures, stored procedures, functions, reports, scripts etc.', 'Optimize and maintain legacy systems', 'Modify databases according to requests and perform tests', 'Solve database performance, business logic issues', 'Liaise with developers to improve applications and establish best practices', 'Gather user and product requirements and identify new features', 'Provide data management support to users', 'Ensure all database logic meet company and performance requirements', 'Research and suggest new database products, services and protocols']\n",
      "requirements: ['At least 3 years’ experience in developing and administrating complex databases', 'Bachelor’s Degree in Computer Science or related field', 'Proven work experience as a Database developer preferably using MSSQL', 'Hands on experience with T-SQL, developing complex database structures, stored procedures, functions, reports etc. according to company requirements.', 'Good understanding of data management (e.g. permissions, security and monitoring)', 'Basic understanding of Git version control system', 'Excellent analytical and organization skills', 'Ability to understand requirements and a problem-solving attitude.', 'Excellent verbal and written communication skills', 'English level: Intermediate — reading/writing documents, daily meetings with teams', 'Team player']\n",
      "application_link: https://www.pracuj.pl/aplikuj/database-developer-krakow-aleja-29-listopada-20,oferta,1003918824?apid=49bad92e-ae89-40aa-99f8-a4aeb967c5ef&oca=None&acv=0\n",
      "\n",
      "--- Job Record 87 ---\n",
      "id: 140\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-warszawa-jana-nowaka-jezioranskiego-53a,oferta,1003921883\n",
      "title: Senior Data Engineer\n",
      "work_location: Company locationJana Nowaka-Jeziorańskiego 53a, Praga-Południe, WarszawaWarszawa, Masovian\n",
      "validity: valid for over a monthto 07 April 2025\n",
      "contract_type: contract of employment, contract of mandate, B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'Запрошуємо працівників з України']\n",
      "technologies: ['Microsoft Azure', 'Google Cloud Platform', 'AWS', 'Python', 'SQL', 'PostgreSQL', 'Microsoft SQL Server', 'Apache Spark', 'Azure DevOps', 'Apache Airflow', 'in house', \"at the client's site\", 'you focus on a single project at a time', 'you develop several projects simultaneously', 'you can change the project', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you develop the code \"from scratch\"', 'you focus on product development', 'agile', 'scrum']\n",
      "responsibilities: ['Designing, implementing, and optimizing modern cloud-based solutions.', 'Building and launching new data models and data pipelines.', 'Implementing best practices in data engineering including data integrity, quality, and documentation.', 'Optimization of existing analytical solutions.', 'Leading small size teams of engineers and being a role model.']\n",
      "requirements: ['4+ years of experience delivering complex data warehouse / data lake / business intelligence solutions.', '2+ years of experience working with cloud services (Azure / GCP / AWS).', 'Knowledgeable and experienced in Python.', 'Experience in SQL and data analysis, knowledge of relational databases (preferably SQL Server, PostgreSQL).', 'Knowledge of public cloud architecture, security, networking concepts and best practices (MS Azure preferred).', 'Knowledge of DWH data modeling practices and ETL/ELT development.', 'Conceptual and analytical skills – the ability to define, analyze and document complex business and technical requirements.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-warszawa-jana-nowaka-jezioranskiego-53a,oferta,1003921883?apid=82802a7b-de63-4aac-8c2d-41e410bc3d5a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 88 ---\n",
      "id: 141\n",
      "url: https://www.pracuj.pl/praca/treasury-data-engineer-warszawa-tadeusza-rejtana-17,oferta,1003873171\n",
      "title: Treasury Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 3 daysto 01 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Tadeusza Rejtana 17, Mokotów, WarszawaWarszawa, Masovian']\n",
      "technologies: ['SQL', 'Python', \"at the client's site\", 'you have influence on the product', 'you focus on product development', 'agile']\n",
      "responsibilities: ['Build and maintain efficient data pipelines between key platforms and SAP systems.', 'Develop and manage ETL processes to ensure smooth and accurate data integration.', 'Work closely with IT and business teams to ensure data is structured and accessible.', 'Use SQL, Python, and other tools to analyze and optimize data.', 'Collaborate with stakeholders to understand business needs and deliver data-driven solutions.', 'Maintain clear technical documentation for processes and pipelines.', 'Ensure data quality, accuracy, and governance for financial reporting.', 'Support balance sheet and liquidity reporting with accurate data models.', 'Identify opportunities for process improvements and automation.']\n",
      "requirements: ['5+ years of experience in similar role, with  2+ years in Data Engineering.', 'Experience with data management and transformation projects.', 'Hands-on knowledge of SQL, Python, and data engineering tools.', 'Strong communication and collaboration skills to work with different teams.', 'A proactive, problem-solving mindset with great attention to detail.', 'Previous experience in financial services or banking.', 'Ability to work independently while also being a great team player.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/treasury-data-engineer-warszawa-tadeusza-rejtana-17,oferta,1003873171?apid=2b994b22-a401-4923-bf36-7902ff48ff0d&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 89 ---\n",
      "id: 142\n",
      "url: https://www.pracuj.pl/praca/database-administrator-dba-data-platform-engineer-warszawa-stanislawa-zaryna-2b,oferta,1003891856\n",
      "title: Database Administrator / DBA / Data Platform Engineer\n",
      "work_location: None\n",
      "validity: valid for 11 daysto 09 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Stanisława Żaryna 2B, Mokotów, WarszawaWarszawa, Masovian']\n",
      "technologies: ['SQL', 'Microsoft Azure', 'ETL', 'Microsoft SQL Server', 'agile', 'scrum', 'technical knowledge exchange within the company']\n",
      "responsibilities: ['Implement Data Platform initiatives.', 'Monitor and maintain Data Services components.', 'Constantly improve SQL performance.', 'Optimize data in Data Services to achieve storage reliability and handle data growth.', 'Implement security practices in Data Services.', 'Work with Development team to optimize data structures and improve application performance.', 'Work with BI team to help with OPS changes.', 'Work with users to implement change management.']\n",
      "requirements: ['Proficiency in any modern programming language with at least 2 years usage on commercial projects.', 'At least 5 years of experience as a DBA or Data Platform engineer with strong understanding of relational (MS SQL Server) and non-relational databases.', 'At least 3 years of experience building and maintaining data pipelines and ETL processes on custom technology or any public like Kafka, Apache Hadoop etc.', 'At least 3 years of experience in data modeling and architecture including designing data schemas, data warehousing and data streaming in HA environment.', 'Experience in infrastructure management for data platform on on-prem and cloud (MS Azure).', 'Experience in implementing data security measures and ensuring compliance with relevant regulations including DRP and BCP for data platform.', 'Great collaboration and communication skills.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/database-administrator-dba-data-platform-engineer-warszawa-stanislawa-zaryna-2b,oferta,1003891856?apid=f67816d3-33de-40ff-8f88-2594a2903de5&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 90 ---\n",
      "id: 143\n",
      "url: https://www.pracuj.pl/praca/senior-ai-engineer-warszawa-jasna-14-16a,oferta,1003930660\n",
      "title: Senior AI Engineer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze miesiącdo 28 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Python', 'AWS', 'wewnątrz organizacji', 'koncentrujesz się na jednym projekcie', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'koncentrujesz się na utrzymaniu kodu', 'agile', 'scrum']\n",
      "responsibilities: ['Realizacja zadań developerskich w oparciu o technologie: Python, AutoGen, LangGraph, Crew.AI, LangChain, AWS AI Foundry, Amazon Bedrock', 'Projektowanie i implementacja wysokowydajnych rozwiązań', 'Stała współpraca z członkami zespołu', 'Aktywny udział w inicjatywach poprawiających procesy wytwarzania oprogramowania w organizacji', 'Proaktywne identyfikowanie i rozwiązywanie problemów technicznych oraz wspieranie zespołu w kryzysowych sytuacjach', 'Wspieranie zespołu w pracy nad kluczowymi elementami projektu, mentoring oraz egzekwowanie najlepszych praktyk w zakresie rozwoju oprogramowania']\n",
      "requirements: ['Masz co najmniej 7 lat doświadczenia komercyjnego w Python', 'Znasz frameworki: LangChain, LangGraph do budowy modułowych systemów AI', 'Posiadasz doświadczenie w pracy z NLP, klasyczną sztuczną inteligencją, uczeniem maszynowym', 'Masz doświadczenie z rozwiązaniami opartymi na modelach językowych', 'Posiadasz 4 lata doświadczenia w pracy z chmurowymi platformami AI', 'Masz pierwsze doświadczenie w roli Liderskiej', 'Dobrze i swobodnie komunikujesz się w języku angielskim - min. poziom B2 (praca w zespole międzynarodowym)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-ai-engineer-warszawa-jasna-14-16a,oferta,1003930660?apid=9bd30246-7881-4166-ba20-a6927e990efb&oca=None&acv=0\n",
      "\n",
      "--- Job Record 91 ---\n",
      "id: 144\n",
      "url: https://www.pracuj.pl/praca/fullstack-developer-ai-gdansk,oferta,1003916001\n",
      "title: Fullstack Developer (AI)\n",
      "work_location: None\n",
      "validity: ważna jeszcze 23 dnido 21 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['GdańskGdańsk, pomorskie']\n",
      "technologies: ['PHP', 'Laravel', 'Vue.js', 'React.js', 'Gitflow', 'Azure DevOps']\n",
      "responsibilities: ['Tworzenie, rozwój i utrzymanie aplikacji webowych', 'Prowadzenie dokumentacji technicznej', 'Dbanie o jakość kodu oraz korzystanie z dobrych praktyk programistycznych', 'Aktywny udział w spotkaniach oraz proponowanie możliwych usprawnień', 'Opracowywanie, udoskonalanie i integrowanie rozwiązań AI']\n",
      "requirements: ['Minimum 3 lata doświadczenia na podobnym stanowisku', 'Dobra znajomość PHP, Laravel oraz Vue.js lub React', 'Dobra znajomość i doświadczenie w korzystaniu z narzędzi programistycznych opartych na sztucznej inteligencji oraz znajomość Python', 'Język angielski w stopniu komunikatywnym, min. B2', 'Doświadczenie w pracy z metodykami zwinnymi']\n",
      "application_link: https://www.pracuj.pl/aplikuj/fullstack-developer-ai-gdansk,oferta,1003916001?apid=72948675-aa36-4f81-8234-8f3c1d918b76&oca=None&acv=0\n",
      "\n",
      "--- Job Record 92 ---\n",
      "id: 145\n",
      "url: https://www.pracuj.pl/praca/inzynier-baz-danych-oracle-warszawa,oferta,1003872772\n",
      "title: Inżynier baz danych Oracle\n",
      "work_location: None\n",
      "validity: ważna jeszcze 3 dnido 01 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Oracle', 'Linux', 'PL/SQL', 'Windows Server']\n",
      "responsibilities: ['Projektowanie, wdrażanie oraz zarządzanie bazami danych Oracle w konfiguracji Real Application Clusters (RAC) z co najmniej 2 nodami, wraz z ASM (Automatic Storage Management) i Data Guard', 'Administrowanie bazami danych w środowisku produkcyjnym na platformach Linux (Red Hat, Oracle Enterprise Linux, CentOS) oraz Windows (2003, 2008), zapewniając wysoką dostępność i optymalną wydajność systemów', 'Monitorowanie i optymalizacja wydajności baz danych Oracle, w tym analiza zapytań SQL, indeksów oraz innych elementów wpływających na wydajność systemu', 'Identyfikowanie wąskich gardeł wydajnościowych, wdrażanie poprawek i zapewnienie ciągłej optymalizacji wydajności', 'Zarządzanie backupami i odzyskiwaniem danych (RMAN)', 'Zarządzanie Oracle RAC, ASM i Data Guard', 'Tworzenie i optymalizacja skryptów PL/SQL', 'Zarządzanie narzędziami administracyjnymi Oracle', 'Wsparcie i troubleshooting', 'Bezpieczeństwo i zarządzanie dostępem', 'Dokumentacja i raportowanie']\n",
      "requirements: ['Minimum 3 lata doświadczenia w administracji bazami danych Oracle w wersji 11g bądź wyższej w konfiguracji Real Application Clusters (RAC - co najmniej 2 nody) wraz z ASM i\\u202fData Guard w\\u202fśrodowisku produkcyjnym na platformach Linux (Red Hat, Oracle Enterprise Linux, CentOS) i\\u202fWindows (2003, 2008), popartego posiadaniem co najmniej jednego z poniższych certyfikatów:', 'Oracle Database 12c Administrator Certified Professional', 'Oracle Database 11g Administrator Certified Professional', 'Wiedza oraz doświadczenie z zakresu performance, tuning and analysis, strategii backup/restore – biegła znajomość RMAN,', 'Biegła znajomość PL/SQL,', 'Biegła znajomość narzędzi do administracji Oracle.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/inzynier-baz-danych-oracle-warszawa,oferta,1003872772?apid=20c1d611-dc0d-49d8-bc3f-7f05ed9f5955&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 93 ---\n",
      "id: 146\n",
      "url: https://www.pracuj.pl/praca/data-migration-architect-wroclaw-legnicka-70,oferta,1003918397\n",
      "title: Data Migration Architect\n",
      "work_location: Company locationLegnicka 70, Fabryczna, WrocławWrocław, Lower Silesia\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['SQL', 'agile', 'scrum']\n",
      "responsibilities: ['Develop and execute data migration plans', 'Analyze source systems', 'Prepare scripts for data processing and loading into staging databases', 'Identify dependencies and open issues, coordinating them with customers and digital transformation teams']\n",
      "requirements: ['Deep knowledge of OSS/BSS systems in the telecommunications industry', 'Experience in data migration projects', 'Proficiency in SQL', 'Salesforce administration skills (nice to have)', 'Advanced English proficiency (work in an international environment)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-migration-architect-wroclaw-legnicka-70,oferta,1003918397?apid=061bf764-d675-4791-808d-da1f52d7fcc1&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 94 ---\n",
      "id: 147\n",
      "url: https://www.pracuj.pl/praca/data-scientist-warszawa-pulawska-2,oferta,1003930310\n",
      "title: Data Scientist\n",
      "work_location: Company locationPuławska 2, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for a monthto 28 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'More than one vacancy']\n",
      "technologies: ['Python', 'SQL', 'PySpark', 'Azure Cloud', 'Databricks', \"at the client's site\"]\n",
      "responsibilities: ['Actively participate in each stage of the data science lifecycle, from business understanding through model development to industrialization', 'Design, build and maintain statistical/machine learning models to optimize business operations', 'Explore and experiment with innovative ML techniques and approaches to maximize value generated by the team', 'Be a thought leader in development of methodology within big scale transformational projects', 'Communicate model & analysis results to a non-technical audience that includes stakeholders and senior management, and win their buy-in', 'Coach and mentor other team members on multiple aspects of a data scientist role']\n",
      "requirements: ['2+ years of professional experience building and industrializing machine learning solutions', 'Very good command of Python and its use with version control (git) and testing', 'Practical knowledge of SQL, PySpark & Databricks', 'Experience in application of ML concepts and methodologies (regression and classification, time series modeling, feature engineering and selection, regularization etc.)', 'Solid knowledge of ML techniques and algorithms (incl. various regression types, ensembles, clustering, decision trees, boosting, etc.) and their pros and cons', 'Ability to reframe business challenges into data science problems and adjust ML methods & tools accordingly', 'Good communication and presentation skills, in particular facilitation of grasping complex topics by broad audiences, incl. non-technical stakeholders', 'Track record of effective collaboration with IT / data engineering teams in development and production phase']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-warszawa-pulawska-2,oferta,1003930310?apid=e784c19f-bd40-41e9-be30-a1b6c498b5e5&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 95 ---\n",
      "id: 148\n",
      "url: https://www.pracuj.pl/praca/data-engineer-data-warehousing-warszawa-pulawska-2,oferta,1003895019\n",
      "title: Data Engineer - Data Warehousing\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 14 dnido 12 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Szukamy wielu kandydatówwakaty: 2']\n",
      "technologies: ['Python', 'spark', 'Apache Hive', 'ETL', 'R', 'Hadoop', 'AIRFLOW', 'DAGSTER', 'MSSQL', 'Microsoft Azure', 'u klienta']\n",
      "responsibilities: [\"Projektowanie i implementacja pipeline'ów danych – Tworzenie i optymalizacja procesów przetwarzania danych przy użyciu Pythona, Spark/PySpark oraz narzędzi orkiestracji danych (Airflow/Dagster)\", 'Zarządzanie i optymalizacja baz danych – Praca z systemami baz danych, takimi jak Apache Hive, Hadoop, MS SQL Server, PostgreSQL, ClickHouse, w celu efektywnego przechowywania i przetwarzania dużych zbiorów danych', 'Integracja i przetwarzanie danych marketingowych – Implementacja rozwiązań ETL do ekstrakcji, transformacji i ładowania danych z różnych źródeł, w tym platform marketingowych (Google Ads, Facebook Insights)', 'Współpraca z zespołem i rozwój infrastruktury danych – Wsparcie działań DevOps oraz wykorzystanie Microsoft Azure do budowy i zarządzania środowiskiem przetwarzania danych, przy jednoczesnym zapewnieniu zgodności z najlepszymi praktykami w zakresie bezpieczeństwa i wydajności']\n",
      "requirements: ['3-5 lat doświadczenia w obszarze Data Engineering', \"Biegłość w Pythonie oraz doświadczenie w tworzeniu pipeline'ów danych\", 'Doświadczenie z bazami danych takimi jak Apache Hive, Hadoop, Microsoft SQL Server, PostgreSQL, ClickHouse itp.', 'Znajomość narzędzi orkiestracji danych (Airflow/Dagster)', 'Doświadczenie z Spark, PySpark, PySQL lub R', 'oraz narzędziami ETL, takimi jak SSIS', 'Biegła znajomość języka angielskiego']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-data-warehousing-warszawa-pulawska-2,oferta,1003895019?apid=25a66fa2-4d97-42b2-8135-88e1248b3073&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 96 ---\n",
      "id: 149\n",
      "url: https://www.pracuj.pl/praca/technical-leader-snowflake-dbt-warszawa-pulawska-2,oferta,1003912991\n",
      "title: Technical Leader (Snowflake/DBT)\n",
      "work_location: Company locationPuławska 2, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for 22 daysto 20 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior), expert\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Snowflake Data Cloud', 'DBT', 'GitLab', \"at the client's site\", 'agile']\n",
      "responsibilities: ['Lead Snowflake optimization and ensure best practices are followed for efficient data processing and decision-making.', 'Manage complex data transformations using DBT, focusing on Jinja and best practices for Snowflake.', 'Oversee CI/CD processes with GitLab, ensuring smooth DevOps/DataOps integration.', 'Support data migration to the Data Vault model, including dimensional modeling and data product development.', 'Handle data onboarding with Talend, integrating multiple data sources (CSV, APIs, SAP BW, S3) and managing scheduled tasks using tools like AutomateNow.']\n",
      "requirements: ['4 -5 years in distributed databases, particularly Snowflake', 'Strong expertise in Snowflake optimization and best practices', 'Advanced DBT knowledge', 'Proficiency in GitLab, CI/CD, and DevOps/DataOps', 'Experience with dimensional modeling and Data Vault migration', 'Familiarity with Informatica PowerCenter, including parameter management']\n",
      "application_link: https://www.pracuj.pl/aplikuj/technical-leader-snowflake-dbt-warszawa-pulawska-2,oferta,1003912991?apid=4ac5924b-64a0-4fd6-bdcc-b64ae8ae575d&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 97 ---\n",
      "id: 150\n",
      "url: https://www.pracuj.pl/praca/ml-ops-engineer-warszawa-pulawska-2,oferta,1003900967\n",
      "title: ML Ops Engineer\n",
      "work_location: Company locationPuławska 2, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for 16 daysto 14 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), junior specialist (Junior)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['Microsoft Azure', 'Python', 'Databricks', 'SQL', 'in house', 'agile']\n",
      "responsibilities: ['Optimize and implement data science and ML solutions in Azure cloud environments', 'Lead the full lifecycle of data science projects, leveraging best practices like DevOps, CI/CD, and model management', 'Collaborate with engineering to improve data consumption', 'Manage and troubleshoot ML pipelines, ensuring smooth operations']\n",
      "requirements: ['Minimum 2 years of experience in ML Ops or ML engineering', 'Familiarity with Azure cloud and Databricks', 'Proficiency in Python, SQL, and Git', 'Excellent written and spoken English']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ml-ops-engineer-warszawa-pulawska-2,oferta,1003900967?apid=c58403ef-6c71-432c-83ae-a095699c75d1&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 98 ---\n",
      "id: 151\n",
      "url: https://www.pracuj.pl/praca/data-warehouse-analyst-warszawa,oferta,1003930259\n",
      "title: Data Warehouse Analyst\n",
      "work_location: None\n",
      "validity: valid for a monthto 28 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['agile', 'scrum']\n",
      "responsibilities: [\"Establishing the content of the Predictive Solutions tool's online interfaces with the Bank's systems\", 'Determining the content of batch interfaces of the Predictive Solutions tool with the data warehouse, analysis and implementation of the interface', 'Extracting and compiling various sources of information and large data sets from complex systems to identify and analyse outliers', 'Setting up process for monitoring, tracking, and trending department data', 'Preparing and reporting mandated reports and analysis', 'Implementing and using the analytics software and systems to support the departments goals']\n",
      "requirements: ['5+ years of experience in a similar role', 'Expertise in best practises, data warehouses and reporting techniques', 'Knowledge of data warehouse issues', 'Past experience as a data analyst in complex IT projects', 'Ability to consistently interpret and assess the details for risks', 'Fluent Polish', 'Preferably higher technical or financial education']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-warehouse-analyst-warszawa,oferta,1003930259?apid=7b9ff1f8-8634-4f42-83f9-612272f95d4a&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 99 ---\n",
      "id: 152\n",
      "url: https://www.pracuj.pl/praca/senior-oracle-pl-sql-developer-rybnik-generala-jaroslawa-dabrowskiego-9,oferta,1003898266\n",
      "title: Senior Oracle PL/SQL developer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 15 dnido 13 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: None\n",
      "technologies: ['SQL', 'Oracle', 'PL/SQL', 'Git', 'Linux', 'Enterprise Architect', 'wewnątrz organizacji', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne']\n",
      "responsibilities: ['Projektowanie i implementacja procesów ETL w bazie danych Oracle', 'Projektowanie struktur baz danych oraz ich dokumentacja', 'Tworzenie procedur składowanych na bazie danych Oracle', 'Tworzenie zapytań na potrzeby analiz i raportów', 'Współpraca z administratorami baz danych w celu strojenia wydajności']\n",
      "requirements: ['Minimum 4 lata doświadczenia na stanowisku programisty baz danych Oracle', 'Bardzo dobra znajomość zagadnień baz danych (SQL, PL/SQL)', 'Umiejętność optymalizacji zapytań, w tym analiza planów wykonania zapytań', 'Umiejętność pracy z dużymi zbiorami danych', 'Umiejętność pracy w zespole, komunikatywność, zaangażowanie, sumienność, dociekliwość, umiejętność analitycznego myślenia, kreatywność', 'Znajomość GIT', 'Znajomość systemów rodziny Linux (podstawowa)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-oracle-pl-sql-developer-rybnik-generala-jaroslawa-dabrowskiego-9,oferta,1003898266?apid=89680b5d-af76-4912-8cba-909e508fdcbf&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 100 ---\n",
      "id: 153\n",
      "url: https://www.pracuj.pl/praca/specjalista-it-ds-zarzadzania-bazami-danych-bydgoszcz-lochowska-69,oferta,1003900857\n",
      "title: Specjalista IT ds. Zarządzania Bazami Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 16 dnido 14 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Łochowska 69, BydgoszczBydgoszcz, kujawsko-pomorskie', 'praca stacjonarna']\n",
      "technologies: ['T-SQL', 'Microsoft SQL Server', 'SQL', 'Tableau', 'Microsoft Power BI']\n",
      "responsibilities: ['Tworzenie, utrzymywanie i zarządzanie bazami danych', 'Zapewnienie optymalnej wydajności, dostępności i bezpieczeństwa baz danych', 'Monitorowanie stanu baz danych i rozwiązywanie problemów związanych z wydajnością', 'Optymalizacja zapytań SQL i struktury baz danych w celu poprawy wydajności', 'Przeprowadzanie regularnych audytów wydajności i wdrażanie działań naprawczych', 'Wsparcie techniczne dla użytkowników baz danych, w tym rozwiązywanie problemów i odpowiadanie na zapytania', 'Przeprowadzanie szkoleń dla użytkowników z zakresu korzystania z baz danych', 'Tworzenie i aktualizacja dokumentacji technicznej dotyczącej zarządzania bazami danych, w tym procedur operacyjnych i konfiguracji systemów', 'Koordynacja działań związanych z rozwojem i utrzymaniem infrastruktury bazodanowej']\n",
      "requirements: ['Wykształcenie wyższe techniczne (preferowany kierunek Informatyka - Inżynier Systemów Bazodanowych, Administrator Baz Danych, Architekt Baz Danych, Analityk Baz Danych, Programowanie aplikacji biznesowych)', 'Doświadczenie w zarządzaniu bazami danych', 'Umiejętność pracy z systemami DMBS: Microsoft SQL Server i Azure Data Studio', 'Umiejętność pracy z bazami danych MS SQL Server, w tym biegła znajomość języka T-SQL', 'Znajomość platformy Azure, w tym Azure SQL Database, Azure Data Lake Storage i Azure Synapse Analytics', 'Umiejętność analitycznego myślenia i rozwiązywania problemów', 'Umiejętność szybkiego uczenia się i dostosowywania się do nowych technologii', 'Umiejętność pracy pod presją czasu i terminowości']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-it-ds-zarzadzania-bazami-danych-bydgoszcz-lochowska-69,oferta,1003900857?apid=f69ce373-0501-4881-921a-ea5257a06ba0&oca=None&acv=0\n",
      "\n",
      "--- Job Record 101 ---\n",
      "id: 154\n",
      "url: https://www.pracuj.pl/praca/mlops-engineer-krakow-zablocie-43a,oferta,1003872612\n",
      "title: MLOps Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 3 dnido 01 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Zabłocie 43A, Podgórze, KrakówKraków, małopolskie']\n",
      "technologies: ['Python', 'Google Cloud Platform', 'Apache Airflow', 'Terraform', 'Jenkins', 'Docker', 'wewnątrz organizacji', 'u klienta', 'koncentrujesz się na jednym projekcie', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'koncentrujesz się na utrzymaniu kodu', 'agile', 'scrum']\n",
      "responsibilities: ['Utrzymywanie najlepszych praktyk ML Ops (w tym kontrola wersji, pipeline’y CI/CD, rejestr modeli i punkty końcowe na Vertex AI).', 'Implementacja narzędzi ML Ops do wspierania procesów rozwoju, treningu, optymalizacji, wdrażania i monitorowania modeli.', 'Wdrażanie oraz zarządzanie modelami ML na platformie GCP, z zapewnieniem ich wydajności i skalowalności.', 'Rozwiązywanie problemów związanych z wydajnością modeli i pipeline’ów.', 'Współpraca z zespołem Data Scientistów w celu przygotowania danych do treningu modeli.', 'Tworzenie i utrzymywanie pipeline’ów do trenowania i wdrażania modeli.', 'Wdrożenie automatycznego monitoringu i alertów w celu śledzenia wydajności modeli.', 'Zapewnienie jakości danych i bezpieczeństwa, w tym weryfikacja zgodności z wymaganiami.']\n",
      "requirements: ['Praktyczne doświadczenie i bardzo dobra znajomość Python.', 'Praktyczne doświadczenie w pracy z GCP.', 'Znajomość narzędzi i procesów związanych z wersjonowaniem, CI/CD, wdrażaniem i monitorowaniem modeli.', 'Silne umiejętności analityczne i rozwiązywania problemów.', 'Umiejętność współpracy z zespołami DevOps.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/mlops-engineer-krakow-zablocie-43a,oferta,1003872612?apid=470ce2af-3654-4e9e-bb13-f8acc7da6dfa&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 102 ---\n",
      "id: 155\n",
      "url: https://www.pracuj.pl/praca/demand-generation-director-poznan,oferta,1003882534\n",
      "title: Demand Generation Director\n",
      "work_location: Company locationPoznańPoznań, Greater Poland\n",
      "validity: valid for 8 daysto 06 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'director', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: [\"at the client's site\"]\n",
      "responsibilities: ['A minimum of 5–7 years of experience in marketing, with at least 2 years in a leadership role.', 'A proven track record of success in demand generation, with experience across inbound, outbound, and referral strategies.', 'Deep knowledge of data-driven marketing, including analytics, attribution models, and campaign optimization.', 'Expertise in marketing and sales technologies such as HubSpot, Salesforce, or equivalent platforms.', 'Strong understanding of AI, cloud technologies, and their applications in marketing and business growth.', 'Exceptional communication and leadership skills, with the ability to work across a dynamic and fast-growing organization.', 'Fluency in English; German a definite plus.']\n",
      "requirements: []\n",
      "application_link: https://www.pracuj.pl/aplikuj/demand-generation-director-poznan,oferta,1003882534?apid=484e61a1-47a4-42ea-99b2-87c73b683563&oca=None&acv=0\n",
      "\n",
      "--- Job Record 103 ---\n",
      "id: 156\n",
      "url: https://www.pracuj.pl/praca/elastic-stack-engineer-warszawa,oferta,1003898135\n",
      "title: Elastic Stack Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 15 dnido 13 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Elasticsearch', 'Kibana', 'Logstash']\n",
      "responsibilities: ['Projektowanie, wdrażanie i optymalizacja środowisk Elastic Stack (Elasticsearch, Logstash, Kibana, Beats).', 'Administracja i tuning klastra Elasticsearch dla wydajności, wysokiej dostępności i skalowalności.', 'Tworzenie i optymalizacja zapytań oraz indeksów Elasticsearch.', 'Integracja Elastic Stack z innymi systemami (np. SIEM, monitoring, analityka).', 'Automatyzacja procesów wdrażania i zarządzania (np. Ansible, Terraform).', 'Monitorowanie i rozwiązywanie problemów związanych z wydajnością i stabilnością systemu.', 'Wdrażanie polityk bezpieczeństwa i kontroli dostępu w Elastic Stack.']\n",
      "requirements: ['Znajomość Elastic Stack (Elasticsearch, Kibana, Logstash, Beats).', 'Doświadczenie w administracji dużymi klastrami Elasticsearch.', 'Umiejętność optymalizacji zapytań i indeksowania.', 'Znajomość języka Painless oraz mechanizmów agregacji i skryptów w Elasticsearch.', 'Doświadczenie w integracji Elastic Stack z innymi narzędziami (np. SIEM, Prometheus, Kafka).', 'Znajomość technologii chmurowych (AWS, GCP, Azure) i konteneryzacji (Docker, Kubernetes).', 'Podstawowa znajomość narzędzi CI/CD oraz automatyzacji (Ansible, Terraform, GitLab CI).', 'Umiejętność rozwiązywania problemów wydajnościowych i debugowania Elasticsearch.', 'Znajomość zagadnień związanych z bezpieczeństwem danych i kontrolą dostępu w Elastic Stack.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/elastic-stack-engineer-warszawa,oferta,1003898135?apid=d32564d5-5b7d-4451-ba63-1051b212abf9&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 104 ---\n",
      "id: 157\n",
      "url: https://www.pracuj.pl/praca/mlops-with-kubeflow-aws-sagemaker-warszawa-pulawska-2,oferta,1003891200\n",
      "title: MLOps with Kubeflow/AWS Sagemaker\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 11 dnido 09 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior), ekspert\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Kubernetes', 'ML', 'Bash', 'AWS', 'Python', 'kubeflow', 'sagemaker', 'Data Science', 'u klienta', 'agile']\n",
      "responsibilities: ['Badanie i wdrażanie narzędzi, frameworków oraz platform MLOps na potrzeby projektów Data Science.', 'Realizacja backlogu działań w celu podniesienia dojrzałości MLOps w organizacji.', 'Proaktywne wprowadzanie nowoczesnego, zwinnego i zautomatyzowanego podejścia do Data Science.', 'Prowadzenie wewnętrznych szkoleń i prezentacji na temat korzyści i zastosowania narzędzi MLOps.']\n",
      "requirements: ['Doświadczenie w pracy z Kubernetes.', 'Doświadczenie w operacjonalizacji projektów Data Science (MLOps) przy użyciu co najmniej jednego z popularnych frameworków lub platform (np. Kubeflow, AWS Sagemaker).', 'Dobra znajomość koncepcji ML i AI oraz praktyczne doświadczenie w rozwijaniu modeli ML.', 'Biegłość w Pythonie zarówno w kontekście ML, jak i automatyzacji. Dobra znajomość Bash oraz narzędzi wiersza poleceń systemu Unix.', \"Doświadczenie w implementacji pipeline'ów CI/CD/CT.\", 'Doświadczenie w pracy z AWS.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/mlops-with-kubeflow-aws-sagemaker-warszawa-pulawska-2,oferta,1003891200?apid=12e07eb8-1c2c-4e3c-af6d-88ec77d46bdd&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 105 ---\n",
      "id: 158\n",
      "url: https://www.pracuj.pl/praca/data-engineer-smart-city-lodz-dowborczykow-30,oferta,1003879421\n",
      "title: Data Engineer - Smart City\n",
      "work_location: None\n",
      "validity: valid for 7 daysto 05 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Dowborczyków 30, Śródmieście, ŁódźŁódź, Łódź', 'Запрошуємо працівників з України']\n",
      "technologies: ['Python', 'Scala', 'Grafana', 'Kubernetes', 'Apache Airflow', 'Kafka', 'Apache Spark', 'Java', 'Node.js', \"at the client's site\", 'you focus on a single project at a time', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'you focus on code maintenance', 'agile', 'scrum']\n",
      "responsibilities: ['Cooperate with the Data Scientist in order to understand the inputs and outputs of the models as well as the transformations requested to design the features', 'Design the services API requested by the use cases', 'Bring his/her expertise in microservices architecture to propose and design the overall architecture of the service', 'Cooperate with the DevOps team to understand the platform constraints and improve the CI/CD and MLOps flow']\n",
      "requirements: []\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-smart-city-lodz-dowborczykow-30,oferta,1003879421?apid=31f6aac9-92fe-4c45-921f-fe8344d915dc&oca=None&acv=0\n",
      "\n",
      "--- Job Record 106 ---\n",
      "id: 159\n",
      "url: https://www.pracuj.pl/praca/head-of-database-lake-platform-security-krakow-kapelanka-42a,oferta,1003921024\n",
      "title: Head of Database & Lake Platform Security\n",
      "work_location: None\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'manager / supervisor', 'Запрошуємо працівників з України']\n",
      "technologies: ['PostgreSQL', 'MongoDB']\n",
      "responsibilities: ['Define secure configuration baselines for database management system software, including but not limited to Oracle, Db2, SAP ASE, SQL Server, Db2 z/OS, MongoDB, and PostgreSQL, Teradata, HADOOP.', 'Work with database technical subject matter experts to agree secure configuration baselines.', 'Work with database technical subject matter experts to define/develop/implement checks for compliance scans.', 'Work with database technical subject matter experts to provide remediation guidance for IT Service Owners.', 'Work with the Configuration Baseline Management team to ensure they receive configuration compliance data.', 'Interact with stakeholders across the organisation to understand their security needs and expectations.', 'Define and maintain capability strategy, supported by Enterprise Architecture, Security Architecture and, Control Owners, in response to business strategies, regulator expectations, technology and practice advancement, best practice, and threat actor evolution [will overlap with Architecture].', 'Ensure success with delivery partners (in alignment with support functions). Runs / drives respective Delivery forum, QBRs, SteerCos and Capability PODs.', 'Maintain and prioritise a capability backlog based on objectives and value released to identify what teams work on next. Supports the prioritisation of backlogs from supporting technology and operations/service teams.', 'Close working with Control Owners: Oversees Control Owner activity from a technical point-of-view, e.g. accurate assessment of control defect severities.', 'Close working with Service Owners: understands general performance of associated services, exceptions, customer feedback and service uplift roadmaps.', 'Close working with Technology/Platform Owners: understands general performance of associated IT services, significant bugs, technology health, customer feedback and technology uplift roadmaps (including technical debt resolution).', 'Run a Pod per L2 capability with Architecture, Engineering, Service Delivery, Control Owner, Programme Manager, and Product Management.', 'Own all medium-rated and below risk Control Issues, Audit points and Regulatory findings.']\n",
      "requirements: ['Minimum 5 years’ in-depth experience with multiple database technologies from the list of Oracle, Db2, SAP ASE, SQL Server, Db2 z/OS, MongoDB, and PostgreSQL, Teradata, HADOOP.', 'Demonstrated experience with database platform security.', 'Minimum 2 years’ experience leading a technical team.', 'Demonstrated understanding of and experience with Center for Internet Security (CIS) benchmarks.', 'Strong stakeholder management skills, with demonstrated experience of understanding and meeting the needs of multiple stakeholders.', 'Excellent communication skills, including the ability to translate complex technical concepts into business-friendly language.', 'Customer-centric consultancy approach.', 'Strong analytical and problem-solving skills.', 'Ability to manage budgets and allocate resources effectively.', 'Reliant and adaptive to changing situations, with strong desire to delegate and empower the team.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/head-of-database-lake-platform-security-krakow-kapelanka-42a,oferta,1003921024?apid=2e0b1104-2cd3-4bb1-8382-d1271f04498a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 107 ---\n",
      "id: 160\n",
      "url: https://www.pracuj.pl/praca/ai-developer-wroclaw-plac-powstancow-slaskich-1,oferta,1003930005\n",
      "title: AI Developer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze miesiącdo 28 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca stacjonarna, praca zdalna\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Szukamy wielu kandydatów']\n",
      "technologies: ['Python', 'RESTful API', 'OpenAI AP', 'MySQL', 'Docker', 'Elasticsearch', 'wewnątrz organizacji', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'scrum', 'kanban']\n",
      "responsibilities: ['projektowanie / implementacja / wdrażanie / utrzymywanie narzędzi do automatyzacji pracy wewnętrznych działów biznesowych w oparciu o różne techniki oraz podejścia,', 'projektowanie / implementacja / wdrażanie / utrzymywanie narzędzi optymalizujących pracę w działach biznesowych,', 'integracja wdrażanych narzędzi z istniejącymi systemami zarówno in-hause dev jak i rozwiązaniami firm trzecich,', 'tworzenie, rozwój i wsparcie wytwarzanych systemów edukacyjno-szkoleniowych,', 'zapewnienie wysokiej jakości dostarczanych rozwiązań,', 'współpraca z innymi obszarami działu IT oraz firmy.']\n",
      "requirements: ['Idealnie gdybyś miała/miał praktyczne doświadczenie w większości niżej wskazanych obszarach, jednak nie jest to wymóg konieczny.', 'Jeżeli czujesz się silna/silny z części niżej wskazanych zagadnień to śmiało aplikuj!', 'doświadczenie w pracy na podobnym stanowisku a z naszej strony gwarantujemy znalezienie ciekawych projektów zarówno dla seniora jak i mida,', 'praktyczna znajomość języka Python,', 'praktyczna znajomość RESTful API na poziomie korzystania z gotowego APi i/lub projektowania / tworzenia nowych usług,', \"praktyczna znajomość framework'ów webowych typu React, Django, Flask, etc.,\", \"praktyczna umiejętność korzystania ze swagger'a do projektowania, tworzenia, dokumentowania usług opartych o REST API,\", 'praktyczna znajomość OpenAI API,', 'praktyczna znajomość silnika bazy danych MySQL lub pokrewnych systemów baz danych SQL,', \"praktyczna znajomość Docker'a na poziomie korzystania z gotowych kontenerów i/lub tworzenia obrazów,\", 'praktyczna znajomość narzędzi klasy DevSecOps (CI/CD) np. GitLab,', \"praktyczna znajomość Elasticsearch'a,\", 'praktyczna znajomość rozwiązań przeznaczonych do kolejkowania np. RabbitMQ, Redis,', 'praktyczne doświadczenie w projektowaniu, tworzeniu oraz utrzymywaniu mikrousług,', 'praktyczna umiejętność analizy kody oraz wykonywania code review,', 'praktyczna znajomość zagadnień związanych z mapowaniem obiektowo-relacyjnym (ORM).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-developer-wroclaw-plac-powstancow-slaskich-1,oferta,1003930005?apid=01ad29f6-b13d-46e9-b499-420033601bf2&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 108 ---\n",
      "id: 161\n",
      "url: https://www.pracuj.pl/praca/engineering-manager-oracle-cloud-krakow-lubicz-23a,oferta,1003890787\n",
      "title: Engineering Manager - Oracle Cloud\n",
      "work_location: None\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Lubicz 23a, Grzegórzki, KrakówKraków, Lesser Poland', 'contract of employment', 'manager / supervisor']\n",
      "technologies: ['Google Cloud Platform', 'Azure', 'AWS', 'Oracle', 'Python', 'Bash']\n",
      "responsibilities: ['Lead and manage a team of cloud engineers, providing mentorship, guidance, and career development opportunities.', 'Foster a collaborative and inclusive team environment that encourages continuous learning and improvement.', 'Conduct regular performance reviews and provide constructive feedback to team members.', 'Oversee the planning, design, and implementation of Oracle Cloud Infrastructure (OCI) solutions.', 'Ensure projects are delivered on time and within scope and budget.', 'Coordinate with cross-functional teams to ensure successful integration and deployment of cloud solutions.', 'Track project progress, identify potential risks, and implement mitigation strategies.', 'Provide technical leadership and expertise in OCI, including computing, storage, networking, and database services.', 'Stay current with the latest OCI features and best practices to ensure the team’s skills remain current.', 'Conduct regular architecture reviews to ensure cloud solutions are designed for scalability and meet business requirements.', 'Design and architect robust, scalable, and secure cloud solutions that meet business requirements.', 'Conduct regular reviews and assessments of existing infrastructure to identify areas for improvement and optimization.', 'Develop and maintain infrastructure-as-code (IaC) scripts for automated provisioning and configuration.', 'Collaborate with internal stakeholders, including product managers, developers, and operations teams, to align cloud initiatives with business goals.', 'Communicate technical concepts and project status effectively to both technical and non-technical stakeholders.', 'Gather and analyze requirements from stakeholders to ensure cloud solutions meet their needs.', 'Establish and maintain best practices for cloud operations, including monitoring, alerting, and incident response.', 'Develop and maintain operational runbooks and documentation for cloud infrastructure.']\n",
      "requirements: ['8+ years of relevant work experience in software engineering & technology.', 'At least five years’ experience in an SRE or very similar leadership role.', 'Deep expertise in the mentality, processes, and tools needed to deliver SRE principles.', 'Cloud Services experience with Google Cloud / Azure / AWS.', 'Proven experience in managing and leading engineering teams.', 'Extensive experience with Oracle Cloud Infrastructure, including hands-on experience with OCI services.', 'Strong background in cloud architecture, DevOps practices, and infrastructure automation.', 'Experience in managing large-scale cloud deployments and migrations.', 'Proficiency in scripting languages (e.g., Python, Bash) and infrastructure-as-code tools (e.g., Terraform).', 'Deep understanding of cloud networking, storage, and computing concepts.', 'Experience with monitoring and logging tools (e.g., Oracle Cloud Monitoring, Prometheus, Grafana).', 'Excellent leadership and interpersonal skills.', 'Strong project management skills with a track record of successfully delivering complex projects.', 'Ability to mentor and develop junior engineers and team members.', 'Experience in driving continuous improvement initiatives within the team.', 'Relevant Oracle Cloud certifications (e.g., Oracle Cloud Infrastructure Architect Professional) are highly desirable.', 'Strong problem-solving and troubleshooting skills.', 'Solid communication and collaboration skills.', 'Ability to work effectively in a fast-paced, dynamic environment.', 'Strong organizational and time management skills.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/engineering-manager-oracle-cloud-krakow-lubicz-23a,oferta,1003890787?apid=74228711-4c9b-4676-b1fb-c7cc97e445cd&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 109 ---\n",
      "id: 162\n",
      "url: https://www.pracuj.pl/praca/senior-prompt-engineer-gen-ai-katowice,oferta,1003890914\n",
      "title: Senior Prompt Engineer – Gen AI\n",
      "work_location: None\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KatowiceKatowice, Silesian', 'contract of employment']\n",
      "technologies: ['META', 'OpenAI', 'Python', 'Google Colab']\n",
      "responsibilities: ['Prompt Development: Develop, refine, and evaluate prompts for diverse tasks, including text, image, or video generation, summarization, sentiment analysis, and data analysis. Ensure these prompts interact effectively with generative AI models, delivering high-quality, relevant, and contextually accurate outputs.', 'Best Practices Adherence: Follow prompt engineering best practices such as Few Shot Prompting and Chain of Thought Prompting, maintaining standards for prompt quality, testing, documentation, and optimization.', 'Issue Resolution: Identify, troubleshoot, and resolve prompt issues and defects.', 'Responsible and Ethical AI Implementation: Implement prompt methodologies that uphold responsible and ethical AI principles, aiming to mitigate the risk of biases in generative AI model responses.', 'Continuous Research: Explore innovative prompt techniques and tools to advance skills and elevate prompt quality through continuous research.']\n",
      "requirements: ['Excellent Communication Skills: Proficiency in English and Polish (both written and oral) and strong interpersonal skills.', 'Hands-on Experience:', 'Minimum of 1 year of hands-on experience in data-related roles.', 'Minimum of 6 months of hands-on experience with AI models and prompt engineering tools.', 'Technical Proficiency:', 'Proficiency in one or more natural language processing and generation technologies, such as OpenAI GPT-4, Meta Llama 2/3, others.', 'Some experience with Python programming language.', 'Experience with prompt development frameworks and tools, such as OpenAI Playground, Google Colab', 'Familiarity with Large Language Model (LLM) evaluation metrics, such as BLEU and ROUGE', 'Understanding of natural language processing and generation principles', 'Commercial Experience: Demonstrated experience in designing, implementing, and maintaining AI-driven solutions is a plus.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-prompt-engineer-gen-ai-katowice,oferta,1003890914?apid=01228356-6f74-43e4-8e9f-92f2636117aa&oca=None&acv=0\n",
      "\n",
      "--- Job Record 110 ---\n",
      "id: 163\n",
      "url: https://www.pracuj.pl/praca/ai-applications-engineer-in-data-ai-she-he-they-warszawa,oferta,1003882060\n",
      "title: AI Applications Engineer in Data&AI (She/He/They)\n",
      "work_location: None\n",
      "validity: valid for 7 daysto 05 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian']\n",
      "technologies: ['Python', 'SQL', 'Docker', 'Kubernetes', 'Git', 'Jenkins']\n",
      "responsibilities: ['Bachelor’s or Master’s in one of the quantitative disciplines: econometrics, computer science, statistics, mathematics, economics or any other related field.', 'Expert knowledge of Python (OOP principles, key libraries, testing, best practices) and SQL.', 'Familiarity with the key machine learning and statistical methods and modelling techniques, e.g. time series, regression, clustering, PCA, decision trees, association rules, neural networks.', 'Experience with Large Language Models, APIs, frameworks, and popular applications (RAG, chatbots, vision, speech, images).', 'Hands-on experience in creating end-to-end data processing pipelines (sourcing, exploration, automation, visualization, etc.).', 'Hands-on experience with at least one of the cloud providers (GCP, AWS, Azure) and/or data analytics platforms (Databricks, Dataiku, Alteryx, etc.).', 'Knowledge of Spark and general familiarity with Big Data concept will be an advantage.', 'Knowledge of additional DevOps components will be considered as an advantage: Docker, Kubernetes, Git, Airflow, Jenkins, etc.', 'Willingness to learn and broaden your functional knowledge (finance, logistics, sales, etc.), as well as presentation/communication/consulting skills.']\n",
      "requirements: ['ABAP skills.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-applications-engineer-in-data-ai-she-he-they-warszawa,oferta,1003882060?apid=a82cb7d3-422a-47fd-a14a-34398520da6a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 111 ---\n",
      "id: 164\n",
      "url: https://www.pracuj.pl/praca/senior-machine-learning-ai-engineer-wroclaw-powstancow-slaskich-95,oferta,1003929828\n",
      "title: Senior Machine Learning / AI Engineer\n",
      "work_location: None\n",
      "validity: valid for a monthto 27 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Powstańców Śląskich 95, Krzyki, WrocławWrocław, Lower Silesia', 'contract of employment']\n",
      "technologies: ['Python']\n",
      "responsibilities: []\n",
      "requirements: []\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-machine-learning-ai-engineer-wroclaw-powstancow-slaskich-95,oferta,1003929828?apid=7b8ace51-cce1-48cf-b549-bc8eeba78aa2&oca=None&acv=0\n",
      "\n",
      "--- Job Record 112 ---\n",
      "id: 165\n",
      "url: https://www.pracuj.pl/praca/data-engineer-analyst-warszawa-pulawska-2,oferta,1003890615\n",
      "title: Data Engineer - Analyst\n",
      "work_location: Company locationPuławska 2, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['Python', 'Snowflake', 'AWS', 'Tableau', 'Thoughtspot', 'Airflow']\n",
      "responsibilities: ['The candidate needs to have extensive experience with building complex solutions using cloud technologies (AWS, Snowflake) and workflow orchestration and data management tools', 'Experience with data visualization tools such as Tableau or Thoughtspot is a must', 'The candidate needs to have practical and proven experience with setting up CI/CD pipelines, workflow automation and developing data pipelines using Python, Airflow, dbt.', 'The candidate needs to be able to converse pro-actively with customers to understand their needs and collect requirements']\n",
      "requirements: ['AWS', 'Snowflake', 'Tableau/Thoughtspot', 'Practical and proven experience with setting up CI/CD pipelines, workflow automation and developing data pipelines using Python, Airflow', 'The resources need to be able to do data analysis and engineering work and be able to work with customers in SSF and Europe, i.e. need to be able to work during the golden hours and occasionally be able to attend meetings up to say 8 pm their time']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-analyst-warszawa-pulawska-2,oferta,1003890615?apid=4b7f110e-b26b-4379-8c1a-0ecd9768e53c&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 113 ---\n",
      "id: 166\n",
      "url: https://www.pracuj.pl/praca/youth-development-program-mistrzowie-automatyzacji-poznan-28-czerwca-1956-r-406,oferta,1003878120\n",
      "title: Youth Development Program - Mistrzowie Automatyzacji\n",
      "work_location: None\n",
      "validity: ważna jeszcze 4 dnido 02 marca 2025\n",
      "contract_type: umowa o staż / praktyki\n",
      "employment_type: pełny etat, część etatu\n",
      "position: None\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['28 Czerwca 1956 r. 406, Wilda, PoznańPoznań, wielkopolskie', 'praktykant / stażysta', 'Szukamy wielu kandydatów']\n",
      "technologies: ['Python', 'AI', 'wewnątrz organizacji', 'u klienta', 'koncentrujesz się na jednym projekcie', 'rozwijasz kilka projektów jednocześnie', 'możesz zmienić projekt', 'masz wpływ na wybór narzędzi i technologii']\n",
      "responsibilities: []\n",
      "requirements: []\n",
      "application_link: https://www.pracuj.pl/aplikuj/youth-development-program-mistrzowie-automatyzacji-poznan-28-czerwca-1956-r-406,oferta,1003878120?apid=d307f997-dcfd-4c86-9d56-cb6a13a4a882&oca=None&acv=0\n",
      "\n",
      "--- Job Record 114 ---\n",
      "id: 167\n",
      "url: https://www.pracuj.pl/praca/it-trainee-warszawa-aleje-jerozolimskie-158,oferta,1003888081\n",
      "title: IT Trainee\n",
      "work_location: None\n",
      "validity: valid for 9 daysto 07 March 2025\n",
      "contract_type: None\n",
      "employment_type: None\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Aleje Jerozolimskie 158, Ochota, WarszawaWarszawa, Masovian', 'internship / apprenticeship contract', 'part time', 'trainee']\n",
      "technologies: ['Python', 'SQL', 'NLP', 'Microsoft Power BI', 'in house']\n",
      "responsibilities: ['Areas to kick off your career:', 'Data Science/ AI Engineering', 'BI/Python Development / Data Engineering', 'Software Engineering for Automation (like RPA Developer / Workflow Designers)', 'Strategic / Organizational Communication Management', 'Cloud Platform Support/ Engineer', 'Data Analysis / Process Improvement Analysis', 'Product Specialist', 'Data Governance Engineer']\n",
      "requirements: ['You’re currently in your 2nd year or higher of a Bachelor’s program or pursuing a Master’s in fields like Computer Science, Software Engineering, Machine Learning, Mathematics, Statistics, Econometrics, or something similar', 'You’ve got some hands-on experience or interest in areas like Python, BI, SQL, NLP, RPA development, Cloud engineering, Workflow designing, Data analysis and reporting, data engineering, process improvement, Data Science, or Artificial Intelligence', 'You speak English well (B2+)', 'You’re eager to learn and grow', 'You have a quality mindset', 'You’re organized and ready to tackle challenges']\n",
      "application_link: https://www.pracuj.pl/aplikuj/it-trainee-warszawa-aleje-jerozolimskie-158,oferta,1003888081?apid=3d66b934-0662-413d-b9b7-67a52e73faca&oca=None&acv=0\n",
      "\n",
      "--- Job Record 115 ---\n",
      "id: 168\n",
      "url: https://www.pracuj.pl/praca/data-engineer-lodz-wersalska-6,oferta,1003878076\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 4 daysto 02 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Wersalska 6, Bałuty, ŁódźŁódź, Łódź', 'contract of employment']\n",
      "technologies: ['SQL', 'Python']\n",
      "responsibilities: ['Developing data models, designing and implementing ETL processes on cloud platforms (in particular Google Cloud), and ensuring data quality and integrity throughout the data lifecycle', 'Implementing new features to enable new business cases', 'Focusing on stability, performance tunning and innovation of the applications]', 'Taking care of proper up-to-date system documentation', 'Actively contributing to knowledge sharing and to a learning culture', 'Working in the international projects in agile methodologies']\n",
      "requirements: ['Good knowledge of SQL / Data Engineering', 'Basic knowledge of Python', 'Good knowledge of Data warehouse, relational and non-relational database solutions', 'Basic understanding of modern software development practices such as CI/CD']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-lodz-wersalska-6,oferta,1003878076?apid=c25b0d29-7f17-48f8-bc87-b0231528a6ff&oca=None&acv=0\n",
      "\n",
      "--- Job Record 116 ---\n",
      "id: 169\n",
      "url: https://www.pracuj.pl/praca/model-developer-lodz-wersalska-6,oferta,1003878081\n",
      "title: Model Developer\n",
      "work_location: None\n",
      "validity: valid for 4 daysto 02 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Wersalska 6, Bałuty, ŁódźŁódź, Łódź', 'contract of employment']\n",
      "technologies: ['R', 'Python', 'SAS', 'SQL']\n",
      "responsibilities: ['Development and maintenance\\u202fof AIRB/IFRS9\\u202fcredit risk\\u202fmodels (PD, LGD, CCF) for multiple portfolios or operational risk and climate risk models', 'Group-wide methodological responsibility for quantitative credit risk or operational and climate risk forecasting models', 'Forward-looking construction of a cross-functional methodology architecture', 'Estimation and optimization of separation power, quality of quantification and stability of forecasts and comprehensive uniform calibration on Basel compliant definition of default', 'Ensuring compliance with regulatory/accounting standard requirements (Basel / IFRS9 etc.)\\u202fand EBA GL', 'Programming of prototypes for impact and scenario analysis in different programming languages (R, Python, SAS, SQL)', 'Data preparation, statistical and empirical investigations, handling of very large amounts of data, their aggregation and evaluation', 'Preparation of technical specifications, presentations and documentation of quantitative credit risk forecasting models', 'Internal and external communication, including auditors, regulators, external partners and rating agencies']\n",
      "requirements: ['Master degree with very good grades in mathematics, physics, econometrics or related fields', 'Very good mathematical-statistical skills as well as knowledge of the mathematical-statistical basis of model development (multivariate statistical methods, stochastic processes, etc.)', 'Minimum 3 years of professional experience in banking, preferably within risk modelling or validation', 'Very good knowledge of data modelling software and coding (Python/R, SAS/SQL) with experience in analysis of huge data sets', 'For credit risk model developers: knowledge of regulations from credit risk models area (CRR, EBA GL, IFRS9)', 'English C1 level']\n",
      "application_link: https://www.pracuj.pl/aplikuj/model-developer-lodz-wersalska-6,oferta,1003878081?apid=fb0cb25b-bc20-4707-ae1f-b3e6b6708f77&oca=None&acv=0\n",
      "\n",
      "--- Job Record 117 ---\n",
      "id: 170\n",
      "url: https://www.pracuj.pl/praca/senior-cloud-engineer-oracle-cloud-krakow-lubicz-23a,oferta,1003872104\n",
      "title: Senior Cloud Engineer - Oracle Cloud\n",
      "work_location: None\n",
      "validity: valid for 2 daysto 28 February 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Lubicz 23a, Grzegórzki, KrakówKraków, Lesser Poland', 'contract of employment']\n",
      "technologies: ['Oracle', 'Python', 'Bash', 'Google Cloud Platform', 'Azure', 'AWS', 'Java']\n",
      "responsibilities: ['Design and implement scalable, reliable, and secure Oracle Cloud Infrastructure (OCI) solutions.', 'Architect cloud environments to meet business and technical requirements, ensuring high availability and disaster recovery capabilities.', 'Deploy and manage OCI services, including computing, storage, networking, and database solutions.', 'Automate infrastructure provisioning, configuration, and management using infrastructure-as-code (IaC) tools like Terraform.', 'Monitor and optimize the performance and cost of cloud infrastructure.', 'Conduct regular assessments and implement improvements to ensure efficient use of resources.', 'Work closely with cross-functional teams, including developers and operations, to ensure seamless integration and operation of cloud solutions.', 'Provided technical support and troubleshooting for cloud infrastructure issues, ensuring timely resolution.', 'Create and maintain comprehensive documentation for cloud infrastructure designs, configurations, and procedures.', 'Mentor and train junior engineers and other team members on OCI best practices and technologies.']\n",
      "requirements: ['2+ years of relevant work experience in software engineering & technology.', 'Extensive hands-on experience with Oracle Cloud Infrastructure (OCI), including a deep understanding of its services and capabilities.', 'Proven experience designing, deploying, and managing cloud infrastructure in a production environment.', 'Proficiency in scripting languages (e.g., Python, Bash) and infrastructure-as-code (IaC) tools (e.g., Terraform).', 'Relevant Oracle Cloud certifications (e.g., Oracle Cloud Infrastructure Architect Professional) are highly desirable.', 'At least three years experience in a Platform/SRE or very similar leadership role.', 'Deep expertise in the mentality, processes, and tools needed to deliver SRE principles.', 'Cloud Services experience with Google Cloud / Azure / AWS.', 'Experience with high throughput / low latency / highly available microservice-based architecture.', 'Proficiency in infrastructure, network, database, operating systems, troubleshooting, and remediation.', 'Architecture-level knowledge of Windows and Linux and Infrastructure systems.', 'Experience with production deployment, monitoring, and operational support for enterprise-class applications.', 'Experience working with Continuous Integration/ Continuous Deployment tools.', 'Experience in performance diagnostics, capacity planning, performance architecture design, performance tuning, performance monitoring.', 'A strong mix of Software Engineer and Operation Support skills.Eager to learn new technologies and platform patterns.', 'Strong customer service orientation with a focus on managing and exceeding customer expectations.', 'Degree in Computer Science or Engineering fields or equivalent experience.', 'Excellent troubleshooting skills.', 'Unix systems or administration experience.', 'Web technologies – http, proxy, java, etc.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-cloud-engineer-oracle-cloud-krakow-lubicz-23a,oferta,1003872104?apid=75fba3ee-db1e-4016-8703-eebb6bc483e0&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 118 ---\n",
      "id: 171\n",
      "url: https://www.pracuj.pl/praca/site-reliability-engineer-krakow-lubicz-23a,oferta,1003872098\n",
      "title: Site Reliability Engineer\n",
      "work_location: None\n",
      "validity: valid for 2 daysto 28 February 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Lubicz 23a, Grzegórzki, KrakówKraków, Lesser Poland', 'contract of employment']\n",
      "technologies: []\n",
      "responsibilities: [\"Assist in developing and maintaining infrastructure as code artifacts for GPC's cloud environments\", 'Collaborate with cross-functional teams to enable software engineers to work efficiently across cloud environments', 'Support the transition of applications between platforms', 'Participate in reviewing and improving existing processes, including observability, operations, and engineering system tuning', 'Contribute to maintaining production systems uptime and reliability', 'Assist in building and deploying new Virtual, Cloud, and/or physical IT Infrastructure', 'Help with the administration of Production/Non-Production resources including Linux/Windows VMs', 'Support the implementation of change control and auditing processes', 'Assist in maintaining backups, security, and Cloud Policies across the organization', 'Help maintain the Governance of resources via Policies and Monitoring setup', 'Support the maintenance of security and network policies across the organization', 'Collaborate with other cloud, security, and network engineers']\n",
      "requirements: ['2+ years of experience in cloud infrastructure design and engineering', 'Familiarity with cloud platforms (GCP preferred)', 'Understanding of production system designs including Infrastructure as Code, High Availability, and Performance Monitoring', 'Experience with Cloud build methodologies and automation toolsets like Terraform or similar', 'Basic infrastructure development automation skills in Python, Go, Ruby, or related languages', 'Experience with Linux/Unix Systems (e.g., Redhat, IBM AIX)', 'Familiarity with Cloud Infrastructure Management', 'Understanding of CI/CD toolchains', 'Experience in basic infrastructure, network, database, operating systems, or security troubleshooting', \"Bachelor's degree in Engineering or related field\", 'Good written and verbal communication skills in English']\n",
      "application_link: https://www.pracuj.pl/aplikuj/site-reliability-engineer-krakow-lubicz-23a,oferta,1003872098?apid=097896c8-2218-4291-889b-b9eed47e223d&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 119 ---\n",
      "id: 172\n",
      "url: https://www.pracuj.pl/praca/principal-software-engineer-ai-ml-python-java-m-f-x-krakow-jana-dekerta-24,oferta,1003869387\n",
      "title: Principal Software Engineer (AI / ML- Python, JAVA) (m/f/x)\n",
      "work_location: Company locationJana Dekerta 24, Podgórze, KrakówKraków, Lesser Poland\n",
      "validity: valid for a dayto 27 February 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'expert', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Python', 'Java', 'C++', 'PyTorch', 'TensorFlow', 'in house']\n",
      "responsibilities: ['Innovative Research & Development: Lead and contribute to architecture design and development of the functionality aimed at enhancing the capabilities of the HERE Navigation Assistant. This includes developing new algorithms, models, and techniques to improve location intelligence and natural language processing.', 'System Architecture & Design: Design, develop, and optimize the architecture of the HERE Navigation Assistant to ensure scalability, reliability, and performance. Collaborate with cross-functional teams to integrate HERE Navigation Assistant into the various parts of the HERE product portfolio.', 'Algorithm Development: Develop and implement advanced algorithms for processing and interpreting complex location-related queries. Ensure these algorithms are efficient, accurate, and capable of providing precise results.', 'Data Analysis & Interpretation: Analyze large datasets to extract meaningful insights that can be used to improve the HERE Navigation Assistant. Utilize machine learning and statistical techniques to interpret data and inform decision-making processes.', 'Collaboration & Mentorship: Work closely with other engineers, data scientists, and product managers to drive the success of R&D projects. Provide mentorship and guidance to junior team members, fostering a culture of continuous learning and innovation.', 'Technical Documentation: Prepare comprehensive technical documentation for the algorithms, models, and systems developed. Ensure that all documentation is clear, detailed, and easy to understand.', 'Problem Solving & Troubleshooting: Identify and resolve complex technical issues related to the HERE Navigation Assistant. Employ strong problem-solving skills to address challenges quickly and effectively.', 'Stay Updated: Keep abreast of the latest advancements in AI, machine learning, and location intelligence technologies. Apply this knowledge to continually improve and innovate the HERE Navigation Assistant.']\n",
      "requirements: ['Educational Background: A Master’s or Ph.D. degree in Computer Science, Artificial Intelligence, Machine Learning, or a related field.', 'Extensive Experience: At least 8-10 years of experience in R&D roles, with a strong focus on AI, machine learning, and natural language processing. Experience in the navigation or location intelligence domain is highly desirable.', 'Technical Proficiency: Proficiency in programming languages such as Python, Java, or C++. Hands-on experience with machine learning frameworks such as PyTorch, TensorFlow, or similar will be a benefit.', 'Algorithm Expertise: Demonstrated expertise in developing and optimizing algorithms for natural language processing and location intelligence. Proven ability to process and interpret large datasets.', 'System Design & Architecture: Strong experience in designing and developing scalable, reliable, and high-performance systems. Familiarity with cloud platforms and microservices architecture.', 'Problem-Solving Skills: Exceptional analytical and problem-solving skills. Ability to tackle complex technical challenges creatively and effectively.', 'Collaboration & Communication: Excellent collaboration skills, with a track record of working effectively in cross-functional teams. Strong written and verbal communication skills.', 'Mentorship: Experience in mentoring and guiding junior engineers and researchers. A passion for fostering a collaborative and innovative work environment.', 'Adaptability: Ability to stay updated with the latest technological advancements and apply this knowledge to drive continuous improvement.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/principal-software-engineer-ai-ml-python-java-m-f-x-krakow-jana-dekerta-24,oferta,1003869387?apid=fead73c7-1e64-45a1-bbc1-1a2f39d70c06&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 120 ---\n",
      "id: 173\n",
      "url: https://www.pracuj.pl/praca/ml-engineer-distributed-training-specialist-wroclaw-mosiezna-23,oferta,1003890527\n",
      "title: ML Engineer - Distributed Training Specialist\n",
      "work_location: None\n",
      "validity: valid for 7 daysto 05 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Mosiężna 23, Fabryczna, WrocławWrocław, Lower Silesia', 'Запрошуємо працівників з України']\n",
      "technologies: ['Python', 'PyTorch', 'DeepSpeed', 'FSDP', 'Megatron', 'in house', 'you focus on a single project at a time', 'you develop several projects simultaneously', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'you focus on code maintenance', 'agile']\n",
      "responsibilities: ['Implement and optimize distributed training pipelines for large-scale multimodal models', 'Set up and maintain training infrastructure across multiple nodes/GPUs', 'Develop and optimize data loading pipelines for multimodal inputs', 'Monitor and improve training efficiency and resource utilization', 'Implement checkpointing and fault tolerance mechanisms']\n",
      "requirements: [\"Bachelor's/Master's in Computer Science, Engineering, or related field\", '5+ years of experience in ML engineering', 'Proven track record with large-scale model training and optimization', 'Experience with multimodal data processing and training', 'Proficiency in Python, PyTorch', 'Proficiency with Cloud Technologies', 'Strong understanding of distributed systems and parallel computing', 'Preference but not a must: Strong experience with distributed training frameworks (DeepSpeed, FSDP, Megatron)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ml-engineer-distributed-training-specialist-wroclaw-mosiezna-23,oferta,1003890527?apid=c1e36a99-966c-4578-941f-4e0f98d9562b&oca=None&acv=0\n",
      "\n",
      "--- Job Record 121 ---\n",
      "id: 174\n",
      "url: https://www.pracuj.pl/praca/ai-ml-solution-engineer-krakow,oferta,1003897658\n",
      "title: AI & ML Solution Engineer\n",
      "work_location: Company locationKrakówKraków, Lesser Poland\n",
      "validity: valid for 14 daysto 12 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'More than one vacancy']\n",
      "technologies: ['AWS', 'Microsoft Azure', 'Google Cloud Platform', 'in house', 'you focus on a single project at a time', 'you have influence on the product', 'you focus on product development', 'you focus on code maintenance', 'agile']\n",
      "responsibilities: ['Design, develop, and implement solutions leveraging AIM (Artificial Intelligence & Machine Learning) to optimize business processes.', 'Collaborate with cross-functional teams to understand requirements and translate them into technical solutions.', 'Support the integration of AIM technologies into existing systems and ensure seamless functionality.', 'Troubleshoot and resolve technical issues related to AIM applications.', 'Document technical designs, processes, and best practices.', 'Stay up to date with the latest advancements in AIM and recommend improvements for system performance and efficiency.']\n",
      "requirements: ['Experience in solution engineering with a focus on AIM-related projects.', 'Proficiency in AI/ML frameworks and tools.', 'Strong understanding of data processing, analytics, and model deployment.', 'Knowledge of cloud platforms (AWS, Azure, or GCP) and APIs integration.', 'Problem-solving mindset with the ability to work in a fast-paced environment.', 'Excellent communication skills and ability to work effectively in a team.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-ml-solution-engineer-krakow,oferta,1003897658?apid=f5058f7a-d531-443a-9f4d-f251b8862697&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 122 ---\n",
      "id: 175\n",
      "url: https://www.pracuj.pl/praca/stazysta-w-zespole-hurtowni-danych-i-systemow-raportowych-f-m-d-warszawa-inflancka-4,oferta,1003920386\n",
      "title: Stażysta w Zespole Hurtowni Danych i Systemów Raportowych (f/m/d)\n",
      "work_location: None\n",
      "validity: ważna jeszcze 24 dnido 22 marca 2025\n",
      "contract_type: umowa zlecenie\n",
      "employment_type: pełny etat, część etatu\n",
      "position: None\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Inflancka 4, Śródmieście, WarszawaWarszawa, mazowieckie', 'praktykant / stażysta', 'Запрошуємо працівників з України']\n",
      "technologies: ['SQL', 'Python']\n",
      "responsibilities: ['Wsparcie w rozwoju i wdrażaniu aplikacji w ramach strumienia Hurtowni Danych.', 'Aktywny udział w prowadzeniu dokumentacji technicznej projektów.', 'Współpraca w zespole zwinnym (Agile), w tym udział w spotkaniach i realizacji zadań.', 'Pomoc w analizie logów systemowych oraz rozwiązywaniu błędów oprogramowania.']\n",
      "requirements: ['Kreatywność i chęć rozwoju w obszarze technologii danych', 'Minimum podstawowa znajomość SQL (PostgreSQL, MySQL)', 'Bardzo dobra znajomość funkcji Excela i chęć pracy z danymi w narzędziach analitycznych']\n",
      "application_link: https://www.pracuj.pl/aplikuj/stazysta-w-zespole-hurtowni-danych-i-systemow-raportowych-f-m-d-warszawa-inflancka-4,oferta,1003920386?apid=bc5dd112-a8bf-43e4-a156-6b5a1b09d54a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 123 ---\n",
      "id: 176\n",
      "url: https://www.pracuj.pl/praca/ai-engineer-lead-krakow-szlak-49,oferta,1003914986\n",
      "title: AI Engineer Lead\n",
      "work_location: None\n",
      "validity: ważna jeszcze 22 dnido 20 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Szlak 49, Stare Miasto, KrakówKraków, małopolskie']\n",
      "technologies: ['Python', 'AWS', 'GCP', 'Azure']\n",
      "responsibilities: ['Jako AI Engineer Lead będziesz:', 'tworzyć i wdrażać modele uczenia maszynowego i głębokiego uczenia, nastawione na rozwiązywanie realnych problemów biznesowych,', 'ulepszać istniejące algorytmy i rozwijać strategie wdrażania AI w organizacji, w tym optymalizować LLM pod kątem wydajności i kosztów,', 'zarządzać zespołem Data Science - określać kierunek działania i dbać o rozwój kompetencji,', 'organizować i prowadzić cykliczne spotkania (1:1, weekly, rozmowy rozwojowe) z zespołem,', 'wspierać zespół w zakresie najlepszych praktyk związanych z ML, MLOps i inżynierią danych,', 'wyznaczać zakres danych, które zbiera i analizuje zespół data science,', 'przewodzić zespołowi w pokonywaniu trudności przy zbieraniu i analizie danych,', 'wyznaczać cele, priorytety i dążyć z zespołem do ich realizacji,', 'analizować najnowsze trendy w AI i szukać ich zastosowania w praktyce,', 'współpracować z innymi działami w celu identyfikacji obszarów, w których AI może przynieść największą wartość,', 'tworzyć i utrzymywać dokumentację techniczną oraz raportować wyniki badań i wdrożeń,', 'określać i rozwijać potrzebne kompetencje w zespole oraz prowadzić jakościowe rekrutacje.']\n",
      "requirements: ['Aplikuj, jeśli:', 'masz doświadczenie z LLM (Large Language Models) i modelami generatywnymi,', 'masz kilkuletnie doświadczenie w pracy nad modelami uczenia maszynowego i ich wdrażaniu w środowisku produkcyjnym,', 'bardzo dobrze znasz język Python oraz ekosystem narzędzi AI/ML (np. PyTorch, TensorFlow, scikit-learn),', 'masz doświadczenie w inżynierii funkcji, przetwarzaniu danych i pracy z dużymi zbiorami danych,', 'umiesz optymalizować i skalować modele AI,', 'masz doświadczenie w zarządzaniu zespołem, umiesz i chcesz dzielić się wiedzą,', 'umiesz rozwiązywać złożone problemy techniczne i myśleć analitycznie,', 'dasz doświadczenie w prowadzeniu badań i eksperymentów nad nowymi metodami AI,', 'umiesz przekuć potrzeby biznesowe na język i strukturę dostępnych danych,', 'tłumaczysz złożone tematy w prosty sposób, dostosowujesz komunikat do specyfiki odbiorcy,', 'jesteś otwarta/y na pracę w biurze przynajmniej 2 dni w tygodniu.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-engineer-lead-krakow-szlak-49,oferta,1003914986?apid=8d1e516a-5f96-4be5-968e-949dfa004c0a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 124 ---\n",
      "id: 177\n",
      "url: https://www.pracuj.pl/praca/senior-valuation-controller-krakow,oferta,1003890445\n",
      "title: Senior Valuation Controller\n",
      "work_location: None\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KrakówKraków, Lesser Poland', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['VBA', 'Python', 'SQL', 'agile', 'scrum', 'industry-specific e-learning platforms']\n",
      "responsibilities: ['Ensuring governance and controls of Fair Value through oversight of FVA, IPV, and PVA processes', 'Leading and participating in valuation methodology discussions, ensuring global alignment and process improvements', 'Acting as a key contact for Trading teams, Risk Management, Operations, and Finance for valuation-related matters', 'Driving the implementation of changes in FVAs, IPV, and PVA processes, engaging with business stakeholders', 'Coordinating with other asset class valuation teams to enhance consistency and efficiency in controls and processes', 'Promoting continuous improvement initiatives by optimizing procedures, systems, and leadership within PC transformation', 'Maintaining internal control standards and ensuring the timely implementation of assurance and audit points', 'Ensuring compliance with internal and external regulatory requirements, policies, and procedures', 'Managing relationships with internal and external auditors, addressing valuation-related queries and concerns', 'Providing guidance and leadership on valuation-related topics to junior team members and stakeholders']\n",
      "requirements: ['A graduate degree with at least 3 years of experience in Product Control or Valuations', 'Demonstrable knowledge of Product Control, Credit, and Interest Rate products, including accounting, valuations, and risk controls', 'A strong understanding of regulatory requirements and guidelines for valuation', 'Excellent communication skills, with the ability to explain valuation concepts to senior stakeholders in a non-technical manner', 'Strong analytical skills and attention to detail', 'A highly adaptive control-oriented mindset with the ability to identify and mitigate valuation risks', 'Experience in leading valuation-related projects and driving process improvements', 'Proven ability to build and maintain relationships with stakeholders at various levels of seniority', 'The ability to develop valuation methodologies and shape policy for IPV/FVAs', 'Experience in managing teams, especially across multiple locations']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-valuation-controller-krakow,oferta,1003890445?apid=baac0e2a-c3b5-47db-80e6-c85644126443&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 125 ---\n",
      "id: 178\n",
      "url: https://www.pracuj.pl/praca/data-engineer-warszawa-lucka-15,oferta,1003897594\n",
      "title: Data Engineer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 14 dnido 12 marca 2025\n",
      "contract_type: umowa o pracę, umowa zlecenie, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Snowflake Data Cloud', 'Parquet', 'dbt', 'Python', 'Bash', 'Collibra', 'wewnątrz organizacji', 'u klienta', 'możesz zmienić projekt', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'agile', 'scrum', 'kanban']\n",
      "responsibilities: ['Doświadczenie z analityką, bazami danych, systemami danych;', 'Bardzo dobrą znajomość Snowflake;', 'Doświadczenie z pisaniem query, mikropartycji, znasz Parqeta;', 'Masz doświadczenie z DBT;', 'Rozumiesz koncepcje przepływu danych i masz podstawy modelowania;', 'Rozumiesz proces DevOps;', 'Znasz języki skryptowe, jak Python lub Bash;', 'Pracowałeś z Github;', 'Znajomość Angielskiego na poziomie min. B2.']\n",
      "requirements: ['Znajomość procesu Data Quality;', 'Rozumienie API GraphQL;', 'Znajomość collibra.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-warszawa-lucka-15,oferta,1003897594?apid=edc78174-eb5f-430c-9b62-ba0889457ba3&oca=None&acv=0\n",
      "\n",
      "--- Job Record 126 ---\n",
      "id: 179\n",
      "url: https://www.pracuj.pl/praca/data-entry-clerk-krakow-lubicz-23a,oferta,1003914959\n",
      "title: Data Entry Clerk\n",
      "work_location: None\n",
      "validity: ważna jeszcze 22 dnido 20 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Lubicz 23a, Grzegórzki, KrakówKraków, małopolskie']\n",
      "technologies: ['u klienta', 'koncentrujesz się na jednym projekcie', 'możesz zmienić projekt', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu']\n",
      "responsibilities: ['Wykorzystywanie zebranych informacji dotyczących bezpieczeństwa i prywatności danych jako dowodów w procesach dokumentowania odpowiedzi oraz innych działań związanych z zapewnieniem zgodności dla produktów i usług oferowanych przez naszego klienta.', 'Wspieranie tworzenia odpowiedzi oraz kompletowanie końcowych dokumentów, zgodnie z oryginalnym formatem zapytania lub wymaganiami portalu oceny (jeśli dotyczy).', 'Wspieranie codziennych działań związanych z wdrażaniem programu zgodności operacyjnej produktów ochrony danych.', 'Wykonywanie innych zadań i obowiązków związanych z zgodnością, ochroną danych oraz prywatnością, zgodnie z poleceniami.']\n",
      "requirements: ['Doświadczenie w Wprowadzaniu Danych: Wcześniejsze doświadczenie w pracy związanej z wprowadzaniem danych.', 'Biegłość w pakiecie Google Office: Wymagana jest znajomość i umiejętność pracy z narzędziami Google Office, takimi jak Gmail, Chat, Meet, Kalendarz, Dysk, Dokumenty, Arkusze, Prezentacje i Formularze.', 'Umiejętność efektywnej komunikacji: Wymagane są również umiejętności analizy danych.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-entry-clerk-krakow-lubicz-23a,oferta,1003914959?apid=613e7ace-f0f2-4dd1-ad60-c889a3351834&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 127 ---\n",
      "id: 180\n",
      "url: https://www.pracuj.pl/praca/data-reporting-senior-developer-krakow,oferta,1003905763\n",
      "title: Data & Reporting Senior Developer\n",
      "work_location: None\n",
      "validity: valid for 17 daysto 15 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KrakówKraków, Lesser Poland', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['SQL', 'Google Cloud Platform', 'BigQuery', 'Data Studio', 'Python', 'Tableau', 'Microsoft Power BI', 'agile', 'scrum', 'kanban', 'waterfall', 'industry-specific e-learning platforms']\n",
      "responsibilities: ['Deliver assigned projects end-to-end, including data discovery, cleaning, transformation, and validation', 'Develop and implement data models based on business requirements', 'Perform matching and linking across datasets to ensure data consistency', 'Securely transfer data from sources to target systems', 'Utilize ETL and data tools for seamless data integration across databases and platforms', 'Provide complex reporting solutions within Google Cloud and Microsoft SQL Server environments', 'Develop and optimize SQL queries and scripts for data extraction and transformation', 'Collaborate with stakeholders to understand business needs and process logic', 'Identify and implement internal process improvement opportunities', 'Document processes by creating roadmaps, descriptions, and technical documentation']\n",
      "requirements: ['At least 3 years of experience in a data-related role (SQL development, data analysis, data engineering, or data architecture)', 'Expertise in SQL programming and relational database systems', 'Hands-on experience with Google Cloud Platform, especially BigQuery and Data Studio', 'Knowledge of ETL principles and data warehousing', 'Experience in developing business reports and front-end dashboards', 'Proficiency in Python for data processing and automation', 'Fluent English', 'Proactive problem-solving approach and goal-oriented mindset', 'Ability to identify risks and process blockers and propose solutions', 'Experience in working with stakeholders and business partners to drive project success']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-reporting-senior-developer-krakow,oferta,1003905763?apid=70c235af-1394-4576-8632-a9b0818ac451&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 128 ---\n",
      "id: 181\n",
      "url: https://www.pracuj.pl/praca/data-developer-java-python-wroclaw-powstancow-slaskich-9,oferta,1003890374\n",
      "title: Data Developer (Java/Python)\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 10 dnido 08 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Java', 'Python', 'Spark', 'Pandas', 'Hadoop', 'Airflow', 'Elasticsearch', 'MongoDB', 'Redis', 'Kubernetes', 'Helm', 'SKLearn', 'Tensorflow', 'RedHat OpenShift', 'GIT', 'Jenkins', 'Docker']\n",
      "responsibilities: ['praca nad rozwiązaniami przeciwdziałającymi praniu pieniędzy (AML) opartymi na AI, z których korzystają fintechy, banki i organy regulacyjne na całym świecie,', 'projektowanie i rozwijanie skalowalnego pipeline’u do przetwarzania danych i ML przy użyciu najnowszych technologii Big Data w szybko zmieniającym się, zwinnym środowisku,', 'badanie nowych obszarów technologicznych i ich dogłębne zrozumienie poprzez szybkie samokształcenie,', 'rozwiązywanie problemów w celu wspierania skalowalnych i zrównoważonych rozwiązań projektowych,', 'praca w odpowiedzialności za rozwój produktu, w tym za wszystkie etapy cyklu życia: przekładanie wymagań produktu na wykonalne projekty i zadania, rozwój, pisanie testów jednostkowych i rozwiązywanie problemów produkcyjnych,', 'praca zdalna w zespole międzynarodowym (+1h różnicy względem polskiej strefy czasowej),', 'stawka do 130 zł/h przy B2B w zależności od doświadczenia.']\n",
      "requirements: ['masz minimum 3 lata doświadczenia w pracy jako Software Developer,', 'masz doświadczenie w opracowywaniu produktów zorientowanych na dane przy użyciu języka Java lub Python,', 'masz doświadczenie w pracy z frameworkami i narzędziami do przetwarzania danych takimi jak Spark, Pandas, Hadoop, Airflow,', 'masz doświadczenie w pracy z rozproszonymi bazami danych (Elasticsearch, MongoDB, Redis itp.),', 'masz doświadczenie w developmencie architektury mikroserwisowej,', 'masz doświadczenie w pracy w środowiskach skonteneryzowanych, z narzędziami takimi jak Kubernetes, Helm,', 'masz doświadczenie w pracy w Agile.', 'znasz język angielski w stopniu umożliwiającym codzienną, płynną komunikację w środowisku międzynarodowym (min. B2+/C1).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-developer-java-python-wroclaw-powstancow-slaskich-9,oferta,1003890374?apid=534a8a6d-505c-451c-8a2e-5631d5d2ac27&oca=None&acv=0\n",
      "\n",
      "--- Job Record 129 ---\n",
      "id: 182\n",
      "url: https://www.pracuj.pl/praca/ms-sql-developer-warszawa-postepu-17a,oferta,1003917593\n",
      "title: MS SQL Developer\n",
      "work_location: None\n",
      "validity: valid for 23 daysto 21 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: None\n",
      "additional_info: ['Postępu 17A, Mokotów, WarszawaWarszawa, Masovian']\n",
      "technologies: ['T-SQL', 'Microsoft SQL Server', 'SQL', 'Microsoft Power BI', 'Azure DevOps', 'Microsoft Azure', 'SAP', 'in house', 'you develop several projects simultaneously', 'you can change the project', 'you have influence on the technological solutions applied', 'you focus on product development', 'agile', 'technical leader', 'architect', 'devOps', 'big data developer', 'data scientist', 'business analyst', 'SAP specialist']\n",
      "responsibilities: ['Manage development of MS SQL based business solutions for our Customers', 'Work with complex datasets by gathering business requirements, designing data models, and integrating multiple data sources', 'Cooperate with multinational teams to develop and sustain MS SQL based products', 'Use of automate release/code deployment process for various business cases']\n",
      "requirements: ['Minimum 2-3 years of professional hands-on experience with T-SQL Programming with exposure to MS SQL Server an enterprise production environment', 'Familiarity with SQL Back-end development of stored procedures, keys, constraints, triggers, functions etc.', 'Expert level knowledge of designing, constructing, administering and maintaining data warehouses', 'Strong Ownership and Passion', 'Consulting mindset', 'Problem solving', 'Must have: ability to communicate in spoken & written English; Polish is nice to have:)', 'Translate requirements from the business and analyst into technical code', 'Hybrid work with 2-3 days at the Warsaw office']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ms-sql-developer-warszawa-postepu-17a,oferta,1003917593?apid=749d4b74-d90a-4aca-8106-49631c23121b&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 130 ---\n",
      "id: 183\n",
      "url: https://www.pracuj.pl/praca/jira-developer-wroclaw-antoniego-slonimskiego-1,oferta,1003929613\n",
      "title: Jira Developer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze miesiącdo 27 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca zdalna, praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Jira', 'Git', 'Jenkins']\n",
      "responsibilities: ['Administracja i konfiguracja systemu Jira w celu zapewnienia optymalnego działania', 'Tworzenie i zarządzanie projektami w Jira, włączając w to tworzenie schematów issue, tablic kanban i scrum, konfigurację workflow oraz zarządzanie dostępem użytkowników', 'Tworzenie i dostosowywanie paneli i raportów w Jira w celu monitorowania postępów projektów', 'Wprowadzanie zmian i uaktualnień w systemie Jira w zależności od potrzeb użytkowników i organizacji', 'Szkolenie i wsparcie użytkowników Jira w zakresie korzystania z platformy oraz rozwiązywanie problemów technicznych związanych z Jira', 'Monitorowanie i analiza wydajności systemu Jira, identyfikowanie potencjalnych problemów i proponowanie rozwiązań', 'Wdrażanie nowych funkcji i modułów do systemu Jira w celu zapewnienia rozbudowanej funkcjonalności', 'Współpraca z innymi administratorami systemów w organizacji w celu integracji Jira z innymi narzędziami i systemami', 'Śledzenie i stosowanie najnowszych aktualizacji i poprawek w systemie Jira w celu zapewnienia zgodności z wymaganiami bezpieczeństwa i funkcjonalnością.', 'Obsługa dodatków m.in eazyBI']\n",
      "requirements: ['Znajomość Atlassian Jira: Doświadczenie w pracy z Jira jako użytkownik, zarządzanie projektami, konfiguracja środowiska, tworzenie i zarządzanie tablicami, raportami, oraz przepływami pracy', 'Znajomość Atlassian Marketplace: Umiejętność identyfikacji, oceny i implementacji istniejących wtyczek z Atlassian Marketplace, które mogą poprawić funkcjonalność Jiry w organizacji', 'Doświadczenie w integracji: Umiejętność integrowania Jiry z innymi narzędziami i usługami, takimi jak systemy kontroli wersji (np. Git), narzędzia CI/CD (np. Jenkins), narzędzia do zarządzania kodem (np. Bitbucket) oraz inne narzędzia Atlassian', 'Zrozumienie procesów biznesowych: Zrozumienie typowych procesów biznesowych i przepływów pracy w organizacjach, co pozwala na lepsze dostosowanie Jiry do potrzeb użytkowników', 'Umiejętności analityczne i rozwiązywania problemów: Zdolność do analizowania wymagań, identyfikowania problemów oraz proponowania i implementowania skutecznych rozwiązań', 'Dobre umiejętności komunikacyjne i pracy zespołowej: Komunikacja z zespołem i interesariuszami, zdolność do pracy w zespole, jak również umiejętność pracy niezależnej', 'Znajomość języków skryptowych i API Jiry: Znajomość REST i Java API Jiry umożliwiających automatyzację zadań i integrację z innymi systemami']\n",
      "application_link: https://www.pracuj.pl/aplikuj/jira-developer-wroclaw-antoniego-slonimskiego-1,oferta,1003929613?apid=78ddc922-fb2d-4375-9e06-f735fe96dd59&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 131 ---\n",
      "id: 184\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-utrzymania-oprogramowania-warszawa-adama-branickiego-13,oferta,1003897484\n",
      "title: Specjalista ds. Utrzymania Oprogramowania\n",
      "work_location: None\n",
      "validity: ważna jeszcze 14 dnido 12 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), młodszy specjalista (Junior)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Adama Branickiego 13, Wilanów, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL']\n",
      "responsibilities: ['diagnoza i rozwiązywanie problemów zgłaszanych przez Klientów (również obcojęzycznych),', 'analiza potrzeb klienta,', 'zrozumienie/uzupełnienie informacji o zgłoszonym przez klienta problemie,', 'przygotowanie i realizacja przypadków testowych,', 'współpraca i konsultacje rozwiązań technicznych z zespołami produktowymi i deweloperskimi,', 'sformułowanie kompleksowej odpowiedzi bądź opisanie przypadku testowego i skierowanie problemu do naprawy,', 'przygotowanie dokumentacji wewnętrznej.']\n",
      "requirements: ['podstawowa znajomość SQL,', 'znajomość języka angielskiego w stopniu komunikatywnym,', 'umiejętności analityczne, łatwość przyswajania wiedzy, spostrzegawczość, komunikatywność, samodzielność.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-utrzymania-oprogramowania-warszawa-adama-branickiego-13,oferta,1003897484?apid=70d5b38b-49c3-4f56-8e43-c6c08145285d&oca=None&acv=0\n",
      "\n",
      "--- Job Record 132 ---\n",
      "id: 185\n",
      "url: https://www.pracuj.pl/praca/data-analyst-in-coo-office-warszawa-grzybowska-78,oferta,1003887723\n",
      "title: Data Analyst in COO Office\n",
      "work_location: None\n",
      "validity: valid for 9 daysto 07 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Grzybowska 78, Wola, WarszawaWarszawa, Masovian', 'contract of employment', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['SQL']\n",
      "responsibilities: ['Creating and maintaining of data lakes, data models and data flows using Alteryx & Power Bi to ensure managerial data system are organized in the efficient way', 'Defining  and presenting visualization from multiple data sources regarding headcount forecasting to support management in decision making', 'Continually evolving reporting framework to ensure relevance and alignment with business environment', 'Participation in various projects aiming at stabilizing our data information workflow in line with business needs']\n",
      "requirements: ['1-3 years of experience with database and model design and segmentation techniques. Proven experience in using statistical packages.', 'Knowledge of SQL, Excel and Power BI with aptitude for learning other analytics tools', 'Alteryx knowledge would be an asset', 'Experience in producing dashboards for C-level management and ability to present data in a synthetic way', 'Excellent communication skills. Fluent English, additionally French will be an advantage.', 'Strong time management and organizational skills. Proactiveness to adjust strategy to integrate new internal and external factors that might arise', 'Detail and goal - oriented mindset']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-analyst-in-coo-office-warszawa-grzybowska-78,oferta,1003887723?apid=be82c936-1a62-4323-9bc6-5eb9a89f7459&oca=None&acv=0\n",
      "\n",
      "--- Job Record 133 ---\n",
      "id: 186\n",
      "url: https://www.pracuj.pl/praca/inzynier-etl-warszawa,oferta,1003914810\n",
      "title: Inżynier ETL\n",
      "work_location: None\n",
      "validity: ważna jeszcze 22 dnido 20 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['ETL', 'PL/SQL', 'Apache Hadoop', 'SSAS', 'SSRS', 'DAX', 'MDX']\n",
      "responsibilities: ['Projektowanie i implementacja procesów ETL w oparciu o SQL Server Integration Services (SSIS) oraz Pentaho Data Integration (PDI)', 'Optymalizacja procesów ETL oraz zarządzanie wydajnością systemów', 'Integracja danych z różnych źródeł oraz zapewnienie spójności danych', 'Tworzenie raportów i analiz opartych na danych z procesów ETL', 'Współpraca z zespołem analityków i programistów w zakresie wymagań technicznych', 'Rozwiązywanie problemów związanych z procesami ETL i optymalizacja istniejących rozwiązań', 'Tworzenie dokumentacji technicznej oraz raportów z realizowanych procesów ETL', 'Dbałość o jakość danych oraz monitorowanie wydajności rozwiązań ETL']\n",
      "requirements: ['Minimum 3-letnie doświadczenie w pracy z ETL i relacyjnymi bazami danych, szczególnie Oracle', 'Umiejętność programowania w PL/SQL', 'Umiejętność analizy wymagań i tworzenia dokumentacji technicznej', 'Znajomość języka polskiego na poziomie umożliwiającym swobodną komunikację']\n",
      "application_link: https://www.pracuj.pl/aplikuj/inzynier-etl-warszawa,oferta,1003914810?apid=13b91e6b-1b8e-40a5-a2a2-015f387ca916&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 134 ---\n",
      "id: 187\n",
      "url: https://www.pracuj.pl/praca/senior-data-analyst-krakow,oferta,1003897452\n",
      "title: Senior Data Analyst\n",
      "work_location: None\n",
      "validity: valid for 14 daysto 12 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KrakówKraków, Lesser Poland', 'Запрошуємо працівників з України']\n",
      "technologies: ['SAS', 'Python', 'PySpark', 'SQL', 'Hadoop', 'JIRA', 'Confluence', 'Prophecy', 'in house', 'you focus on a single project at a time', 'you develop several projects simultaneously', 'you can change the project', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'agile', 'scrum', 'waterfall']\n",
      "responsibilities: ['Learn and understand source systems data, models and data standards.', 'Understanding data requirements from model development & monitoring teams, working on DQ issues, liaising with project teams for new ingests or data assets, working with teams to ensure data governance and compliance.', 'Develop ETL pipelines to make data assets available on GRADE.', 'Contribute to designing roadmaps for data, application and business process architecture to support future state environments.', 'Prepare effective material for dissemination to key business stakeholders at all levels of seniority.', 'Effectively manage relationships across key functional teams.', 'Work with management to prioritize business and information needs.', 'Encourage and drive process improvement opportunities.']\n",
      "requirements: ['Solid and relevant experience in a similar job role.', 'Bachelor’s degree in IT or a numerate subject; Master’s degree preferred.', 'Strong theoretical and practical knowledge of programming tools, including proficiency in at least one of the following tools: SAS, Python, PySpark, SQL, and Hadoop Big Data, experience with Prophecy is a plus.', 'Business Analysis – requirements gathering and specification,', 'Ability to comprehend complex data architecture.', 'Strong organizational, analytical, problem-solving, and project management skills.', 'Multitasking ability and the capacity to work under pressure within tight timelines are essential.', 'Working knowledge of JIRA, Confluence, and Monday.com.', 'Open personality with effective communication skills and the flexibility to work in an international team.', 'Previous experience in IT/Analytics is desirable.', 'Preferable knowledge of banking risk data, systems, and risk models.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-analyst-krakow,oferta,1003897452?apid=f26dae18-6309-4621-8a73-c00d45522acf&oca=None&acv=0\n",
      "\n",
      "--- Job Record 135 ---\n",
      "id: 188\n",
      "url: https://www.pracuj.pl/praca/master-data-administrator-company-structure-katowice-ksiedza-piotra-sciegiennego-3,oferta,1003920141\n",
      "title: Master Data Administrator (Company Structure)\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Księdza Piotra Ściegiennego 3, KatowiceKatowice, Silesian', 'contract of employment']\n",
      "technologies: []\n",
      "responsibilities: ['Work closely with internal clients (Business Controllers and Business Unit Leaders, local HR Managers) by supporting their Master Data needs and ensuring data quality through emails and calls.', 'Monitor employee identification Master Data quality, drive its day-to-day management and propose improvements to relevant governance processes.', 'Analyze Company Structure Master Data to derive data quality KPIs and ensure effective communication with local leaders. Drive actions based on KPIs.', 'Monitor and continuously improve Master Data management processes, ensuring they are being adhered to across the company.', 'Collaborate with project and IT development teams to refine and improve the Master Data maintenance application (e.g.by participating to UAT, User Requirements definition).']\n",
      "requirements: ['Experience: At least 2 years of experience in Master Data management or related fields.', 'Education: Bachelor’s degree.', 'Languages: Good communication skills in English.', 'Skills: Strong analytical and problem-solving skills, with proficiency in Excel. Customer-focused mindset.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/master-data-administrator-company-structure-katowice-ksiedza-piotra-sciegiennego-3,oferta,1003920141?apid=9f318ad8-81f9-4a77-be74-4995156d45d1&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 136 ---\n",
      "id: 189\n",
      "url: https://www.pracuj.pl/praca/inzynier-big-data-warszawa,oferta,1003914700\n",
      "title: Inżynier Big Data\n",
      "work_location: None\n",
      "validity: ważna jeszcze 22 dnido 20 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Scala', 'Python', 'Apache Hadoop', 'Spark', 'Hive', 'HDFS', 'NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'SQL', 'Linux', 'RESTful API', 'SOAP/ESB']\n",
      "responsibilities: ['Implementacja rozwiązań Big Data,', 'Praca z bazami NoSQL,', 'Integracja systemów,', 'Optymalizacja wydajności rozwiązań Hadoop/Spark.']\n",
      "requirements: ['Min. 5-letnie doświadczenie w Scala/Python,', 'Doświadczenie w Apache Hadoop & Family (Spark, Hive, HDFS),', 'Znajomość NoSQL (MongoDB, Cassandra, HBase),', 'Znajomość SQL, Linux, RESTful API, SOAP/ESB,', 'Znajomość języka polskiego na poziomie swobodnej komunikacji.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/inzynier-big-data-warszawa,oferta,1003914700?apid=19a875ef-a1ee-4f6b-9452-f4591b378d19&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 137 ---\n",
      "id: 190\n",
      "url: https://www.pracuj.pl/praca/information-technology-it-data-lims-labvantage-senior-application-owner-warszawa-bobrowiecka-8,oferta,1003854558\n",
      "title: Information Technology IT & Data - LIMS (LabVantage) Senior Application Owner\n",
      "work_location: None\n",
      "validity: valid for 9 daysto 07 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior), expert\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Bobrowiecka 8, Mokotów, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: []\n",
      "responsibilities: ['end to end responsibility for tech & data solutions in its scope', 'defining & executing roadmap', 'being accountable for support', 'being the business partner for business process & product owners', 'delivering solutions on the items that have the right impact & relevance', 'optimizing IT costs on its products', 'putting impact on value creation optimization through internal process and partner collaboration optimization', 'KEY DELIVERABLES', 'IT product core design & roadmap', 'business partnering & prioritization', 'IT product maintenance & enhancement – with the right level of support', 'benchmarking vs peers/market – tech trends, new functionalities', 'delighting user experience through technology, data & automation']\n",
      "requirements: ['several years in IT Quality or Operations, including consulting, with a high level of tech-savviness', 'advanced skills and experience with LabVantage LIMS are essential', 'experience in managing IT products with end-to-end responsibility', 'good knowledge of Value Chain processes, data & solutions architecture', 'high knowledge of Quality processes, data & solutions architecture', 'fluency in English']\n",
      "application_link: https://www.pracuj.pl/aplikuj/information-technology-it-data-lims-labvantage-senior-application-owner-warszawa-bobrowiecka-8,oferta,1003854558?apid=b0620b36-3516-4704-8de9-c98db505ccac&oca=None&acv=0\n",
      "\n",
      "--- Job Record 138 ---\n",
      "id: 191\n",
      "url: https://www.pracuj.pl/praca/senior-mlops-with-aws-warszawa-pulawska-2,oferta,1003877583\n",
      "title: Senior MLOps with AWS\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 4 dnido 02 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior), ekspert\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['AWS', 'Python', 'Bash', 'wewnątrz organizacji', 'agile']\n",
      "responsibilities: ['Research and integrate MLOps tools and frameworks.', 'Improve MLOps maturity across projects.', 'Promote agile, automated Data Science practices.', 'Conduct internal training on MLOps.']\n",
      "requirements: ['Experience with Kubernetes, AWS, Kubeflow/SageMaker.', 'Proficiency in Python, Bash/Unix.', 'Experience in CI/CD pipelines and ML model development.', 'Strong troubleshooting and communication skills.', 'English (B2+).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-mlops-with-aws-warszawa-pulawska-2,oferta,1003877583?apid=70ddbfde-ea58-4cd0-8c47-46fc329f3f6a&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 139 ---\n",
      "id: 192\n",
      "url: https://www.pracuj.pl/praca/product-manager-warszawa-dobra-40,oferta,1003890217\n",
      "title: Product Manager\n",
      "work_location: None\n",
      "validity: ważna jeszcze 10 dnido 08 marca 2025\n",
      "contract_type: umowa zlecenie, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: None\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Dobra 40, Śródmieście, WarszawaWarszawa, mazowieckie', 'kierownik / koordynator, menedżer']\n",
      "technologies: ['wewnątrz organizacji', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'product owner', 'analityk biznesowy']\n",
      "responsibilities: ['Definiowanie wizji i strategii dla produktów Grupy Cloud Technologies w oparciu o potrzeby rynku, analizę konkurencji i cele biznesowe.', 'Tworzenie i zarządzanie roadmapami produktów oraz wyznaczanie priorytetów.', 'Przeprowadzanie badań rynkowych, potrzeb klientów, działań konkurencji oraz analizy trendów.', 'Zbieranie i analizowanie feedbacku od użytkowników, klientów i interesariuszy.', 'Współpraca z zespołami technicznymi, projektowymi, marketingowymi i prawnymi w celu opracowania i wdrożenia nowych funkcjonalności.', 'Nadzór nad rozwojem produktów, zapewnienie zgodności z harmonogramem i budżetem.', 'Współpraca z działami marketingu, sprzedaży i obsługi klienta w zakresie komunikacji wartości produktów.', 'Śledzenie kluczowych wskaźników wydajności (KPI) produktów.', 'Identyfikowanie i wdrażanie usprawnień w oparciu o dane i analizy.']\n",
      "requirements: ['Doświadczenie w zarządzaniu produktem w branży nowych technologii.', 'Umiejętność analizy rynku oraz danych.', 'Proaktywność i gotowość do działania.', 'Umiejętność budowania i utrzymywania relacji biznesowych z klientami oraz interesariuszami.', 'Doskonałe umiejętności komunikacyjne.', 'Umiejętność tworzenia i prezentowania koncepcji produktu, roadmapy i statusu projektu różnym zespołom i interesariuszom.', 'Biegła znajomość języka angielskiego (poziom C1).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/product-manager-warszawa-dobra-40,oferta,1003890217?apid=698ad592-6527-4943-9b10-593abda66149&oca=None&acv=0\n",
      "\n",
      "--- Job Record 140 ---\n",
      "id: 193\n",
      "url: https://www.pracuj.pl/praca/stazysta-ai-lodz-skladowa-35,oferta,1003908187\n",
      "title: Stażysta AI\n",
      "work_location: None\n",
      "validity: ważna jeszcze ponad miesiącdo 31 marca 2025\n",
      "contract_type: umowa o staż / praktyki\n",
      "employment_type: pełny etat\n",
      "position: None\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Składowa 35, Śródmieście, ŁódźŁódź, łódzkie', 'praktykant / stażysta', 'praca stacjonarna', 'Szukamy wielu kandydatówwakaty: 5']\n",
      "technologies: ['Python', 'JavaScript', 'Linux', 'AWS', 'Confluence', 'Jira', 'wewnątrz organizacji', 'koncentrujesz się na jednym projekcie', 'agile']\n",
      "responsibilities: ['Przyczynianie się do rozwoju projektu z zakresu IT', 'Tworzenie dokumentacji projektowej', 'Rozwój kompetencji własnych', 'Wsparcie działu innowacji', 'Prezentacja postępów prac przed szeroką publiką w języku angielskim', 'Współpraca z działami wspierającymi (księgowość, dział prawny, administracja, HR)', 'Raportowanie działań własnych i zespołu', 'Implementacja nowych funkcjonalności w naszych zrównoważonych projektach', 'Testowanie własnych rozwiązań']\n",
      "requirements: ['Język angielski na poziomie średniozaawansowanym, pozwalającym na swobodną komunikację', 'Dobra organizacja pracy', 'Podstawowa znajomość Javascript', 'Podstawowa znajomość Python', 'Zainteresowanie i chęć rozwoju w dziedzinie IT', 'Umiejętność pracy w zespole, komunikatywność, odwaga i analityczne podejście do rozwiązywania problemów']\n",
      "application_link: https://www.pracuj.pl/aplikuj/stazysta-ai-lodz-skladowa-35,oferta,1003908187?apid=0b10becd-807e-42dd-9d68-88007ece095e&oca=None&acv=0\n",
      "\n",
      "--- Job Record 141 ---\n",
      "id: 194\n",
      "url: https://www.pracuj.pl/praca/treasury-data-engineer-warszawa-pulawska-2,oferta,1003877527\n",
      "title: Treasury Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 4 daysto 02 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Puławska 2, Mokotów, WarszawaWarszawa, Masovian']\n",
      "technologies: ['SQL', 'Python', \"at the client's site\"]\n",
      "responsibilities: ['Design, implement, and maintain efficient data pipelines linking core platforms with SAP systems.', 'Manage ETL processes to ensure high-quality data integration and reliability.', 'Collaborate with IT and business teams to ensure seamless data integration across platforms (including tools like DataIku, Exasol, and APOS).', 'Utilise SQL, Python, and other relevant tools for data processing and analysis.', 'Work with business stakeholders to understand data requirements and deliver actionable insights.', 'Document all data pipelines, processes, and models in detail.', 'Identify areas for improvement and implement best practices in data engineering.', 'Develop and maintain data mappings, transformation rules, and integration specifications.', 'Provide expert support during the deployment of new functionalities and assist with troubleshooting.', 'Offer ongoing support and knowledge sharing to both end-users and team members.', 'Ensure the availability of required data attributes, including frequency and granularity.', 'Review and challenge taxonomy requirements related to hierarchical data structures.']\n",
      "requirements: ['Proven experience in data management and/or execution of data operating models, particularly in transformation projects.', 'A minimum of 5 years of overall work experience, with at least 2 years of relevant data engineering experience.', 'Ability to work independently and take initiative.', 'Strong communication and presentation skills, with the ability to manage senior stakeholders.', 'A track record of delivering high-quality performance and acting as a role model of the bank’s core values.', 'Highly motivated with a strong work ethic and attention to detail.', 'Experience in data management and operating models within a Tier 1 Bank.', 'Familiarity with organisational design and process implementation to support regulatory financial reporting.', 'A deep understanding of end-to-end data operating models.', 'Ability to display authority, integrity, and ethical standards in all work.', 'Exposure to business architecture principles and methods.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/treasury-data-engineer-warszawa-pulawska-2,oferta,1003877527?apid=aad84a79-fcf6-46cb-be7e-e72f68eb0369&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 142 ---\n",
      "id: 195\n",
      "url: https://www.pracuj.pl/praca/ml-ops-engineer-warszawa-pulawska-2,oferta,1003877510\n",
      "title: ML Ops Engineer\n",
      "work_location: Company locationPuławska 2, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for 4 daysto 02 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), junior specialist (Junior)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['Microsoft Azure', 'Python', 'Databricks', 'SQL', 'in house', 'agile']\n",
      "responsibilities: ['Optimize and implement data science and ML solutions in Azure cloud environments', 'Lead the full lifecycle of data science projects, leveraging best practices like DevOps, CI/CD, and model management', 'Collaborate with engineering to improve data consumption', 'Manage and troubleshoot ML pipelines, ensuring smooth operations']\n",
      "requirements: ['Minimum 2 years of experience in ML Ops or ML engineering', 'Familiarity with Azure cloud and Databricks', 'Proficiency in Python, SQL, and Git', 'Excellent written and spoken English']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ml-ops-engineer-warszawa-pulawska-2,oferta,1003877510?apid=bcaa1b9a-858c-4f41-a702-ad25a84f0489&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 143 ---\n",
      "id: 196\n",
      "url: https://www.pracuj.pl/praca/tester-ai-llm-warszawa-aleje-jerozolimskie-181-b,oferta,1003929409\n",
      "title: Tester (AI / LLM)\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze miesiącdo 27 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Cypress', 'Playwright', 'Git', 'Jira', 'IBM Watson', 'ChatGPT', 'LlamaIndex', 'koncentrujesz się na jednym projekcie', 'rozwijasz kilka projektów jednocześnie', 'możesz zmienić projekt', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne']\n",
      "responsibilities: ['Współpraca z klientem i zespołem projektowym', 'Tworzenie strategii testowej i raportowanie wyników', 'Opracowywanie przypadków i skryptów testowych', 'Przeprowadzanie różnych rodzajów testów', 'Tworzenie i aktualizacja dokumentacji testowej', 'Przygotowanie raportów z testów', 'Rejestracja wyników testów, zgłaszanie incydentów', 'Analiza i obsługa błędów zgłaszanych przez klienta']\n",
      "requirements: ['Minimum 5 lat doświadczenia w testowaniu oprogramowania', 'Udokumentowane doświadczenie w min. 3 projektach z zakresu QA', 'Doświadczenie w tworzeniu procedur i procesów testowych', 'Znajomość cyklu rozwoju oprogramowania i wiedza z zakresu IT', 'Znajomość języka angielskiego na poziomie umożliwiającym swobodną komunikację z klientem oraz analizę dokumentacji technicznej', 'Bardzo wysokie umiejętności pracy w zespole', 'Odpowiedzialność, samodzielność, chęć do kontaktu z klientem', 'Umiejętność pracy w środowisku Scrum']\n",
      "application_link: https://www.pracuj.pl/aplikuj/tester-ai-llm-warszawa-aleje-jerozolimskie-181-b,oferta,1003929409?apid=570653a1-f3b7-4a28-9e60-5646dc7260ff&oca=None&acv=0\n",
      "\n",
      "--- Job Record 144 ---\n",
      "id: 197\n",
      "url: https://www.pracuj.pl/praca/senior-etl-developer-warszawa,oferta,1003908045\n",
      "title: Senior ETL Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 18 dnido 16 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatów']\n",
      "technologies: ['BigQuery', 'u klienta', 'koncentrujesz się na jednym projekcie', 'możesz zmienić projekt', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'agile']\n",
      "responsibilities: ['Praca z technologiami DataStage / Ab Initio', 'Stawianie i konfiguracja środowisk', 'Automatyzacja ścieżki wdrożenia całego środowiska', 'Wsparcie zespołu w procesach migracyjnych do BigQuery', 'Usprawnianie istniejących systemów on-premise']\n",
      "requirements: ['Minimum 5 lat doświadczenia w pracy z DataStage / Ab Initio', 'Znajomość procesów automatyzacji wdrożeń i konfiguracji środowisk', 'Umiejętność pracy w środowisku on-premise']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-etl-developer-warszawa,oferta,1003908045?apid=e2f3e95d-a559-449b-9eba-08f088f14c7e&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 145 ---\n",
      "id: 198\n",
      "url: https://www.pracuj.pl/praca/data-engineer-warszawa,oferta,1003908046\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 18 daysto 16 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'More than one vacancy']\n",
      "technologies: ['Python', 'SQL', 'MDX', 'Bash', 'Microsoft Azure', 'Airflow', 'PySpark', 'Scala', 'Snowflake Data Cloud', \"at the client's site\", 'you focus on a single project at a time', 'you can change the project', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'agile']\n",
      "responsibilities: ['Build, optimize, and maintain ETL/ELT pipelines for advanced analytics and business intelligence', 'Configure and manage data storage solutions, including SQL databases and data lakes', 'Oversee production data pipelines, ensuring seamless connections within the infrastructure', 'Advocate for best engineering practices, including automation, CI/CD, and maintainability']\n",
      "requirements: ['Around 2-3 years of experience in data engineering', 'Programming skills in Python, SQL, MDX, and Bash scripting', 'Hands-on experience with Azure data services (e.g. Data Factory, Synapse, Data Lake, Blob Storage, SharePoint, Databricks)', 'Experience with data orchestration tools (ADF, Databricks Jobs, or Airflow)', 'Experience with PySpark / Scala', 'Familiarity with Snowflake and its integration with Azure Cloud - nice to have', 'Very good English - B2+']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-warszawa,oferta,1003908046?apid=1ebf1e69-84b9-4e69-b1a9-d888fc3dd2c6&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 146 ---\n",
      "id: 199\n",
      "url: https://www.pracuj.pl/praca/data-scientist-warszawa,oferta,1003908048\n",
      "title: Data Scientist\n",
      "work_location: None\n",
      "validity: valid for 18 daysto 16 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'More than one vacancy']\n",
      "technologies: ['Python', 'SQL', 'PySpark', \"at the client's site\", 'you focus on a single project at a time', 'you can change the project', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'agile']\n",
      "responsibilities: ['Actively participate in each stage of the data science lifecycle, from business understanding through model development to industrialization', 'Design, build and maintain statistical/machine learning models to optimize business operations', 'Explore and experiment with innovative ML techniques and approaches to maximize value generated by the team', 'Communicate model & analysis results to a non-technical audience that includes stakeholders and senior management']\n",
      "requirements: ['At least 2 years of experience in building and industrializing machine learning solutions (especially traditional ML and time series)', 'Very good command of Python and its use with version control (Git) and testing', 'Practical knowledge of SQL, PySpark & Databricks', 'Experience in application of ML concepts and methodologies (regression and classification, time series modeling, feature engineering and selection, regularization etc.)', 'Knowledge of ML techniques and algorithms (incl. various regression types, ensembles, clustering, decision trees, boosting, etc.) and their pros and cons', 'Ability to translate business problems into ML solutions and communicae effectively with a variety of stakeholders', 'Fluent English – at least B2+/C1']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-warszawa,oferta,1003908048?apid=d1d64622-06e4-4b4b-954d-de00c0121873&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 147 ---\n",
      "id: 200\n",
      "url: https://www.pracuj.pl/praca/data-scientist-warszawa,oferta,1003908049\n",
      "title: Data Scientist\n",
      "work_location: None\n",
      "validity: ważna jeszcze 18 dnido 16 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatów']\n",
      "technologies: ['Python', 'SQL', 'u klienta', 'koncentrujesz się na jednym projekcie', 'możesz zmienić projekt', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'agile']\n",
      "responsibilities: ['Budowa inteligentnych rozwiązań służących automatyzacji i optymalizacji procesów obsługowych', 'Opracowywanie i realizacja złożonych analiz statystycznych, ekonometrycznych i uczenia maszynowego w zakresie efektywności procesowej', 'Projektowanie i implementacja nowych Data Martów (ABT) oraz procesów raportowych na platformie Hurtowni Danych/Business Intelligence', 'Rozwój i utrzymanie procesów HD, np. budowanie Data Martów m.in. pod procesy modelowania/scoringu']\n",
      "requirements: ['2-letnie doświadczenie komercyjne w roli Data Scientist', 'Dobra znajomość Pythona i/lub 4GL oraz podstaw relacyjnych baz danych (SQL)', 'Doświadczenie w pisaniu produkcyjnych rozwiązań ML oraz z testami danych, testami jednostkowymi', 'Znajomość rozwiązań metod ilościowych: zaawansowanych modeli ekonometrycznych, modeli', 'Klasyfikacyjnych, rachunku prawdopodobieństwa', 'Wykształcenie wyższe (Data Science / Ekonometria / Matematyka itp.)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-warszawa,oferta,1003908049?apid=a5fad6f1-921f-49ef-a552-3db0cb76f642&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 148 ---\n",
      "id: 201\n",
      "url: https://www.pracuj.pl/praca/data-engineer-warszawa,oferta,1003908040\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 18 dnido 16 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatów']\n",
      "technologies: ['Python', 'Apache Airflow', 'PostgreSQL', 'Docker', 'Kubernetes', 'SSIS', 'u klienta', 'koncentrujesz się na jednym projekcie', 'możesz zmienić projekt', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'agile', 'scrum']\n",
      "responsibilities: ['Tworzenie wewnętrznej platformy AI/ML, która hostuje modele ML i umożliwia Data Scientistom testowanie oraz wdrażanie tych modeli', 'Ścisła współpraca z zespołem DevOps w celu optymalizacji procesów', 'Rozszerzenie platformy, aby stała się wszechstronnym narzędziem dla różnych zespołów, w tym do modelowania ryzyka i automatyzacji']\n",
      "requirements: ['3-4-letnie doświadczenie komercyjne w roli Data Engineera', 'Bardzo dobra znajomość: Python, Apache Airflow, PostgreSQL', 'Doświadczenie z LLM i projektami NLP']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-warszawa,oferta,1003908040?apid=82ed9ffc-b0db-4ac7-95a7-9dff67eef5fe&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 149 ---\n",
      "id: 202\n",
      "url: https://www.pracuj.pl/praca/mlops-engineer-warszawa,oferta,1003908043\n",
      "title: MLOps Engineer\n",
      "work_location: None\n",
      "validity: valid for 18 daysto 16 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'More than one vacancy']\n",
      "technologies: ['Microsoft Azure', 'Python', 'SQL', \"at the client's site\", 'you focus on a single project at a time', 'you can change the project', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'agile']\n",
      "responsibilities: ['Optimize, standardize and implement data science and machine learning solutions at scale and in cloud-based environments (Azure)', 'Take part in the end-to-end lifecycle of data science projects through the use of DevOps, code, experiment and model management, CI/CD and other best practices', 'Cooperate with engineering to improve the way of consuming data and deploying models in production', 'Design and lead monitoring, troubleshooting, debugging and incident management for ML pipelines']\n",
      "requirements: ['At least 2 years of experience in MLOps or ML engineering / similar role, particularly in productionizing and scaling ML models', 'Experience working with Azure cloud environment and Databricks - MUST HAVE', 'Proficiency with Python and SQL (while building ML & data pipelines)', 'Understanding of data science lifecycle and the way data scientists work to deliver value', 'Fluent English – at least B2+/C1']\n",
      "application_link: https://www.pracuj.pl/aplikuj/mlops-engineer-warszawa,oferta,1003908043?apid=c29749db-097c-4e3a-bd07-5687b4cdad83&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 150 ---\n",
      "id: 203\n",
      "url: https://www.pracuj.pl/praca/senior-azure-data-engineer-krakow,oferta,1003877357\n",
      "title: Senior Azure Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 4 daysto 02 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: None\n",
      "additional_info: ['KrakówKraków, Lesser Poland']\n",
      "technologies: ['Azure Databricks', 'Azure SQL', 'Synapse', 'Spark', 'ETL', 'Azure', 'Delta Lake', 'Delta Live', 'Python', 'SQL']\n",
      "responsibilities: ['mplementing data products and solutions based on business & IT requirements on the Analytics Enablement platform', 'Building data pipelines, implementing data integration solutions, and data models following AEP and client’s standards in Azure']\n",
      "requirements: ['5-7+ years experience required in a similar role', 'Independently used Azure Databricks for transformations & processing data', 'Extensive knowledge of Azure Databricks, Spark and clusters', 'Advanced knowledge of Azure SQL & Synapse for modelling and serving data', 'Experience with Azure Functions and Logic Apps', 'Knowledge of Delta Lake and Delta Live Tables', 'Experience in and expert knowledge of ETL and transformation technology across the different technologies', 'Knowledge in Python and SQL, preferably also other programming languages', 'Experience in bringing analytics & reporting solutions from idea and experiment to scalable production and supportable deployments', 'Leadership skills with experience in coaching and mentoring a team', 'Strong problem-solving and root cause identification skills', 'Experience working on complex systems, handling multiple solutions, and participating in large-scale projects', 'Ability to work effectively and build relationships at all levels in an organisation', 'Team player skills, proactivity, and the ability to take initiative', 'Very good time management skills and ability to prioritise', 'A can-do and hands-on attitude while dealing with some uncertainty in requirements & features to deliver MVP solutions', 'An Agile mind-set and operate as part of an Agile Devops team.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-azure-data-engineer-krakow,oferta,1003877357?apid=ee5ec7b8-6cdc-4ff6-a64e-ff9f3a9ce4d6&oca=None&acv=0\n",
      "\n",
      "--- Job Record 151 ---\n",
      "id: 204\n",
      "url: https://www.pracuj.pl/praca/it-compliance-data-engineer-warszawa-pulawska-180,oferta,1003887305\n",
      "title: IT Compliance Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 9 daysto 07 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Puławska 180, Mokotów, WarszawaWarszawa, Masovian', 'contract of employment', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Python', 'C#', 'JavaScript', 'Microsoft Azure', 'SQL']\n",
      "responsibilities: ['Designing and implementing data-based controls assurance and monitoring automation', 'Analysing data, designing exception reports, KPIs/KRIs, and continuous monitoring to detect and respond to threats in real-time', 'Identifying opportunities to improve risk and compliance reporting processes', 'Collaborating across departments to deliver agreed objectives on time and with defined quality', 'Conducting digital/IT security compliance assessments of new initiatives and existing solutions to identify risks, as well as optimal ways of security monitoring', 'Assisting in the development and implementation of the digital/IT security assurance & regulatory compliance framework', 'Contributing to the creation and operation of global digital/technology compliance, both external (e.g., PCI-DSS, SWIFT, NIS2, GDPR/CCPA) and internal (cybersecurity policies and standards)']\n",
      "requirements: ['Master’s degree in Cyber Security, Information Technology, Computer Science, or a related field, or equivalent experience, is required.', 'Experience with data analytics, reporting, and automation tools', 'Proficient in programming languages such as Python, C#, and JavaScript (JS)', 'Some experience in Cloud Engineering, preferably with Microsoft Azure, and SQL databases', 'Understanding of AI/ML concepts and the ability to translate complex cyber concepts into clear recommendations', 'Experience in IT controls, compliance, and/or risk assessments', 'Excellent analytical and problem-solving skills, with a solid foundation in mathematics for AI technologies', 'Hands-on experience in roles related to data analytics, IT controls (data-driven / continuous) assessment, cyber security, IT administration']\n",
      "application_link: https://www.pracuj.pl/aplikuj/it-compliance-data-engineer-warszawa-pulawska-180,oferta,1003887305?apid=4e191c72-1d00-4d13-96f0-99d72a1693f6&oca=None&acv=0\n",
      "\n",
      "--- Job Record 152 ---\n",
      "id: 205\n",
      "url: https://www.pracuj.pl/praca/it-compliance-senior-analyst-warszawa-pulawska-180,oferta,1003887303\n",
      "title: IT Compliance Senior Analyst\n",
      "work_location: None\n",
      "validity: valid for 9 daysto 07 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Puławska 180, Mokotów, WarszawaWarszawa, Masovian', 'contract of employment', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Microsoft Excel', 'Microsoft Power BI', 'Power Automate']\n",
      "responsibilities: ['Assist in the development and implementation of the digital / IT security assurance & regulatory compliance framework.', 'Providing requirements and posture summaries of cyber security maturity and compliance with regulations.', 'Translate security controls requirements / control objectives into potential ways of their assessment, where possible based on data with the use of available tools of IT / security monitoring or dedicated data analysis.', 'Assist in IT controls automation, controls assurance / monitoring automation initiatives. Analyze data from various sources, design and build exception reports, KPIs/KRIs, and continuous monitoring, to detect and respond to threats in real-time.', 'Conduct digital / IT security compliance assessments of new initiatives, as well as existing solutions and infrastructure, to identify potential risks that could impact the organization. Prioritize findings, evaluate potential impact on the organization. Discuss results with relevant partners', 'Perform vendor risk and compliance reviews, based on SOC2Type2 / ISO27k or equivalent, as well as external monitoring (BlueVoyant) reports.', 'Assist in development of mitigation strategies and remediation plans to address identified risks.', 'Identify and implement opportunities to improve risk and compliance reporting processes.']\n",
      "requirements: ['Cybersecurity & IT Risk Expertise: Hands-on experience in cybersecurity, IT administration, or security assessments, with a good understanding of IT controls, compliance, and risk assessments.', 'Communication & Stakeholder Management: Excellent ability to report findings, influence partners, and translate complex cybersecurity concepts into clear recommendations and actions.', 'IT & OT Security Standards: Solid understanding of key security frameworks and standard processes, including NIST, ISO 27001, SOC Trust Criteria, and CIS Controls.', 'Understanding of key cyber security regulations and risk management principles, e.g., GDPR, HIPAA, SOX, NIS2, PCI-DSS.', 'Data Analytics & Automation: Experience with data analysis and reporting tools, including Excel, Power BI, and Power Automate, to support automation and risk assessment processes.', 'Project & Time Management: Target- and deadline-focused, capable of working independently and within matrix teams on multiple assignments in a fast-paced environment.', 'Education: Master’s degree in Cybersecurity, Information Technology, Computer Science, or a related field, or equivalent experience.', 'Certifications: Relevant cybersecurity certifications such as CISA, GSEC, or equivalent.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/it-compliance-senior-analyst-warszawa-pulawska-180,oferta,1003887303?apid=2b474b44-3215-4b44-bfec-052126527db3&oca=None&acv=0\n",
      "\n",
      "--- Job Record 153 ---\n",
      "id: 206\n",
      "url: https://www.pracuj.pl/praca/data-security-engineer-warszawa-pulawska-180,oferta,1003887301\n",
      "title: Data Security Engineer\n",
      "work_location: None\n",
      "validity: valid for 9 daysto 07 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Puławska 180, Mokotów, WarszawaWarszawa, Masovian', 'contract of employment', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Azure', 'AWS']\n",
      "responsibilities: ['Delivers security engineering tasks for Data Security solutions', 'Establishes standards and guidelines, communicating these to all partners to ensure consistency as well as compliance', 'Work DBS application teams to onboard their platforms  into the Data Security solutions', 'Collaborates closely with the vendors that provide solutions for Data Security to BAT (both directly/indirectly)', 'Problem-solve, involving leading teams in identifying, researching, and coordinating the resources necessary to effectively troubleshoot/diagnose complex issues', 'Strives for continuous improvements, while finetuning the configurations and automating repeatable tasks', 'Proactively researches, monitors, learns, and assesses industry/technology advancements', 'Recommend and plan use of new features and functionality to improve overall system performance, security and availability as well as business productivity.']\n",
      "requirements: ['Data Security & Compliance: Strong understanding of data protection regulations (GDPR, HIPAA, CCPA) and experience implementing security solutions like encryption, access controls, DLP tools, and database security', 'Application & Network Security: Experience in designing and securing applications, implementing secure coding practices, performing threat modeling, penetration testing, and ensuring secure network design with firewalls, VPNs, and security monitoring tools', 'Security Testing & Vulnerability Management: Hands-on experience with security assessments, static and dynamic application security testing (SAST, DAST), vulnerability management, and security patching', 'Incident Response & Risk Management: Expertise in handling security incidents, identifying root causes, mitigating risks, and implementing security monitoring through SIEM solutions and log analysis', 'Access Control & Authorization: Proficiency in implementing access control mechanisms, managing authentication/authorization systems, privilege management, and database security hardening', 'Cloud & Mobile Security: Understanding of cloud security standard processes for Azure and AWS, securing cloud-based data, mobile security principles, secure app development, and mobile device management (MDM)', 'Data Governance & Classification: Experience implementing data classification schemes, enforcing governance policies, and ensuring data protection through encryption, tokenization, and compliance frameworks', 'Security Tools & Automation: Hands-on experience with Microsoft Purview, DLP tools, SIEM solutions, security testing tools (SAST, DAST), and automation for security monitoring and vulnerability detection']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-security-engineer-warszawa-pulawska-180,oferta,1003887301?apid=472b645b-5e60-4890-a025-33bdfc36ae88&oca=None&acv=0\n",
      "\n",
      "--- Job Record 154 ---\n",
      "id: 207\n",
      "url: https://www.pracuj.pl/praca/data-architect-france,oferta,11170490\n",
      "title: Data Architect\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Paris, Franceabroad', 'contract of employment', 'full office work']\n",
      "technologies: ['SQL', 'Microsoft Azure', 'Microsoft Power BI', 'Python']\n",
      "responsibilities: ['A postgraduate degree or equivalent qualification in computer science, with a data specialisation.', 'An excellent command of relational database design, normalisation and denormalisation using SQL.', 'Familiarity with enterprise architecture frameworks, ideally backed by TOGAF certification.', 'Experience of producing architectural diagrams and maintaining reference cartography.', 'Proven ability to translate cybersecurity and regulatory compliance requirements into architectural designs.', 'In-depth knowledge of patterns and experience of integration architectures (real-time or near-real time, APL and file-based).', 'Exposure to NoSQL databases and modelling for unstructured data.', 'Awareness of the main types of operational and analytical data architectures (MDM (Master Data Management), data warehouse, data mart, data lake and lakehouse).', 'Full fluency in either English or French, with the commitment to reach a good working level in the other language.']\n",
      "requirements: ['Sound knowledge of data engineering practices (ETL, ELT) and technologies such as Python, Spark and SQL.', 'Certification in or experience of Azure cloud.', 'Familiarity with Microsoft data technologies (Fabric, Power BI, Pureview).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-architect-france,oferta,11170490?apid=8acbbe8f-a47e-4399-8835-2686a1d1e9bc&oca=None&acv=0\n",
      "\n",
      "--- Job Record 155 ---\n",
      "id: 208\n",
      "url: https://www.pracuj.pl/praca/mid-azure-data-engineer-warszawa-pulawska-180,oferta,1003919920\n",
      "title: Mid Azure Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Puławska 180, Mokotów, WarszawaWarszawa, Masovian', 'Запрошуємо працівників з України']\n",
      "technologies: ['T-SQL', 'Python', 'JIRA', 'Azure DevOps', 'scrum', 'kanban']\n",
      "responsibilities: ['Designing and implementing data processing systems on Azure platform. This involves writing efficient and scalable code to process, transform, and clean large volumes of structured and unstructured data.', 'Building data pipelines to ingest data from various sources such as databases, APIs, or streaming platforms.', 'Integrating and transforming data to ensure its compatibility with the target data model or format.', 'Hands-on experience in designing and optimizing of data storage (data lakes, data warehouses, distributed file systems).', 'Familiarity with partitioning, compression, optimization of data storage and retrieval.', 'Identifying and resolving bottlenecks, tuning queries, and implementing caching strategies to enhance data retrieval speed and overall system efficiency.', 'Collaborating with cross-functional teams including data scientists, analysts, and business stakeholders to understand their requirements and provide technical solutions.', 'Independence and responsibility in delivering solutions.', 'Ability to work under agile methodologies.']\n",
      "requirements: ['Proven experience in Azure cloud-based infrastructure, Databricks implementation.', 'Hands on experience with of T-SQL (min. 5 years)', 'Proven experience in Python, PySpark.', 'Very good level of communication including ability to convey information clearly and specifically to co-workers and business stakeholders.', 'Working experience in the agile methodologies – supporting tools (JIRA, Azure DevOps).', 'Experience or familiarity with other cloud technologies, data warehouses, data governance, and business analysis is a plus.', 'English at least at B2 level, ideally C1.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/mid-azure-data-engineer-warszawa-pulawska-180,oferta,1003919920?apid=8eb46708-a2f7-498a-a4cb-7f8b7710a2fa&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 156 ---\n",
      "id: 209\n",
      "url: https://www.pracuj.pl/praca/regular-azure-data-engineer-warszawa-pulawska-180,oferta,1003919914\n",
      "title: Regular Azure Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Puławska 180, Mokotów, WarszawaWarszawa, Masovian', 'Запрошуємо працівників з України']\n",
      "technologies: ['SQL', 'Python', 'scrum', 'kanban']\n",
      "responsibilities: ['Implement data ingestion pipelines using Azure Data Factory, Azure Databricks, and other ETL tools - Create and maintain Azure Data Factory pipelines for data orchestration and Azure Databricks pipelines for data transformation and analytics.', 'Integrate end-to-end data pipelines from structured and unstructured sources (e.g., SQL databases, APIs, event streams) into target data repositories (e.g., Azure Data Lake, Synapse Analytics), ensuring data quality through validation, schema enforcement, and monitoring.', 'Design, build, and manage Azure SQL Databases, focusing on performance tuning, security, indexing, and high availability.', 'Follow best practices for data transformation, ETL optimization, and performance tuning.', 'Provide regular updates on assigned tasks, risks, and dependencies, following agile methodologies (Scrum, Kanban).']\n",
      "requirements: ['2+ years of professional experience in data engineering, preferably with Azure stack solutions.', 'Hands-on experience with Databricks. Proven expertise in cloud data services, data warehousing, big data technologies, and data lakes, particularly within Azure and Databricks ecosystems.', 'Experience with ETL/ELT pipelines using Azure Data Factory and Azure Databricks for data transformations.', 'Strong knowledge of data modeling (e.g., relational, dimensional, or big data architectures), data integration (batch and streaming), and ETL/ELT processes Familiarity with Agile methodologies and tools such as Azure DevOps (ADO) or Jira (nice to have).', 'Proficiency in data engineering and analysis with PySpark, Spark SQL, Python, and SQL.', 'A proactive mindset with strong attention to detail.', 'Readiness to work in cross-functional and cross-cultural environments.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/regular-azure-data-engineer-warszawa-pulawska-180,oferta,1003919914?apid=67644e04-c3ce-4552-851f-72388a51a5c1&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 157 ---\n",
      "id: 210\n",
      "url: https://www.pracuj.pl/praca/junior-azure-data-engineer-warszawa-pulawska-180,oferta,1003919916\n",
      "title: Junior Azure Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: junior specialist (Junior)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Puławska 180, Mokotów, WarszawaWarszawa, Masovian', 'Запрошуємо працівників з України']\n",
      "technologies: ['Python', 'SQL', 'scrum', 'kanban']\n",
      "responsibilities: ['Developing and maintaining data processes by writing source code or updating mapping files.', 'Creation of reports and report modifications in line with requirements.', 'Problem/Incident investigation related to the data sources.', 'Writing documentation and KT to the support team.']\n",
      "requirements: ['About 1 year of experience in Data Engineering.', 'Fluency in Azure services such as Azure Data Factory (ADF), storage account, Databricks.', 'Version control usage via GitHub.', 'Writing code in Python and SQL.', 'Good communication skills and cooperation abilities.', 'English at least at B2 level.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/junior-azure-data-engineer-warszawa-pulawska-180,oferta,1003919916?apid=49009182-2717-4609-b71f-0e11a5aa2459&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 158 ---\n",
      "id: 211\n",
      "url: https://www.pracuj.pl/praca/data-entry-clerk-warszawa,oferta,1003929297\n",
      "title: Data Entry Clerk\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze miesiącdo 27 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Google Office', 'Asana', 'OneTrust', 'Coalfire', 'ServiceNow', 'u klienta']\n",
      "responsibilities: ['Zbieranie, formatowanie, organizowanie, zarządzanie i przechowywanie informacji zebranych jako dowody oraz jednoczesne dokumentowanie odpowiedzi na kwestionariusze zgodności z przepisami dotyczącymi bezpieczeństwa/prywatności,', 'Pomoc w tworzeniu odpowiedzi i składaniu sfinalizowanych odpowiedzi do oryginalnego formatu lub portalu oceny (w razie potrzeby),', 'Wspieranie codziennych czynności związanych z wdrażaniem programu Data Protection Products Compliance Operations.']\n",
      "requirements: ['Wykształcenie wyższe ze specjalizacją w zakresie przetwarzania danych, bezpieczeństwa informacji, analizy danych, zarządzania projektami lub innej pokrewnej dziedziny (lub równoważne doświadczenie),', '5 lat doświadczenia zawodowego, w tym dwa 2 lata praktycznego doświadczenia w działaniach związanych ze zgodnością (przydatne certyfikaty z zakresu bezpieczeństwa informacji, prywatności lub audytu),', 'Znajomość pakietu Google Office — Gmail, Chat, Meet, Kalendarz, Dysk, Dokumenty, Arkusze, Prezentacje, Formularze,', 'Znajomość narzędzi do zarządzania i zgodności, takich jak Asana, OneTrust, Coalfire, ServiceNow lub podobnych,', 'Umiejętność analizy danych,', 'Znajomość zasad zgodności i procesów oceny ryzyka, przepisy dotyczące bezpieczeństwa informacji, prywatności i ochrony danych, regulacje i najlepsze praktyki,', 'Doświadczenie w zakresie standardów zgodności na wszystkich poziomach (lokalnym, stanowym, federalnym i międzynarodowym) oraz znajomość reżimów branżowych, np. NIST, ISO 27000, SOC2, PCI, GDPR itp.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-entry-clerk-warszawa,oferta,1003929297?apid=38c2ddaf-2c5f-408d-b245-8f57f59ebf51&oca=None&acv=0\n",
      "\n",
      "--- Job Record 159 ---\n",
      "id: 212\n",
      "url: https://www.pracuj.pl/praca/architekt-danych-warszawa,oferta,1003914421\n",
      "title: Architekt Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 22 dnido 20 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: None\n",
      "position: starszy specjalista (Senior), ekspert\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['ETL', 'DWH', 'SSIS', 'SSAS', 'SSRS', 'SQL', 'PL/SQL', 'UML', 'Sparx Enterprise Architect', 'Oracle DB 12c', 'OLAP', 'Microsoft Power BI']\n",
      "responsibilities: ['Projektowanie architektury hurtowni danych,', 'Optymalizacja procesów ETL,', 'Implementacja elementów DWH/BI,', 'Modelowanie komponentów architektury DWH/BI w UML.']\n",
      "requirements: ['Min. 5 lat doświadczenia w projektowaniu i optymalizacji procesów ETL,', 'Bardzo dobra znajomość architektury DWH i narzędzi Microsoft (SSIS, SSAS, SSRS, Power BI),', 'Znajomość programowania SQL, PL/SQL,', 'Znajomość notacji UML oraz oprogramowania Sparx Enterprise Architect,', 'Znajomość architektury Oracle DB 12c,', 'Znajomość OLAP, Data Quality, Data Governance,', 'Znajomość języka polskiego na poziomie swobodnej komunikacji.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/architekt-danych-warszawa,oferta,1003914421?apid=8ab9ae09-8d5b-4dfa-8275-380a54173bfd&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 160 ---\n",
      "id: 213\n",
      "url: https://www.pracuj.pl/praca/analityk-danych-warszawa,oferta,1003914427\n",
      "title: Analityk Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 22 dnido 20 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL', 'UML', 'Enterprise Architect']\n",
      "responsibilities: ['Analiza i projektowanie przekształceń danych,', 'Optymalizacja wydajności baz danych,', 'Tworzenie dokumentacji analitycznej,', 'Modelowanie danych w UML.']\n",
      "requirements: ['Min. 3-letnie doświadczenie na podobnym stanowisku,', 'Znajomość SQL oraz baz danych,', 'Znajomość UML oraz narzędzia Enterprise Architect,', 'Umiejętność rozwiązywania problemów i szybkiego uczenia się,', 'Znajomość języka polskiego na poziomie swobodnej komunikacji.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/analityk-danych-warszawa,oferta,1003914427?apid=c9450d64-68c6-4a0b-80b2-b20eac290ccd&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 161 ---\n",
      "id: 214\n",
      "url: https://www.pracuj.pl/praca/data-engineer-warszawa-marszalkowska-126,oferta,1003890042\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: Immediate employment\n",
      "recruitment_method: None\n",
      "additional_info: ['Marszałkowska 126, Śródmieście, WarszawaWarszawa, Masovian', 'contract of employment', 'full office work']\n",
      "technologies: ['SQL', 'Oracle', 'Python', 'Microsoft SQL Server', 'Microsoft Azure']\n",
      "responsibilities: ['Collaborating closely with Engineering, and Business stakeholders to understand the business requirements on data products & services and design the solutions.', 'Implementing and maintaining data pipelines and infrastructure to support the data products & services.', 'Optimizing ingestion processes using Databricks, SQL and AWS ‘big data’ technologies.', 'Working with data and analytics experts to strive for greater functionality in our data systems.', 'Continuously improving our data architecture to improve platform security, cost efficiency and scalability.']\n",
      "requirements: ['3+ years in a similar position.', 'Proven experience on platforms such as AWS or GCP (cloud), Databricks (SaaS¥PaaS).', 'Proven experience with Databricks Data Engineering (mandatory), Python (mandatory), git version control system, data pipeline orchestration tools and SQL as well as NoSQL databases.', 'Excellent analytical and problem-solving skills.', 'Ability to contribute both independently and as part of a team.', 'Proven experience of handling an end-to-end data engineering project starting from problem solving, data ingestion and massaging, modelling, testing and deployment.', 'Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.', 'Proficiency in English and Polish languages at a minimum of B2 level (both mandatory).', 'Operating according to software engineering best practices (PEP8 style guide, clean code, code review, CI/CD pipelines,etc).', 'Degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field. Master’s is desired, bachelor’s will be considered.', 'Impactful: Has a clear understanding of team’s objectives and aligns efforts to these. Empowered to be quick to act and take steps to address issues.', 'Curious: Engages in learning activities and is interested in understanding different ways of doing things including the key activities performed by others in team.', 'Contemplates and learns from previous experiences including those with both successful and disappointing outcomes.', 'Creative: Looks for new ways to solve problems and execute in ways that add value. Tries new ideas and approaches.', 'Communication: Communicates clearly and actively listens to others. Provides others with information they need in a timely manner.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-warszawa-marszalkowska-126,oferta,1003890042?apid=b94e356e-c1e4-4210-84b9-83d495d0422d&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 162 ---\n",
      "id: 215\n",
      "url: https://www.pracuj.pl/praca/java-integration-engineer-warszawa,oferta,1003929289\n",
      "title: Java Integration Engineer\n",
      "work_location: Company locationWarszawaWarszawa, Masovian\n",
      "validity: valid for a monthto 27 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['Java', 'Spring Boot', 'Spring Integration', 'Apache Camel', 'Apache Kafka', 'JMS', 'MQTT', 'RabbitMQ', 'REST', 'SOAP', 'GraphQL', 'NoSQL', 'MySQL', 'PostgreSQL', 'MongoDB', 'Docker', 'Kubernetes', 'AWS', 'Microsoft Azure', 'agile', 'scrum', 'waterfall']\n",
      "responsibilities: ['Design and develop integration solutions using Java and frameworks like Spring Boot, Apache Camel, or other', 'Build and maintain APIs (REST/SOAP) to enable secure and efficient data exchange between systems', 'Implement data transformation, mapping, and routing workflows using tools like JSON, XML, or XSLT', 'Collaborate with cross-functional teams to understand integration requirements and align solutions with business goals', 'Work with middleware platforms (Apache Kafka, HiveMQ) to build reliable message-driven architectures', 'Ensure integration security by using OAuth, JWT, and SSL protocols', 'Troubleshoot and resolve integration issues while optimizing performance', 'Monitor integration pipelines and maintain system logs to ensure data integrity and system reliability', 'Document the solution']\n",
      "requirements: ['At least 3 years of professional experience in software development or system integration', 'Bachelor’s degree in Computer Science, Engineering, or a related field', 'Strong proficiency in Java and object-oriented programming', 'Experience with integration frameworks like Spring Boot, Spring Integration, or Apache Camel', 'Familiarity with message queuing systems such as Apache Kafka, JMS, MQTT or RabbitMQ', 'Hands-on experience with API development and consumption (REST, SOAP, GraphQL)', 'Solid understanding of relational and NoSQL databases (e.g., MySQL, PostgreSQL, MongoDB)', 'Knowledge of DevOps practices, including CI/CD pipelines and containerization (Docker, Kubernetes)', 'Excellent problem-solving skills and the ability to work collaboratively in a team environment', 'Knowledge of security concepts, including encryption standards, TLS/SSL protocols, and secure coding practices', 'Experience in using security mechanisms like OAuth 2.0, JWT, or SAML', 'Fluent use of English and Polish, both written and spoken']\n",
      "application_link: https://www.pracuj.pl/aplikuj/java-integration-engineer-warszawa,oferta,1003929289?apid=403f1935-0b98-4bad-bd20-547d75129c19&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 163 ---\n",
      "id: 216\n",
      "url: https://www.pracuj.pl/praca/senior-data-science-engineer-gdansk,oferta,1003877153\n",
      "title: Senior Data Science Engineer\n",
      "work_location: Company locationGdańskGdańsk, Pomeranian\n",
      "validity: valid for 4 daysto 02 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: None\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['Pandas', 'NumPy', 'SciPy', 'Python', \"at the client's site\"]\n",
      "responsibilities: ['Collaborate with data scientists to interpret notebooks and complete analysis tasks.', 'Transform existing simple scripts into productized workflows that serve as a foundation for end-to-end strategic solutions.', 'Support the development of graph calibration toolkits to assist data scientists in their work.', 'Develop glue code to integrate existing components of data scientist workflows used in previous prototypes.', 'Design and implement new features for the programmatic model builder, enabling seamless integration of data sources, parameterization, and templating.', 'Maintaining and extending the calibration repository.']\n",
      "requirements: ['Strong knowledge of Pandas, NumPy, and SciPy.', 'Proficiency in Python for numerical and mathematical computing.', 'Familiarity with Bash scripting (preferred but not required).', 'General knowledge of Bayesian networks, probability, and statistics.', 'Experience collaborating directly with data scientists to transform quick Jupyter notebook prototypes into production-grade software for team use.', 'Strong communication skills to effectively convey ideas and collaborate with team members.', '5+ years of experience in software engineering or data science, preferably with a mixed SE/DS profile or a DS background with strong technical expertise.', 'Understanding and application of best practices in machine learning, data science, and general software engineering.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-science-engineer-gdansk,oferta,1003877153?apid=2d679b38-3aa4-4f81-909d-a47de7638bd3&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 164 ---\n",
      "id: 217\n",
      "url: https://www.pracuj.pl/praca/programista-big-data-warszawa,oferta,1003929130\n",
      "title: Programista Big Data\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze miesiącdo 27 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Hadoop Clouder', 'Hortonworks', 'Python', 'Scala', 'Java', 'Spark', 'Kafka', 'Apache Nifi', 'Ansible', 'koncentrujesz się na jednym projekcie', 'koncentrujesz się na rozwoju produktu', 'agile']\n",
      "responsibilities: ['Wdrażanie i projektowanie nowoczesnych systemów przetwarzania danych i systemów big data', 'Praca w zespole scrumowym']\n",
      "requirements: ['Min. 5 lata doświadczenia programistycznego, w tym 3 lata doświadczenia jako programista Big Data', 'Znajomość zagadnień związanych z Big Data, hurtownią danych i zarządzania danymi', 'Znajomość platformy Hadoop Clouder/Hortonworks', 'Min. 3 lata doświadczenia komercyjnego w Python/Scala/Java', 'Praktyczne doświadczenie w implementacji i optymalizacji procesów Spark', 'Doświadczenie w Kafka, Apache Nifi, Ansible']\n",
      "application_link: https://www.pracuj.pl/aplikuj/programista-big-data-warszawa,oferta,1003929130?apid=6a5c50ce-643b-4c82-8022-fcd7736ae2b5&oca=None&acv=0\n",
      "\n",
      "--- Job Record 165 ---\n",
      "id: 218\n",
      "url: https://www.pracuj.pl/praca/ml-developer-opole-wladyslawa-reymonta-39,oferta,1003907820\n",
      "title: ML Developer\n",
      "work_location: None\n",
      "validity: valid for 18 daysto 16 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Władysława Reymonta 39, OpoleOpole, Opole']\n",
      "technologies: ['Python', 'ML', 'LLMs', 'AI', 'agile', 'scrum']\n",
      "responsibilities: ['Implementing and optimizing LLM training pipelines', 'Scaling model training to large datasets and distributed environments', 'Deploying trained LLMs to production environments']\n",
      "requirements: ['At least 3 years of experience as a Machine Learning Engineer or Software Developer', 'Strong Python skills and expertise in LLM applications', 'Knowledge of ML Algorithms, Model Training, Cloud Computing', 'Experience with ML Frameworks', 'Problem solving attitude, proactivity and willingness to implement own ideas', 'Ability to think analytically and effectively search for information', 'Very good knowledge of English (B2/C1)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ml-developer-opole-wladyslawa-reymonta-39,oferta,1003907820?apid=8a824c2b-fbb3-4efa-9d34-f75b57d237ad&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 166 ---\n",
      "id: 219\n",
      "url: https://www.pracuj.pl/praca/data-engineer-with-azure-poznan,oferta,1003889869\n",
      "title: Data Engineer with Azure\n",
      "work_location: Company locationPoznańPoznań, Greater Poland\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Microsoft Azure', 'SQL', 'MySQL', 'PostgreSQL', 'HBase', 'DynamoDB', 'MongoDB', 'Snowflake', 'Databricks', 'BigQuery', 'Redshift', 'Apache Spark', 'Apache Flink', 'Scala', 'Python', 'Git', 'Docker', 'Apache Kafka', 'AWS Kinesis', 'Apache Airflow', 'Dagster', 'Elasticsearch', 'Solr', 'Hadoop', 'Hive', 'Presto', \"at the client's site\"]\n",
      "responsibilities: ['Develop innovative solutions for complex data challenges, using creativity and technology.', 'Design and build data systems that fulfill client requirements and optimize user experience.', 'Collaborate across teams to integrate unique approaches and out-of-the-box thinking into problem-solving.', 'Serve as a trusted consultant and architect for customized data solutions for global clients.', 'Handle large-scale data processing and manage distributed systems to ensure reliability and performance.', 'Uphold high data quality standards and refine approaches to align with best practices.', 'Foster a culture of collaboration, craftsmanship, and continuous improvement.']\n",
      "requirements: ['Solid experience in designing scalable data processing systems (including pipelines, advanced ETL processes, data warehouses, and lakes).', 'Proficiency in Microsoft Azure and its data solutions.', 'Strong SQL skills and experience with relational databases (MySQL, PostgreSQL).', 'Familiarity with NoSQL databases (e.g., HBase, DynamoDB, MongoDB).', 'Familiarity with data warehousing solutions (e.g., Snowflake, Databricks, BigQuery, Redshift).', 'Understanding of distributed data processing systems (e.g., Apache Spark, Apache Flink).', 'Proficiency in Python 3.x or Scala, with knowledge of advanced Python constructs (e.g., lambda functions, generators, list comprehensions).', 'Core understanding of object-oriented programming principles.', 'Experience with threading and multi-process computation.', 'Familiarity with Git for version control and Docker for containerization.', 'Good communication skills in English (minimum B2).', 'A passion for continuous learning and embracing new technologies.', 'Strong problem-solving skills and analytical thinking.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-with-azure-poznan,oferta,1003889869?apid=5ee18930-eb1e-4325-b045-94933109558a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 167 ---\n",
      "id: 220\n",
      "url: https://www.pracuj.pl/praca/data-delivery-lead-krakow-powstancow-wielkopolskich-13,oferta,1003914221\n",
      "title: Data Delivery Lead\n",
      "work_location: None\n",
      "validity: valid for 22 daysto 20 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Powstańców Wielkopolskich 13, Podgórze, KrakówKraków, Lesser Poland', 'contract of employment', 'manager / supervisor', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['SQL', 'AWS', 'Microsoft Power BI', 'Jira', 'Confluence', 'Azure DevOps', 'Microsoft Azure', 'in house', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'agile', 'scrum', 'kanban', 'product owner', 'project manager', 'scrum master']\n",
      "responsibilities: ['Collaborating with business stakeholders and programme leads on the development and execution of a data migration framework as well as strategic Reporting & BI assets.', 'Lead cross-functional team of data engineers and analysts through end-to-end delivery lifecycle within established timelines, following Agile methodologies', 'Provide regular updates on Agile Sprint delivery to all stakeholders, handling all technical planning and escalations.', 'Overseeing SDLC and delivery standards across Data Engineers and Data Analysts, to ensure effective delivery of pipelines, models and dashboards on a modern data stack.', 'Working with Product Managers of our ecosystem to understand requirements for building automated take-on of transactional data into their systems.', 'Working with Reporting & BI Product Manager to understand requirements for analytical reporting and implementing best-in-class data engineering standards in team.', 'Actively contributing to Data Governance forums through overseeing delivery of Data Quality Dashboards for the purpose of driving quality improvements.', 'Single point of contact for all resource-related issues for Data roles in Poland.', 'Support IT Service Delivery Lead providing a point of escalation for Incidents, Problems and Change for all data workloads within the ecosystem.']\n",
      "requirements: ['8+ years experience in Data Engineering / Analysis roles, with specialization in Data Migrations and Business Intelligence.', 'Experience in insurance/financial industry and large multinational environments is a distinct plus, with any specific understanding of Client, Policy, Claims data domains.', 'Some previous hands-on experience with cloud-based Lakehouse architectures through Databricks incl. python (spark) ingestion and transformation, SQL, AWS, PowerBI and similar.', 'Strong Agile project management skill with ability to lead cross-functional teams', 'Hands on experience with Agile practices and Agile Scrum/Kanban management tools (Jira, Confluence, Azure DevOps etc.). Agile certification highly desirable.', 'Excellent communications and interpersonal skills to collaborate effective with stakeholders at all levels', 'Strong people management skills across diverse, matrixed organization', 'Ability to work in fast-paced environment and adapt to changing project requirements', 'Previous hands-on experience in Data Engineering, Data Science (AI/ML), Dashboarding, and either/both DataOps and MLOps.', 'Azure (or AWS) cloud support experience desirable – IaaS and PaaS including containerization and serverless', 'Bachelors or Masters degree in Computer Science, Information Technology or similar.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-delivery-lead-krakow-powstancow-wielkopolskich-13,oferta,1003914221?apid=ce566041-29d3-4caf-8e72-ee44e1e8cc47&oca=None&acv=0\n",
      "\n",
      "--- Job Record 168 ---\n",
      "id: 221\n",
      "url: https://www.pracuj.pl/praca/machine-learning-data-engineer-krakow,oferta,1003905060\n",
      "title: Machine Learning Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 17 daysto 15 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KrakówKraków, Lesser Poland', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Python', 'Pyspark', 'AWS', 'Microsoft Azure', 'Google Cloud Platform', 'Spark', 'HDFS', 'HIVE', 'Databricks', 'SQL', 'NoSQL', 'Parquet', 'ORC', 'Avro', 'Git', 'Kafka', 'Apache Flink', 'agile', 'scrum', 'kanban', 'waterfall', 'industry-specific e-learning platforms']\n",
      "responsibilities: ['Developing and optimizing data engineering processes', 'Building robust, fault-tolerant data solutions for both cloud and on-premise environments', 'Automating data pipelines to ensure seamless data flow from ingestion to serving', 'Creating well-tested, clean code in line with modern software engineering principles', 'Working with cloud technologies (AWS, Azure, GCP) to support large-scale data operations', 'Supporting data transformation and migration efforts from on-premise to cloud ecosystems', 'Designing and implementing scalable data models and schemas', 'Maintaining and enhancing big data technologies such as Hadoop, HDFS, Spark, and Cloudera', 'Collaborating with cross-functional teams to solve complex technical problems', 'Contributing to the development of CI/CD pipelines and version control practices']\n",
      "requirements: ['Strong experience in the Data Engineering Lifecycle, especially in building data pipelines', 'Proficiency in Python, Pyspark, and the Python ecosystem', 'Experience with cloud platforms such as AWS, Azure, or GCP (preferably GCP)', 'Expertise in Hadoop on-premise distributions, particularly Cloudera', 'Experience with big data tools such as Spark, HDFS, HIVE, and Databricks', 'Knowledge of data lake formation, data warehousing, and schema design', 'Strong understanding of SQL and NoSQL databases', 'Ability to work with data formats like Parquet, ORC, and Avro', 'Familiarity with CI/CD pipelines and version control tools like Git', 'Strong communication skills to collaborate with diverse teams']\n",
      "application_link: https://www.pracuj.pl/aplikuj/machine-learning-data-engineer-krakow,oferta,1003905060?apid=23f4e751-4abc-4f8f-9310-93139b12a27d&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 169 ---\n",
      "id: 222\n",
      "url: https://www.pracuj.pl/praca/data-and-reporting-developer-krakow-kapelanka-42a,oferta,1003928967\n",
      "title: Data and Reporting Developer\n",
      "work_location: None\n",
      "validity: valid for a monthto 27 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['SQL', 'Python']\n",
      "responsibilities: ['Develop data technology solutions adopting the latest technologies and working in parallel with Data Architects and Analytical Engineers.', 'May include developing code using various algorithms and languages (e.g. Python, TSQL, C#), deep knowledge in data lifecycle management (e.g. data sourcing, ingestion, data warehouse, data refinery, big data and cloud migration) and the associated groups strategic tools (e.g. Google Big Query, Google Looker Studio, Qlik Sense, PowerApps, Alteryx, Collibra, JIRA).', 'Support providing technical leadership, evaluation and recommendations for data consumption engines and new technologies that can be used to advance new features and capabilities around analytics and use derived data sets.', 'Support leveraging sound judgment and problem solving to tackle most critical problems and connect the dots to broader data solution.', 'Support developing software / systems which acquire, aggregate and refine data.', 'Support creating data warehouses and big data systems which maintain enterprise data and allow queries to be run against it.', 'Provide clear direction, prioritises tasks and monitors workflow within own area for a midsized team, or leads project delivery for complex assignments with supervisory responsibilities.', 'Deliver and manage using ETL tools to perform data integration across different databases as well as deliver and manage using data tools to perform data integration across different databases / systems / platforms.']\n",
      "requirements: ['Minimum 2 years’ experience of working in data-related role (SQL developer, data engineer/analyst, etc.).', 'Intermediate SQL programming skills and experience with SQL development environment (Microsoft SQL Server or other).', 'Big Query experience. Understanding of ETL and data warehousing principles.', 'Proficient English language skills.', 'Python programming skills - basic level (nice to have).', 'Alteryx.', 'Data visualisation and dashboard design experience, preferable with Google Looker Studio, Qlik Sense.', 'JIRA and Confluence experience, MS Excel skills (pivot tables, data manipulation and transformation, using formulas).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-and-reporting-developer-krakow-kapelanka-42a,oferta,1003928967?apid=c03dbc3e-1778-4c37-8c62-3c45dc739347&oca=None&acv=0\n",
      "\n",
      "--- Job Record 170 ---\n",
      "id: 223\n",
      "url: https://www.pracuj.pl/praca/enterprise-search-engineer-slovakia,oferta,11134664\n",
      "title: Enterprise Search Engineer\n",
      "work_location: Company locationBratislava, Slovakiaabroad\n",
      "validity: valid for 7 daysto 05 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: full office work, home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'More than one vacancy', 'Робота для іноземцівбез польської']\n",
      "technologies: ['SharePoint Online', 'Graph API', '.NET Framework', 'ServiceNow', 'JSON', 'C#', '.NET', 'CI/CD', 'Azure DevOps']\n",
      "responsibilities: ['NET Framework & C#: Strong experience in building enterprise-level solutions using .NET Framework and C#, including web services and integration with SharePoint.', 'Graph API & REST API: Proven ability to work with Graph API and REST APIs to integrate external systems and data sources into SharePoint search.', 'JSON: Proficiency in working with JSON for data serialization and deserialization between systems.', 'SharePoint Online: Strong experience with SharePoint Online architecture, including search configuration, indexing, and optimization techniques.', 'Azure DevOps CI/CD: Hands-on experience with Azure DevOps for continuous integration and continuous delivery of SharePoint-based solutions and APIs.', 'PowerShell: Expertise in using PowerShell for SharePoint Online administration and automation tasks.', 'Adaptive Cards: Knowledge of developing Adaptive Cards for enhancing the presentation of search results and integrating them into the user interface.']\n",
      "requirements: ['Oportunity to work with one of the worlds top 10 IT services company.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/enterprise-search-engineer-slovakia,oferta,11134664?apid=7a33c6b0-17fc-4367-8c79-38c17c846889&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 171 ---\n",
      "id: 224\n",
      "url: https://www.pracuj.pl/praca/data-engineer-azure-0-5-fte-warszawa-pulawska-2,oferta,1003922692\n",
      "title: Data Engineer (Azure) - 0,5 FTE\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 25 dnido 23 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: None\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['część etatu']\n",
      "technologies: ['Python', 'Microsoft Azure', 'Databricks', 'SQL', 'Apache Spark']\n",
      "responsibilities: ['Poszukiwany Data Engineer specjalizujący się w Databricks.', 'Potrzebna będzie osoba, która zbuduje całą landing zone od podstaw – osieciowanie, infrastruktura jako kod, dostepy, polityki, compliance (SOC2).', \"Kluczowymi skillami tu są: platform engineering, migracja workspace'ów do UC oraz optymalizacja kosztów i monitorowanie.\"]\n",
      "requirements: ['Databricks, Apache Spark, SQL, Python, Azure']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-azure-0-5-fte-warszawa-pulawska-2,oferta,1003922692?apid=16006ad5-fa6b-4f87-b422-5b6f34f12e3e&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 172 ---\n",
      "id: 225\n",
      "url: https://www.pracuj.pl/praca/administrator-hurtowni-danych-i-bi-warszawa-twarda-18,oferta,1003928948\n",
      "title: Administrator Hurtowni Danych i BI\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 27 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Twarda 18, Śródmieście, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Unix', 'Linux', 'Shell', 'SQL', 'SAS', 'Orcle', 'Oracle Exadata', 'wewnątrz organizacji', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt']\n",
      "responsibilities: ['Utrzymanie i rozwój procesów zasilania i raportowania Hurtowni Danych', 'Nadzór nad kontrolą jakości danych w procesach zasilania i przetwarzania', 'Realizacja wdrożeń w obszarze Hurtowni Danych', 'Budowa narzędzi wspierających pracę developerów (SAS/SAS Viya/Oracle)', 'Optymalizacja procesów przetwarzania danych i analiza wydajności', 'Wsparcie użytkowników w identyfikacji i rozwiązywaniu problemów', 'Udział w projektach strategicznych pod kątem technologicznym']\n",
      "requirements: ['Wykształcenie wyższe (IT lub pokrewne) lub ostatnie lata studiów', 'Dobra znajomość systemów Unix/Linux', 'Znajomość języka programowania Shell (BASH/SH)', 'Bardzo dobra znajomość SQL', 'Doświadczenie z technologiami SAS Institute (SAS DI Studio, SAS Management Console, SAS Viya)', 'Umiejętność analitycznego myślenia i dbałość o szczegóły', 'Komunikatywność i umiejętność pracy z użytkownikami', 'Min. 3-letnie doświadczenie w wymienionych technologiach']\n",
      "application_link: https://www.pracuj.pl/aplikuj/administrator-hurtowni-danych-i-bi-warszawa-twarda-18,oferta,1003928948?apid=e9045ad4-04c2-4a57-b27b-8d0f4ad0cab8&oca=None&acv=0\n",
      "\n",
      "--- Job Record 173 ---\n",
      "id: 226\n",
      "url: https://www.pracuj.pl/praca/cloud-compliance-analyst-krakow-kapelanka-42a,oferta,1003928942\n",
      "title: Cloud Compliance Analyst\n",
      "work_location: None\n",
      "validity: valid for a monthto 27 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['Jira', 'Confluence']\n",
      "responsibilities: ['Monitor and ensure compliance with Cloud policy set against the IWPB Cloud Strategy and as provided by our Cloud Service Providers.', 'Drive teams to ensure adherence to security controls.', 'Manage the approach, building stakeholder buy-in around plans, commitments and changes.', 'Establish strong relationships with the local, central and 3rd party teams.', 'Hold regular meetings with the Team leads to ensure that they are on track to meet original transition / conversion plans/forecasts.', 'Provide management to maintain a focus on how compliance initiatives align to wider programme objectives.', 'Help with the development and running of the Cloud Enablement programme office.', 'Be responsible for managing portfolio support activities as required by the IWPB Chief Technology Office function.']\n",
      "requirements: ['Strong/Advanced Microsoft Office skills, particularly Excel is a must.', 'Strong portfolio management skills.', 'Sound analytical skills with an ability to understand complex technical and business issues.', 'Understanding of the business environment and imperatives, particularly the provision of excellent customer service.', 'Good understanding of Cloud FinOps and Cloudability.', 'Excellent communication skills to build and maintain effective working relationships with peer groups in all Business areas.', 'Good understanding of systems JIRA & Confluence.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/cloud-compliance-analyst-krakow-kapelanka-42a,oferta,1003928942?apid=b9557d41-9185-4674-9fdd-5fcf98a9f6e7&oca=None&acv=0\n",
      "\n",
      "--- Job Record 174 ---\n",
      "id: 227\n",
      "url: https://www.pracuj.pl/praca/marketing-data-analyst-e-commerce-warszawa-czerska-8-10,oferta,1003896712\n",
      "title: Marketing Data Analyst (e-commerce)\n",
      "work_location: None\n",
      "validity: valid for 14 daysto 12 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Czerska 8/10, Mokotów, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['SQL', 'Any BI Tool', 'Excel', 'Python', 'Looker Studio', 'GCP', 'BigQuery', 'GTM', 'GA4']\n",
      "responsibilities: ['Querying and analyzing data sets using SQL (Big Query), Python (optionally), Excel/Google Sheets and visualization tools (Looker Studio)', 'Analyzing data from online marketing channels (e.g., social media, email, paid search) to optimize campaign performance and spending, and provide recommendations.', 'Developing and monitoring dashboards to optimize digital marketing campaigns, traffic acquisition spend and ROI.', 'Developing and maintaining attribution models (and others) to assess channel spend effectiveness and returns.', 'Analyzing, segmenting, comparing, and presenting data; drawing conclusions and making recommendations for improvements and tests.', 'Supporting the business team in making efficient marketing budget decisions and executing the strategy.', 'Simplifying complex business problems, tracking them, and enabling fast decision-making based on analytical solutions.', 'Leveraging new technologies and methodologies to enhance the data platform and improve company performance in collaboration with Data Team', 'Ensuring data quality and accuracy in collaboration with other Analysts']\n",
      "requirements: ['the specifics of product-based e-commerce and how to navigate within such an industry', 'how to apply econometric principles in practice', 'Google Stack Tools such as: Google Tag Manager, Google Analytics/GA4, Google Firebase, Google Sheets, Google Apps Script, Google Cloud Platform (BigQuery, Cloud Functions, Pub/Sub)', 'Python or R']\n",
      "application_link: https://www.pracuj.pl/aplikuj/marketing-data-analyst-e-commerce-warszawa-czerska-8-10,oferta,1003896712?apid=c60fa2ab-4b89-4009-86ee-82b470d2145f&oca=None&acv=0\n",
      "\n",
      "--- Job Record 175 ---\n",
      "id: 228\n",
      "url: https://www.pracuj.pl/praca/ekspert-ds-sztucznej-inteligencji-w-obszarze-crm-warszawa-stanislawa-zaryna-2a,oferta,1003919556\n",
      "title: Ekspert ds. Sztucznej Inteligencji w obszarze CRM\n",
      "work_location: None\n",
      "validity: ważna jeszcze 24 dnido 22 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior), ekspert\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Stanisława Żaryna 2A, Mokotów, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL', 'Python']\n",
      "responsibilities: ['Aktywne tworzenie nowych rozwiązań, których celem jest ciągła poprawa procesu budowy spersonalizowanych relacji z klientem z użyciem uczenia maszynowego i sztucznej inteligencji', 'Prowadzenie projektów analitycznych obejmujące zbieranie wymagań, projektowanie, wdrażanie i utrzymywanie rozwiązań uczenia maszynowego i sztucznej inteligencji, w szczególności: segmentacje klientów, modele predykcyjne i systemy rekomendacyjne', 'Monitorowanie rezultatów inicjatyw analitycznych oraz podejmowanie zmian na podstawie uzyskanych danych, zapewniając ich ciągłą efektywność', 'Tworzenie i rozwój data martów analitycznych przy współpracy z jednostkami IT']\n",
      "requirements: ['Co najmniej 3 letnie doświadczenie w analityce biznesowej, analityce predykcyjnej lub przetwarzaniu danych (mile widziane w instytucji finansowej lub telekomunikacyjnej)', 'Doświadczenie w budowie i wdrażaniu rozwiązań z użyciem uczenia maszynowego i sztucznej inteligencji, np. modele klasyfikacyjne, segmentacyjne i rekomendacyjne', 'Bardzo dobra znajomość SQL i Python', 'Wysokie zdolności analityczne, w tym umiejętność interpretacji danych i wyciągania wniosków', 'Doświadczenie projektowe w współpracy z jednostkami IT, biznesowymi oraz dostawcami zewnętrznymi', 'Znajomość języka angielskiego umożliwiająca swobodną komunikację (min. B1/B2)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ekspert-ds-sztucznej-inteligencji-w-obszarze-crm-warszawa-stanislawa-zaryna-2a,oferta,1003919556?apid=616e5ced-7928-4683-8fc9-83f43e65bb64&oca=None&acv=0\n",
      "\n",
      "--- Job Record 176 ---\n",
      "id: 229\n",
      "url: https://www.pracuj.pl/praca/machine-learning-engineer-warszawa,oferta,1003881051\n",
      "title: Machine Learning Engineer\n",
      "work_location: None\n",
      "validity: valid for 7 daysto 05 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'contract of employment', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Python', 'Spark', 'ML', 'Docker', 'Kubernetes', 'Airflow', 'Google Cloud Platform', 'in house']\n",
      "responsibilities: ['Develop and maintain a codebase that generates machine learning models for various business units.', 'Continuously refactor and simplify complexity, such as transitioning from batch to stream processing.', 'Enhance data workflows by integrating new data sources and improving deployment processes.', 'Diagnose and resolve data issues, ensuring robust and reliable performance.', 'Utilize Azure DevOps for CI/CD pipelines, task management, version control, and other DevOps practices.', 'Prepare and maintain documentation for technical specifications, procedures, and outcomes.']\n",
      "requirements: ['At least 4+ years of experience in a similar position.', 'Strong knowledge of Spark and Python.', 'Proficiency in refactoring, maintaining, and debugging existing machine learning solutions.', 'Problem-solving skills with the ability to troubleshoot, optimize, and debug machine learning models.', 'Solid understanding of deployment patterns and usage of machine learning models.', 'Hands-on experience in building complex data processing pipelines.', 'Experience with deployment and provisioning automation tools (Docker, Kubernetes, CI/CD).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/machine-learning-engineer-warszawa,oferta,1003881051?apid=d102cf4f-7549-4ae0-93a9-43426cfa83fa&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 177 ---\n",
      "id: 230\n",
      "url: https://www.pracuj.pl/praca/cloud-data-engineer-snowflake-warszawa,oferta,1003919478\n",
      "title: Cloud Data Engineer (Snowflake)\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['Snowflake Data Cloud', 'AWS', 'SQL']\n",
      "responsibilities: ['Design, develop, and maintain data pipelines to ensure seamless and efficient data migration from on-premise SQL Server to Snowflake on AWS, enabling secure, scalable, and reliable data storage that supports advanced analytics and reporting needs.', 'Implement and manage data integration processes to unify data from multiple sources within Snowflake, ensuring high data quality, consistency, and accessibility, which drives accurate, timely insights for business users across the organisation.', 'Optimise Snowflake performance by enhancing query speeds, data load times, and storage efficiency, ensuring that the data platform remains fast, responsive, and capable of handling high data volumes, which supports the organisation’s operational and analytical requirements.', 'Enforce data security and compliance measures by applying best practices in data encryption, access controls, and adherence to relevant regulatory standards, ensuring sensitive information remains protected and compliant throughout the migration and within Snowflake.', 'Collaborate with data scientists, analysts, and key stakeholders to gather and understand data requirements, aligning Snowflake infrastructure and solutions with business needs and fostering cross-team communication to deliver reliable and actionable data.', 'Evaluate and recommend complementary tools and technologies that enhance Snowflake’s capabilities, keeping the data platform adaptable and current with evolving cloud technologies to ensure the organisation maximises the value of its cloud infrastructure.', 'Lead migration efforts from on-premise to cloud-based infrastructure, coordinating with the Global Data Engineering Lead to establish timelines and address migration challenges, ensuring a smooth transition with minimal business disruption.', 'Document and maintain data infrastructure processes and standards within the Snowflake platform to ensure knowledge sharing, consistency, and ease of maintenance, supporting long-term operational stability and team efficiency.']\n",
      "requirements: ['Extensive Experience with Cloud Data Platforms: The ideal candidate will have significant experience with Snowflake and SQL-based data warehouses, particularly in designing and managing cloud-based data architectures. A deep understanding of ETL processes, data pipeline design, and AWS cloud infrastructure is essential. This expertise should translate into the ability to develop secure, scalable, and efficient data solutions that drive organisational goals.', 'Proven Expertise in Data Migration: We are seeking someone with a strong track record in supporting or leading data migrations, especially transitioning from on-premise SQL Server environments to cloud-based platforms like Snowflake. This experience should demonstrate an ability to manage complex migrations while maintaining data integrity and performance.', 'Experience with Medallion Architecture: A strong understanding of medallion architecture principles is crucial. The candidate should demonstrate expertise in designing and implementing layered data structures (bronze, silver, gold layers) to ensure data reliability, scalability, and accessibility for analytics and business use cases.', 'Data Integration and Optimisation Skills: The candidate should possess robust experience in integrating data from multiple sources into a centralised platform, with a solid grasp of data modelling, transformation, and management principles. The ability to optimise data systems for speed, scalability, and reliability—combined with knowledge of query optimisation and performance enhancement techniques within Snowflake—is a key requirement.', 'Strong Understanding of Data Security: Familiarity with best practices for securing cloud-based systems, including encryption, access control, and compliance with regulatory standards, is critical. The candidate will ensure data security is upheld across all workflows and architectures.', 'Excellent Communication and Stakeholder Engagement: Exceptional communication skills are essential, as the candidate will work with both technical and non-technical stakeholders to translate complex data processes into actionable insights. Collaboration with data architects, governance leads, and analytics teams is vital for success.', 'Proactive Problem-Solving and Innovation: The successful candidate will demonstrate a methodical approach to identifying issues, troubleshooting problems, and designing innovative, long-term solutions. They should be able to foresee potential challenges and implement proactive strategies to address them effectively.', 'Desirable Qualifications: A bachelor’s degree in Computer Science, Information Systems, or a related field is preferred. Certifications in Snowflake, AWS, or similar cloud platforms, as well as experience working in multi-market or global environments, will be highly valued.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/cloud-data-engineer-snowflake-warszawa,oferta,1003919478?apid=790576b3-4a7e-4838-af7c-59f27feb6a6e&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 178 ---\n",
      "id: 231\n",
      "url: https://www.pracuj.pl/praca/data-scientist-nlp-opole-wladyslawa-reymonta-39,oferta,1003907555\n",
      "title: Data Scientist NLP\n",
      "work_location: None\n",
      "validity: valid for 18 daysto 16 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Władysława Reymonta 39, OpoleOpole, Opole']\n",
      "technologies: ['Python', 'NLP', 'ML', 'AI', 'LLMs', 'agile', 'scrum']\n",
      "responsibilities: ['Designing and implementing novel NLP architectures and training methods', 'Conducting research on state-of-the-art LLM advancements', 'Evaluating and analyzing model performance']\n",
      "requirements: ['At least 3 years of experience as a Data Scientist or Software Developer', 'Experience with NLP Techniques and Language Modeling', 'Strong Python skills and expertise in LLM applications', 'Knowledge of ML Algorithms and Model Training', 'Expertise in evaluation and research', 'Problem solving attitude, proactivity and willingness to implement own ideas', 'Ability to think analytically and effectively search for information', 'Very good knowledge of English (B2/C1)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-nlp-opole-wladyslawa-reymonta-39,oferta,1003907555?apid=73310884-e24a-4630-a078-0e9a1db61a77&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 179 ---\n",
      "id: 232\n",
      "url: https://www.pracuj.pl/praca/hadoop-big-data-developer-warszawa,oferta,1003907553\n",
      "title: Hadoop / Big Data Developer\n",
      "work_location: None\n",
      "validity: valid for 18 daysto 16 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'More than one vacancy']\n",
      "technologies: ['Scala', 'Spark', 'Linux', 'Hadoop', 'SQL', 'Git', 'Ansible', 'Bamboo', 'Jenkins', 'Scala', 'Flink', 'Kafka', \"at the client's site\", 'you focus on a single project at a time', 'you can change the project', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'agile']\n",
      "responsibilities: ['Developing and maintaining Hadoop / Big Data solutions within assigned projects', 'Working in an Agile SAFe environment, collaborating with cross-functional teams', 'Designing, implementing, and optimizing data processing pipelines', 'Ensuring high-quality data management and governance', 'Collaborating with stakeholders to understand business requirements and translate them into technical solutions', 'Continuously improving system performance and scalability']\n",
      "requirements: ['3+ years of Java 8+ and/or Scala experience', 'Experience in working in Spark', 'Knowledge of Linux Shell Scripting', 'Knowledge of SQL', 'Knowledge of Hadoop stack (YARN, Sqoop, Hive, Impala, MapReduce Oozie, etc)', 'Familiar with version control and CI/CD tools (Git, Ansible, Bamboo, Jenkins...)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/hadoop-big-data-developer-warszawa,oferta,1003907553?apid=8a1bdb95-3a47-48c7-b9c8-7ad898cd4293&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 180 ---\n",
      "id: 233\n",
      "url: https://www.pracuj.pl/praca/senior-azure-data-engineer-warszawa-pulawska-180,oferta,1003919387\n",
      "title: Senior Azure Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Puławska 180, Mokotów, WarszawaWarszawa, Masovian', 'Запрошуємо працівників з України']\n",
      "technologies: ['SQL', 'Python', 'JIRA', 'Azure DevOps']\n",
      "responsibilities: ['Designing and implementing data processing systems on Azure platform. This involves writing efficient and scalable code to process, transform, and clean large volumes of structured and unstructured data.', 'Building data pipelines to ingest data from various sources such as storages, databases, APIs, or streaming platforms.', 'Support team members with troubleshooting and resolving complex technical issues and challenges.', 'Provide technical guidance in data engineering and with team in selecting appropriate tools, technologies, and methodologies. Be updated with the latest advancements in data engineering and ensuring the team follows best practices and industry standards.', 'Collaborate with stakeholders to understand project requirements, define scope, and create project plans.', 'Support project managers to ensure that projects are executed effectively, meeting timelines, and quality standards. Monitor progress, identify risks, and implement mitigation strategies.', 'Act as a trusted advisor for the customer.', 'Oversee the design and architecture of data solutions, collaborating with data architects and other stakeholders. Ensure data solutions are scalable, efficient, and aligned with business requirements. Provide guidance in areas such as data modeling, database design, and data integration.', 'Align coding standards, conduct code reviews to ensure proper code quality level.', 'Identify and introduce quality assurance processes for data pipelines and workflows.', 'Optimize data processing and storage for performance, efficiency and cost savings.', 'Evaluate and implement new technologies to improve data engineering processes on various aspects (CICD, Quality Assurance, Coding standards).', 'Maintain technical documentation of the project, control validity and perform regular reviews of it.', 'Ensure compliance with security standards and regulations.']\n",
      "requirements: ['At least 6 years of professional experience in the Data & Analytics area', '1+ years of experience (or acting as) in the Senior Consultant or above role with a strong focus on data solutions build in Azure and Databricks/Synapse/(MS Fabric is nice to have).', 'Proven experience in Azure cloud-based infrastructure, Databricks and one of SQL implementation (e.g., Oracle, T-SQL, MySQL, etc.)', 'Proficiency in programming languages such as SQL, Python, PySpark is essential (R or Scala nice to have).', 'Very good level of communication including ability to convey information clearly and specifically to co-workers and business stakeholders.', 'Working experience in the agile methodologies – supporting tools (JIRA, Azure DevOps).', 'Experience in leading and managing a team of data engineers, providing guidance, mentorship, and technical support.', 'Knowledge of data management principles and best practices, including data governance, data quality, and data integration.', 'Good project management skills, with the ability to prioritize tasks, manage timelines, and deliver high-quality results within designated deadlines.', 'Excellent problem-solving and analytical skills, with the ability to identify and resolve complex data engineering issues.', 'Knowledge of data security and privacy regulations, and the ability to ensure compliance within data engineering projects.', 'Knowledge of data orchestration tools.', 'Experience in designing and creating integration and unit tests will be nice to have.', 'Continuous learning mindset, staying updated with the latest advancements and trends in data engineering and related technologies.', 'Experience or familiarity with other cloud technologies, data warehouses, data governance, and business analysis is a plus.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-azure-data-engineer-warszawa-pulawska-180,oferta,1003919387?apid=829f633c-c7d6-4d2c-af3b-5c75c60fd0d9&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 181 ---\n",
      "id: 234\n",
      "url: https://www.pracuj.pl/praca/ai-ml-engineer-business-analysis-krakow,oferta,1003913969\n",
      "title: AI/ML Engineer (Business Analysis)\n",
      "work_location: None\n",
      "validity: valid for 22 daysto 20 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KrakówKraków, Lesser Poland', 'Запрошуємо працівників з України']\n",
      "technologies: ['Machine Learning', 'NLP', 'LLM', 'Python', 'GenAI', 'TensorFlow', 'Microsoft Azure', 'in house', 'you focus on a single project at a time', 'you develop several projects simultaneously', 'you can change the project', 'you have influence on the product', 'you focus on product development', 'agile', 'scrum']\n",
      "responsibilities: ['Be responsible for the end-to-end process from project inception to the design, development, and deployment of GenAI models, leveraging agile and DevOps methodologies to ensure rapid and efficient delivery of high-quality solutions.', 'Collaborate closely with business stakeholders and cross-functional teams to define and prioritize AI-driven product features and capabilities based on their requirements.', 'Continuously monitor and evaluate AI model performance, identifying areas for improvement and implementing necessary enhancements to ensure optimal results.', 'Stay up to date with advancements in GenAI, analytics, and Prompt Engineering, maintaining a competitive edge in the industry.', 'Participate in reviews and audits of AI-driven processes, providing insights and recommendations to ensure alignment with strategic goals and regulatory requirements.', 'Ensure adherence to data privacy and security standards, safeguarding sensitive customer and business information while maintaining compliance with relevant regulations.', 'Establish and maintain strong relationships with internal and external partners, fostering a collaborative environment that encourages knowledge sharing and drives product success.', 'Develop and execute comprehensive system test plans for the GenAI platform, ensuring the highest quality and performance of AI capabilities across various business areas.']\n",
      "requirements: ['Highly numerate with a strong background in mathematical disciplines (e.g., Computer Science (AI/ML), Statistics, Physics, Engineering).', 'Extensive experience in AI model development and lifecycle management.', 'Proven experience working with GenAI, Large Language Models (LLM), and NLP techniques, with familiarity in TensorFlow, PyTorch, Hugging Face, and Scikit-learn.', 'Deep knowledge of software development and experience in coding languages such as Python, C++, JavaScript, HTML.', 'Familiarity with cloud-based AI platforms, especially Microsoft Azure.', 'Banking and/or financial services experience with exposure to Global Banking (GB), Investment Banking (IB), or Commercial Banking (CMB) preferred.', 'Good understanding of financial data, banking products, and processes.', 'Quick thinker with innovative problem-solving skills.', 'Excellent verbal and written communication skills, with experience preparing and presenting key messages to senior management and stakeholders.', 'Track record of implementing GenAI solutions in strictly regulated organizations.', 'Result-oriented team player, fostering strong and collaborative partnerships with key stakeholders.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-ml-engineer-business-analysis-krakow,oferta,1003913969?apid=717c5cb5-d6b0-4ef5-94a9-9173cb0fe68a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 182 ---\n",
      "id: 235\n",
      "url: https://www.pracuj.pl/praca/data-and-reporting-senior-developer-krakow-kapelanka-42a,oferta,1003876642\n",
      "title: Data and Reporting Senior Developer\n",
      "work_location: None\n",
      "validity: valid for 3 daysto 01 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['SQL']\n",
      "responsibilities: ['Conduct code review to assure coding standards are met.', 'Develop coding standards, providing training and coders accreditation and oversee code release process.', 'Identify possible improvements and looking for the new opportunities for platform transformation.', 'Analyse new tools/technologies available and implementation into DBS-Refinery.', 'Work with stakeholders and business partners to understand requirements and process logic; play an active role in understanding core problem statements.', 'Provide guidance on currently available solutions, designing and implementing new ones.', 'Work closely with DevOps team to ensure priorities are correctly assigned and delivered. Resolving potential problems in the process.', 'Monitor data transformation processes, reporting problems to owners and assuring they are properly resolved as well as review existing projects code for cost-based improvement, identifying tasks, providing guidance, and overlooking execution.']\n",
      "requirements: ['5+ years of experience in: data analysis, data modelling with ability to analyse and document data structures and transformations, SQL (BigQuery SQL) - understatement of technical aspects, optimisation rules and coding standards.', 'Code review experience.', 'Knowledge of Python and coding standards.', 'Understanding of IT infrastructure, knowledge required to conduct technical discussions.', 'Wide interest in new technologies, understanding of existing ones.', 'Basic project management experience: ability to plan, track and report the status of the deliverables.', 'English language skills for both verbal and written communication.', 'Processes design and documentation creation skills.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-and-reporting-senior-developer-krakow-kapelanka-42a,oferta,1003876642?apid=b951a58b-fc2d-4461-9327-83c88f620c89&oca=None&acv=0\n",
      "\n",
      "--- Job Record 183 ---\n",
      "id: 236\n",
      "url: https://www.pracuj.pl/praca/data-analyst-warszawa-postepu-15,oferta,1003928689\n",
      "title: Data Analyst\n",
      "work_location: None\n",
      "validity: valid for a monthto 27 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Postępu 15, Mokotów, WarszawaWarszawa, Masovian']\n",
      "technologies: ['SQL', 'Microsoft SQL Server', 'Microsoft Power BI', 'Microsoft Azure', 'Azure SQL Database', 'Azure Data Lake Storage', 'Azure Data Factory', 'Python', 'R', 'PowerShell', 'in house', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'agile']\n",
      "responsibilities: ['Gather and analyze data from various sources, including databases, APIs, and other data repositories.', 'Develop and maintain complex SQL queries for data extraction, manipulation, and analysis within the MS SQL Server environment.', 'Design, develop, and maintain interactive dashboards and reports using Power BI, visualizing key performance indicators and trends.', 'Leverage Azure Cloud services for data storage, processing, and analysis.', 'Automate data-related tasks and processes using scripting languages (e.g., Python, R, PowerShell).', 'Identify and interpret patterns, trends, and anomalies in data to provide actionable insights.', 'Collaborate with stakeholders across different departments to understand their data needs and translate them into effective analytical solutions.', 'Present data findings and recommendations clearly and concisely to technical and non-technical audiences.', 'Ensure data quality and integrity by implementing data validation and cleansing procedures.', 'Stay up-to-date with the latest data analysis techniques, tools, and technologies.', 'Contribute to the development and improvement of data infrastructure and processes.']\n",
      "requirements: [\"Bachelor's degree in a quantitative field such as Computer Science, Statistics, Mathematics, Economics, or a related discipline.\", 'Proven experience as a Data Analyst, preferably with a focus on business intelligence and data visualization.', 'Strong proficiency in SQL and MS SQL Server, including writing complex queries, stored procedures, and functions.', 'Hands-on experience with Power BI, including data modeling, report design, and dashboard development.', 'Familiarity with Azure Cloud services, particularly those related to data storage and processing (e.g., Azure SQL Database, Azure Data Lake Storage, Azure Data Factory).', 'Working knowledge of scripting languages (e.g., Python, R, PowerShell) for data manipulation and automation.', 'Excellent analytical and problem-solving skills, with the ability to identify and interpret complex data patterns.', 'Strong communication and presentation skills, with the ability to effectively convey data insights to various audiences.', 'Ability to work independently and as part of a team.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-analyst-warszawa-postepu-15,oferta,1003928689?apid=a44cbf4b-7dc3-4e1f-9bfa-e7733407a592&oca=None&acv=0\n",
      "\n",
      "--- Job Record 184 ---\n",
      "id: 237\n",
      "url: https://www.pracuj.pl/praca/senior-bi-consultant-warszawa-postepu-15,oferta,1003928685\n",
      "title: Senior BI Consultant\n",
      "work_location: None\n",
      "validity: valid for a monthto 27 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Postępu 15, Mokotów, WarszawaWarszawa, Masovian']\n",
      "technologies: ['Microsoft Power BI', 'Azure Synapse Analytics', 'Azure Data Factory', 'Azure Analysis Services', 'SQL', 'in house', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'agile']\n",
      "responsibilities: ['Lead the design and development of BI solutions using the modern Microsoft stack, including Power BI, Azure Synapse Analytics, Azure Data Factory, and Azure Analysis Services.', 'Gather and analyze client business requirements, translating them into technical specifications.', 'Develop and maintain ETL processes for data integration and transformation.', 'Design and implement data models optimized for performance and scalability.', 'Create interactive dashboards and reports that provide actionable insights to clients.', 'Conduct data quality assurance and testing to ensure accuracy and reliability.', 'Provide training and support to clients on using BI tools and solutions.', 'Stay up-to-date with the latest advancements in the Microsoft BI stack and industry best practices.', 'Mentor and guide junior BI consultants.', 'Contribute to pre-sales activities, including proposal writing and presentations.', 'Effectively manage project timelines and budgets.']\n",
      "requirements: [\"Bachelor's degree in Computer Science, Information Systems, or a related field.\", '5+ years of experience in BI consulting or related roles.', 'Deep understanding of data warehousing concepts and dimensional modeling.', 'Extensive experience with the modern Microsoft BI stack: Power BI (DAX, M) / Azure Synapse Analytics (SQL, Data Engineering) / Azure Data Factory / Azure Analysis Services (MDX)', 'Proficiency in SQL and data manipulation languages.', 'Experience with data visualization best practices.', 'Strong analytical and problem-solving skills.', 'Excellent communication, presentation, and client-facing skills.', 'Ability to work independently and as part of a team.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-bi-consultant-warszawa-postepu-15,oferta,1003928685?apid=e15b6db8-09fb-44c8-b13a-519f6c2b8918&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 185 ---\n",
      "id: 238\n",
      "url: https://www.pracuj.pl/praca/principal-database-engineer-katowice,oferta,1003880726\n",
      "title: Principal Database Engineer\n",
      "work_location: None\n",
      "validity: valid for 7 daysto 05 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['KatowiceKatowice, Silesian', 'contract of employment']\n",
      "technologies: ['Python', 'MySQL', 'PostgreSQL', 'Oracle', 'AWS', 'Ansible', 'Terraform', 'Jenkins']\n",
      "responsibilities: ['Set up and manage on-premises and cloud database environments to ensure robust performance.', 'Optimize database performance, security, and availability across large-scale systems.', 'Collaborate with service operations to prototype solutions that improve reliability.', 'Automate repetitive tasks with Python or Shell scripting.', 'Build best practices for MySQL, Redshift, and PostgreSQL, staying updated with cloud and open-source advances.']\n",
      "requirements: ['Proven experience in database architecture, administration, and tuning for critical systems.', 'Working knowledge of databases like Oracle, MS SQL, PostgreSQL and/or others.', 'Strong Linux skills for database support.', 'Proficiency in Shell or Python scripting.', 'Experience with AWS or similar cloud platforms.', 'Interest in learning new database technologies, tools, and practices.', 'Familiarity with IaC tools like Ansible, Terraform, or Jenkins.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/principal-database-engineer-katowice,oferta,1003880726?apid=42f5ed1b-b6fa-4eff-9bdc-9501c54e4613&oca=None&acv=0\n",
      "\n",
      "--- Job Record 186 ---\n",
      "id: 239\n",
      "url: https://www.pracuj.pl/praca/data-scientist-analyst-warszawa,oferta,1003876547\n",
      "title: Data Scientist Analyst\n",
      "work_location: None\n",
      "validity: ważna jeszcze 4 dnido 02 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL', 'Python', 'Statystyka', 'ML', 'Java', 'Scala', 'spark', 'Hadoop', 'u klienta', 'branżowe platformy e-learningowe']\n",
      "responsibilities: ['udział w projektach z branży bankowej,', 'odpowiedzialność za tworzenie modeli oraz analizę danych na potrzeby projektów z obszarów takich jak proces mining, churn analysis, fraud prevention czy NLP,', 'określenie problemu biznesowego, a następnie szukanie danych - zarówno w organizacji, jak i poza nią,', 'odpowiedzialność za koncepcję i przedstawienie wizji produktu, a następnie w zaangażowanie partnerów biznesowych w jej realizację,', 'realizacja zadań w ramach modelu T-shaped data scientist.', 'praca hybrydowa - spotkania zespołowe w biurze 2 razy w miesiącu (Warszawa),']\n",
      "requirements: ['doświadczenie w tworzeniu modeli, biegłość w statystyce i dobra znajomość zagadnień uczenia maszynowego,', 'zmysł produktowy oraz warsztat inżynierski,', 'znajomość: SQL, Python (numpy, scipy, pandas, matplotlib itp.),', 'znajomość bibliotek MLowych,', 'mile widziane doświadczenie w Java/Scala, doświadczenie w Hadoop oraz Apache Spark,', 'podstawowe umiejętności z obszaru DevOps / Linux Administration.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-analyst-warszawa,oferta,1003876547?apid=0158bb9b-072a-40d3-a78b-afbe60a21bb4&oca=None&acv=0\n",
      "\n",
      "--- Job Record 187 ---\n",
      "id: 240\n",
      "url: https://www.pracuj.pl/praca/senior-machine-learning-engineer-nlp-warszawa-chmielna-73,oferta,1003907318\n",
      "title: Senior Machine Learning Engineer (NLP)\n",
      "work_location: Company locationChmielna 73, Wola, WarszawaWarszawa, Masovian\n",
      "validity: valid for over a monthto 31 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['Python', 'PyTorch', 'NLP', 'LLM', 'MLOps', 'in house', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you develop the code \"from scratch\"', 'you focus on product development', 'agile']\n",
      "responsibilities: [\"Be a part of a small team whose goal is to build tools on top of text data, to help in the automatic analysis of our client's problems.\", 'Own and drive the full machine learning lifecycle from data preprocessing to model deployment, overcoming challenges in data pipelines, experimentation, and transitioning research into robust production-level systems.', \"Design, build, and refine machine learning models, particularly in NLP, to improve our chatbot's conversational abilities and language understanding.\", 'Keep up-to-date with the latest research and techniques in deep learning and NLP.', 'Optimize infrastructure for machine learning production systems.', 'Monitor and analyze model performance in live environments and implement improvements.', 'Collaborate with product managers and engineering teams to identify and implement ML solutions for product features.']\n",
      "requirements: ['Have 4+ years of experience building NLP and deep learning models in industry settings.', 'Have expert knowledge in Python, PyTorch, and common ML frameworks.', 'Have strong software engineering skills and experience deploying models to production.', 'Have a strong passion and drive for self-development – following the latest trends and publications in the field of your specialization on an ongoing basis.', 'Have a team-player mindset.', 'Can communicate in English fluently.', 'At this point, asking about ownership, communication, and problem-solving skills seems like a rhetorical question, right?']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-machine-learning-engineer-nlp-warszawa-chmielna-73,oferta,1003907318?apid=b9a10997-1c39-40e9-a249-334b8e1270ae&oca=None&acv=0\n",
      "\n",
      "--- Job Record 188 ---\n",
      "id: 241\n",
      "url: https://www.pracuj.pl/praca/ml-ai-developer-warszawa,oferta,1003907308\n",
      "title: ML / AI Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 18 dnido 16 marca 2025\n",
      "contract_type: umowa o pracę, umowa zlecenie\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Python', 'Git', 'Docker', 'Kubernetes', 'MLFlow', 'Nvidia TRITON', 'CVAT', 'C++', 'wewnątrz organizacji', 'koncentrujesz się na jednym projekcie', 'możesz zmienić projekt', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'koncentrujesz się na utrzymaniu kodu']\n",
      "responsibilities: ['Prace badawcze i rozwojowe nad wykorzystaniem i budową algorytmów uczenia maszynowego i uczenia głębokiego (modeli LLM, wizyjnych, multimodalnych) w praktycznych zastosowaniach', 'Rozwój i tworzenie rozwiązań programistycznych wspierających proces uczenia, ewaluacji i inferencji modeli uczenia głębokiego', 'Wdrażanie rozwiązań ML/AI w środowisku produkcyjnym do wykorzystania w portalach internetowych i środowiskach GIS', 'Automatyzacja zadań analizy GIS i przetworzeń fotogrametrycznych z wykorzystaniem rozwiązań ML/AI', 'Przetwarzanie dużych zbiorów danych przestrzennych (m.in. filmów, zdjęć i chmur punktów) i optymalizacją procesów przetwarzania tych zbiorów']\n",
      "requirements: ['Biegłość w używaniu języka Python.', 'Bardzo dobra znajomość zagadnień związanych z uczeniem maszynowym i uczeniem głębokim', 'Znajomość systemu operacyjnego Linux', 'Min. 3 lata doświadczenia komercyjnego w pracy z językiem Python.', 'Dobra znajomość matematyki i statystyki', 'Umiejętność samodzielnego rozwiązywania problemów i podejmowania decyzji technicznych.', 'Znajomość metodologii wytwarzania oprogramowania oraz zasad pisania czystego kodu.', 'Doświadczenie w pracy z systemem kontroli wersji Git', 'Doświadczenie z bibliotekami Python: torch, keras, tensorflow, geopandas, shapely, GDAL, numpy, pandas, scipy', 'Płynna znajomość języka polskiego oraz znajomość języka angielskiego na poziomie umożliwiającym wytwarzanie dokumentacji technicznej.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ml-ai-developer-warszawa,oferta,1003907308?apid=db2853a2-d108-4988-b580-5904001dc30a&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 189 ---\n",
      "id: 242\n",
      "url: https://www.pracuj.pl/praca/analytics-engineer-m-w-d-namyslow-kazimierza-pulaskiego-6,oferta,1003896005\n",
      "title: Analytics Engineer (m/w/d)\n",
      "work_location: None\n",
      "validity: valid for 14 daysto 12 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kazimierza Pułaskiego 6, NamysłówNamysłów, Opole', 'contract of employment', 'full office work']\n",
      "technologies: ['SQL', 'Python', 'PowerShell']\n",
      "responsibilities: ['Conduct requirements analysis to understand and document user needs and system specifications.', 'Develop and implement new reports and dashboards to meet business needs.', 'Provide expert consultation and support to various departments on Splunk-related matters.', 'Collaborate with cross-functional international teams to integrate solutions with other systems and applications.', 'Manage and maintain and support the Reporting solutions to ensure optimal performance and availability.', 'Stay updated with the latest BI features and best practices to continuously improve our reporting strategy.', 'Documentation of solutions and system governance.']\n",
      "requirements: ['Proven experience in BI or Data Analytics, and SQL. Preferred in Splunk.', 'Proficiency in data visualization.', 'Experience with requirements analysis and documentation.', 'Excellent problem-solving skills and attention to detail.', 'Strong communication and interpersonal skills.', 'Project management skills.', 'Completed education in IT, preferably university degree.', 'Fluent English.', 'Ability to make judgments and solve problems as well as analytical thinking skills.', 'Profound knowledge of data acquisition from various systems, file based or from databases.', 'Knowledge in Splunk Development.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/analytics-engineer-m-w-d-namyslow-kazimierza-pulaskiego-6,oferta,1003896005?apid=af3cd8b2-ad35-4118-a734-cbcb6383449c&oca=None&acv=0\n",
      "\n",
      "--- Job Record 190 ---\n",
      "id: 243\n",
      "url: https://www.pracuj.pl/praca/cloud-infrastructure-dba-katowice-chorzowska-148,oferta,1003895967\n",
      "title: Cloud Infrastructure DBA\n",
      "work_location: None\n",
      "validity: valid for 14 daysto 12 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Chorzowska 148, KatowiceKatowice, Silesian']\n",
      "technologies: ['PostgreSQL', 'AWS', 'Python', 'Linux', 'Terraform', 'Ansible', 'in house', 'kanban']\n",
      "responsibilities: ['Administering and managing existing database clusters and individual databases (using Iaac tools like Terraform).', 'Designing database structures and collaborating with other teams to achieve technical alignment.', 'Building and setting up new database clusters.', 'Implementing backup, restore, disaster recovery, and business continuity strategies for database systems.', 'Creating and maintaining technical and non-technical documentation for database systems and related tooling.', 'Serving as the liaison between development teams and Infrastructure.', 'Mediating and advising on the division of responsibilities between infrastructure and application layers.']\n",
      "requirements: ['Strong knowledge and practical experience as an administrator (DBA) with PostgreSQL.', 'Working knowledge of AWS, sufficient for efficient day-to-day operations.', 'Basic knowledge of Python, or a willingness to learn the basics.', 'A solid foundational understanding of Linux.', 'Experience with Terraform and/or Ansible.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/cloud-infrastructure-dba-katowice-chorzowska-148,oferta,1003895967?apid=975cdf64-1c00-4d71-a0be-818465adab00&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 191 ---\n",
      "id: 244\n",
      "url: https://www.pracuj.pl/praca/data-scientist-w-departamencie-customer-intelligence-warszawa-rondo-ignacego-daszynskiego-2c,oferta,1003895952\n",
      "title: Data Scientist w Departamencie Customer Intelligence\n",
      "work_location: None\n",
      "validity: ważna jeszcze 14 dnido 12 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['rondo Ignacego Daszyńskiego 2c, Wola, WarszawaWarszawa, mazowieckie']\n",
      "technologies: []\n",
      "responsibilities: ['wykorzystanie zaawansowanej analityki i metod uczenia maszynowego w projektach analitycznych z różnych obszarów działalności Banku, m.in.: dobór najlepiej dopasowanej oferty do klienta w zakresie treści, produktu, kanału komunikacji (modele predykcyjne, segmentacje, systemy rekomendacyjne), optymalizacja oferty pod względem kosztu/zysku, identyfikacja trendów, potencjałów sprzedażowych,', 'projektowanie, wdrażanie i rozwijanie automatycznych procesów przetwarzania dużych zbiorów danych na potrzeby data martów analitycznych (w tym danych pozyskiwanych w czasie rzeczywistym),', 'poszukiwanie nowych źródeł danych (wewnętrznych i zewnętrznych) przydatnych do celów analitycznych oraz włączanie ich w istniejące procesy ETL,', 'odpowiedzialność za wszystkie etapy powstawania nowego rozwiązania analitycznego we współpracy z innymi jednostkami Banku – od zdefiniowania i zrozumienia problemu, przez wybór metody i realizację zadania po prezentację wyników oraz poprawne i efektywne wdrożenie.']\n",
      "requirements: ['wykształcenie wyższe obejmujące wiedzę z obszaru analityki ilościowej (matematyka, statystyka, metody ilościowe, ekonometria itp.),', 'minimum 4-letnie doświadczenie w wykorzystywaniu modelowania ML/AI i innych technik data science do rozwiązywania problemów biznesowych;', 'doświadczenie w budowaniu i utrzymywaniu procesów zasilania i przetwarzania dużej ilości danych w środowisku chmurowym', 'umiejętność interpretacji danych, weryfikowania hipotez na ich podstawie i syntezy uzyskanych wniosków', 'samodzielność i inicjatywa w poszukiwaniu nowych rozwiązań oraz koncentracja na ich biznesowej użyteczności,', 'znajomość SQL oraz co najmniej jednego z języków: R, Python w zakresie pozwalającym na efektywną i samodzielną pracę analityczną; biegła znajomość MS Office (w szczególności MS Excel),', 'gotowość do pracy hybrydowej z warszawskiego biura w trybie: 50/50.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-w-departamencie-customer-intelligence-warszawa-rondo-ignacego-daszynskiego-2c,oferta,1003895952?apid=5641d79d-9413-4c4f-91b7-6fbdf0699c37&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 192 ---\n",
      "id: 245\n",
      "url: https://www.pracuj.pl/praca/staz-z-lwem-data-management-katowice,oferta,1003928309\n",
      "title: Staż z Lwem - Data Management\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 27 marca 2025\n",
      "contract_type: umowa zlecenie\n",
      "employment_type: pełny etat\n",
      "position: None\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['KatowiceKatowice, śląskie', 'praktykant / stażysta']\n",
      "technologies: ['SQL']\n",
      "responsibilities: ['przeprowadzasz kontrole jakości danych oraz optymalizacje narzędzi kontrolnych', 'budujesz raporty na bazie wymagań biznesowych', 'przeglądasz istniejące kody SQL na potrzeby aktualizacji dokumentacji', 'wspierasz zespół w bieżących  zadaniach', 'analizujesz oraz proponujesz działania usprawniające w realizowanych procesach']\n",
      "requirements: ['posiadasz wykształcenie wyższe lub ostatni rok studiów (preferowane kierunki: informatyka, ekonometria, ekonomia i finanse)', 'dobrze znasz język SQL Developer', 'masz zdolności analityczne w zakresie zarządzania wymaganiami oraz interpretacji danych', 'potrafisz biegle obsługiwać Pakiet MS Office, w tym zaawansowane funkcje Excel', 'posiadasz zdolności analityczne oraz wizualizacji i prezentacji wyników analiz', 'cechuje Cię samodzielność, rzetelność, asertywność']\n",
      "application_link: https://www.pracuj.pl/aplikuj/staz-z-lwem-data-management-katowice,oferta,1003928309?apid=27be3343-cef5-4445-87f8-6d54356a10b5&oca=None&acv=0\n",
      "\n",
      "--- Job Record 193 ---\n",
      "id: 246\n",
      "url: https://www.pracuj.pl/praca/senior-data-scientist-location-intelligence-warszawa-zelazna-51,oferta,1003916311\n",
      "title: Senior Data Scientist (Location Intelligence)\n",
      "work_location: None\n",
      "validity: valid for 23 daysto 21 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Żelazna 51, Wola, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['Python', 'Pandas', 'Scikit-learn', 'CatBoost', 'XGBoost', 'TensorFlow', 'Matplotlib/Plotly', 'Optuna', 'Poetry', 'Git', 'Tableau', 'Looker Studio', 'SQL', 'BigQuery', 'Google Cloud Platform', 'PySpark', 'GeoPandas', 'Shapely', 'spatial indexes', 'Carto', 'Esri', 'Kubernetes']\n",
      "responsibilities: ['Data Modeling & Analysis: Creating complex data models using machine learning, deep learning and statistics to understand and predict patterns.', 'Data Processing & Automation: Employing Python and SQL on cloud platforms (mainly GCP), to process and automate petabytes of data.', 'Geospatial Analytics: Using GIS tools, Python GIS libraries and location-based data to extract insights and improve decision-making.', 'Operational Excellence: Identify and mitigate supply chain bottlenecks, manage volume load across different routes, and dynamically allocate volume between Allegro’s own operations (AOK) and third-party carriers.', 'Research Participation: Involvement in research activities whose findings and recommendations directly influence the development direction of our products.', 'Data Management: Collaborating with engineers to foster the availability, integrity, accuracy, and reliability of data and pipelines.', 'Collaboration: Working with the team to tackle complex issues, mentoring junior members, and fostering knowledge sharing.', 'Communication & Data Visualization: Providing clear, visually focused communication of advanced data insights to non-technical stakeholders through report writing and presenting.']\n",
      "requirements: ['Education: Graduated with a degree strongly related to statistical/mathematical modeling, such as Mathematics, Physics, Economics, or Computer Science.', 'Experience: Have a minimum of 4 years of experience in a Data Scientist  role or a similar capacity', 'Mandatory Technical Skills: Are proficient in programming languages, libraries and tools like Python, Pandas, Scikit-learn, CatBoost, XGBoost, TensorFlow, Matplotlib/Plotly, Optuna, Poetry, Git, and Tableau/Looker Studio.', 'Data Analysis Skills: Have experience with analyzing large data volumes using technologies like SQL (BigQuery) and GCP Platform (including Airflow and Vertex AI).', 'Technical Understanding: Good grasp of statistical, machine learning and deep learning methods.', 'Critical Analyzer: Have strong analytical skills for interpreting complex data.', 'Adaptability: Quick at learning new technologies and keeping updated with industry trends.', 'Problem-Solving Skills: Eager to translate business problems into ML solutions.', 'Team Player: Committed to fostering a team-oriented environment centered around shared problem-solving.', 'Communication Skills: Can effectively communicate with business units, from formulating an analytical problem to providing a clear and intuitive presentation of results.', 'Desired Technical Skills: Familiarity with PySpark, GeoPandas, Shapely, spatial indexes, Carto, Esri, Kubernetes is preferred.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-scientist-location-intelligence-warszawa-zelazna-51,oferta,1003916311?apid=b4731f9b-8865-4f09-8d15-431e8596bcde&oca=None&acv=0\n",
      "\n",
      "--- Job Record 194 ---\n",
      "id: 247\n",
      "url: https://www.pracuj.pl/praca/data-visualization-specialist-krakow,oferta,1003889155\n",
      "title: Data Visualization Specialist\n",
      "work_location: None\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KrakówKraków, Lesser Poland', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Tableau', 'Python', 'Pandas', 'NumPy', 'PySpark', 'QlikSense', 'agile', 'scrum']\n",
      "responsibilities: ['Your main responsibilities: Developing advanced dashboards and reports using Tableau', 'Understanding infrastructure requirements and best practices for Tableau deployment', 'Ensuring data integrity and updating data sources as needed', 'Managing the project site on Tableau Server', 'Documenting processes, data sources, and dashboard functionalities', 'Analyzing and driving data-sharing best practices around user access', 'Collaborating with cross-functional teams to support business intelligence needs', 'Querying and displaying large data sets while optimizing Tableau workbook performance', 'Managing stakeholder expectations and influencing decision-making through data insights', 'Supporting the development of scalable and efficient data visualization solutions']\n",
      "requirements: ['3+ years of experience in data analysis and visualization', 'Proficiency in Tableau, including dashboard development and optimization', 'Working knowledge of Tableau administration and architecture', 'Strong stakeholder management and influencing skills', 'High mathematical competence with an analytical mindset', 'Experience ensuring data integrity and managing data sources', 'Ability to work collaboratively with cross-functional teams and stakeholders at all levels', 'Strong problem-solving skills with the ability to translate business needs into visual solutions', 'Knowledge of best practices in data sharing, governance, and user access', 'Strong documentation skills to support dashboard maintenance and scalability']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-visualization-specialist-krakow,oferta,1003889155?apid=f30f3a10-73c9-44b1-a900-031847f01bf7&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 195 ---\n",
      "id: 248\n",
      "url: https://www.pracuj.pl/praca/bi-manager-krakow-wielicka-28,oferta,1003838715\n",
      "title: BI Manager\n",
      "work_location: Company locationWielicka 28, Podgórze, KrakówKraków, Lesser Poland\n",
      "validity: valid for 3 daysto 01 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'contract of employment', 'team manager']\n",
      "technologies: ['Microsoft Power BI', 'SQL', 'Python', 'Microsoft Azure', 'AWS', 'Google Cloud Platform', 'in house', 'you have influence on the product', 'you focus on product development', 'agile', 'product owner', 'business analyst']\n",
      "responsibilities: ['Lead, mentor, and manage a team of BI developers and analysts, fostering a culture of innovation and continuous learning', 'Define clear objectives, performance metrics, and career development plans for team members', 'Ensure the team delivers high-quality, scalable BI solutions that meet business requirements', 'Design and implement Power BI dashboards, reports, and data models, ensuring usability, scalability, and reliability', 'Collaborate with stakeholders to translate business needs into actionable BI solutions', 'Oversee the architecture and implementation of data pipelines, ensuring efficient integration with data warehouses and Databricks', 'Drive best practices for data modeling, report development, and visualization standards', 'Serve as a trusted partner to cross-functional teams, including IT, Finance, Operations, and Marketing, to identify BI needs and opportunities', 'Provide insights and recommendations to senior leadership, enabling data-driven decision-making', 'Establish and maintain strong relationships with external vendors and partners for BI tools and technologies', 'Oversee the design, development, and maintenance of the data warehouse and analytics infrastructure', 'Leverage SQL and Databricks to optimize data processing, storage, and transformation workflows', 'Ensure data accuracy, security, and compliance with organizational and regulatory standards', 'Monitor BI system performance and implement improvements where needed']\n",
      "requirements: ['Proven experience in a BI Manager or similar leadership role, with hands-on expertise in Power BI (5+ years)', 'Deep undertanding of data warehouse concepts, including ETL processes and dimensional modeling', 'Proficiency in SQL for querying, transformation, and optimization', 'Experience working with Databricks or similar cloud-based data platforms', 'Excellent leadership and team management skills, with a track record of delivering successful projects', 'Strong analytical, problem-solving, and decision-making abilities', 'Exceptional communication skills to articulate technical concepts to non-technical stakeholders', 'Knowledge of Python or other programming languages used in data analytics', 'Experience with cloud platforms like Azure, AWS, or Google Cloud', 'Familiarity with data governance, security, and compliance standards', 'Certification in Power BI, SQL, or related BI tools']\n",
      "application_link: https://www.pracuj.pl/aplikuj/bi-manager-krakow-wielicka-28,oferta,1003838715?apid=8bccb58f-7467-4ed8-8f98-1b854a028fe6&oca=None&acv=0\n",
      "\n",
      "--- Job Record 196 ---\n",
      "id: 249\n",
      "url: https://www.pracuj.pl/praca/data-engineer-ai-projects-wroclaw,oferta,1003928242\n",
      "title: Data Engineer (AI Projects)\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 27 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WrocławWrocław, dolnośląskie', 'Szukamy wielu kandydatów']\n",
      "technologies: ['Databricks', 'Python', 'PySpark', 'Git', 'AI', 'Azure Data', 'Azure SQL', 'Generative AI', 'u klienta', 'koncentrujesz się na jednym projekcie', 'możesz zmienić projekt', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu']\n",
      "responsibilities: ['Integracja rozwiązań AI z systemami', 'Rozwiązywanie problemów związanych z utrzymaniem danych i ich jakością', 'Optymalizacja i projektowanie architektury E2E w Azure']\n",
      "requirements: ['4-5 lat doświadczenia jako Data Engineer', 'Bardzo dobra znajomość Microsoft Azure', 'Doświadczenie z bazami danych i hurtowniami danych', 'Znajomość Databricks (Delta Tables, DLT, Unity Catalog, optymalizacja wydajności)', 'Python, PySpark', 'Umiejętność pracy z Azure Data Factory, Azure SQL Database, Azure Key Vault, Azure OpenAI Service', 'Znajomość nowoczesnego modelowania danych', 'Znajomość mechanizmów uwierzytelniania w Azure', 'Język angielski – min. B2 (praca w międzynarodowym środowisku)', 'Gotowość do odwiedzania biura we Wrocławiu co 10 tygodni']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-ai-projects-wroclaw,oferta,1003928242?apid=33880c18-b873-420f-b193-95db79f7ec8c&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 197 ---\n",
      "id: 250\n",
      "url: https://www.pracuj.pl/praca/informatica-powercenter-developer-warszawa,oferta,1003895716\n",
      "title: Informatica PowerCenter Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 14 dnido 12 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Oracle', 'SQL', 'PL/SQL', 'wewnątrz organizacji', 'koncentrujesz się na jednym projekcie', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'tworzysz kod \"od zera\"', 'koncentrujesz się na rozwoju produktu', 'koncentrujesz się na utrzymaniu kodu', 'agile', 'scrum', 'mobile developer', 'lider techniczny', 'support', 'scrum master', 'analityk biznesowy', 'analityk systemowy', 'administrator IT']\n",
      "responsibilities: ['Prowadzenie prac projektowych i implementacyjnych w zakresie rozwoju hurtowni danych,', 'Projektowanie algorytmów oraz implementacja procesów ETL,', 'Udział we wdrożeniu narzędzia Informatica,', 'Projektowanie baz danych i obszarów raportowo-analitycznych,', 'Projektowanie, implementacja i rozwój środowisk raportowo-analitycznych w oparciu o środowisko klasy BI,', 'Testowanie rozwiązań informatycznych,', 'Tworzenie dokumentacji technicznej,', 'Przygotowywanie pakietów instalacyjnych oprogramowania,']\n",
      "requirements: ['Masz minimum 3 lata doświadczenia z zakresu projektowania, implementacji oraz wdrażania rozwiązań ETL w oparciu o platformę Informatica Power Center,', 'Masz minimum 3 lata doświadczenia w pracy z RDBMS Oracle 9i / 10g w zakresie projektowania baz danych i aplikacji,', 'Bardzo dobrze znasz SQL oraz PL/SQL,', 'Posiadasz znajomość języka angielskiego.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/informatica-powercenter-developer-warszawa,oferta,1003895716?apid=703aefd7-aef4-4c6f-b958-fa5caf3a93e5&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 198 ---\n",
      "id: 251\n",
      "url: https://www.pracuj.pl/praca/big-data-developer-warszawa,oferta,1003895692\n",
      "title: Big Data Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 14 dnido 12 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Spark', 'Python', 'Hadoop', 'Kafka', 'Apache Nifi', 'wewnątrz organizacji', 'koncentrujesz się na jednym projekcie', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'tworzysz kod \"od zera\"', 'koncentrujesz się na rozwoju produktu', 'koncentrujesz się na utrzymaniu kodu', 'agile', 'scrum', 'mobile developer', 'lider techniczny', 'support', 'scrum master', 'analityk biznesowy', 'analityk systemowy', 'administrator IT']\n",
      "responsibilities: ['Projektowanie architektury i wdrażanie nowoczesnych systemów przetwarzania danych w obszarze Big Data.', 'Dbanie o wysoką jakość procesu rozwojowo-wdrożeniowego', 'Dbanie o wysoką jakość dokumentacji technicznej', 'Poszukiwanie nowych rozwiązań oraz ich implementację (R&D)', 'Współpraca z innymi zespołami w celu wsparcia developmentu']\n",
      "requirements: ['Min. 4 lata komercyjnego doświadczenia w programowaniu w Spark procesów batch/streaming w Python', 'Masz doświadczenie w integracji danych z wielu źródeł', 'Posiadasz wiedzę z zakresu Big Data, Hurtowni Danych i Zarządzania Danymi', 'Znasz platformę Hadoop Cloudera/Hortonworks', 'Posiadasz wiedzę z zakresu przetwarzania dużych zbiorów danych, w tym standardów projektowania, kodowania, dokumentowania, testowania i wdrażania', 'Znasz technologie i narzędzia strumieniowe takie jak Kafka, Apache Nifi', 'Masz doświadczenie z różnymi formatami danych: JSON, PARQUET, ORC, AVRO', 'Posiadasz zrozumienie typów baz danych i scenariuszy użycia, np. hive, kudu, hbase, Iceberg itp.', 'Znasz język SQL na poziomie zaawansowanym']\n",
      "application_link: https://www.pracuj.pl/aplikuj/big-data-developer-warszawa,oferta,1003895692?apid=d6cbc281-8b68-4e88-b014-d9d0f0c9325d&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 199 ---\n",
      "id: 252\n",
      "url: https://www.pracuj.pl/praca/big-data-junior-warszawa,oferta,1003895688\n",
      "title: Big Data Junior\n",
      "work_location: None\n",
      "validity: ważna jeszcze 14 dnido 12 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: młodszy specjalista (Junior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Hadoop', 'Python', 'SQL', 'Scala', 'wewnątrz organizacji', 'koncentrujesz się na jednym projekcie', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'tworzysz kod \"od zera\"', 'koncentrujesz się na rozwoju produktu', 'koncentrujesz się na utrzymaniu kodu', 'agile', 'scrum', 'mobile developer', 'lider techniczny', 'support', 'scrum master', 'analityk biznesowy', 'analityk systemowy', 'administrator IT']\n",
      "responsibilities: ['Wdrażanie nowoczesnych systemów przetwarzania danych w obszarze Big Data', 'Dbanie o wysoką jakość procesu rozwojowo-wdrożeniowego', 'Dbanie o wysoką jakość dokumentacji technicznej', 'Współpraca z innymi zespołami w celu wsparcia developmentu']\n",
      "requirements: ['Min. 1 rok doświadczenia komercyjnego', 'Znajomość koncepcji Big Data, Hurtowni Danych i Zarządzania Danymi', 'Znajomość platformy Hadoop Cloudera/Hortonworks', 'Min. 1 rok doświadczenia w programowaniu procesów spark w Python (lub Scala)', 'Doświadczenie z różnymi formatami danych: JSON, PARQUET, ORC, AVRO', 'Zrozumienie typów baz danych i scenariuszy użycia, np. hive, kudu, hbase itp.', 'Znajomość języka SQL na poziomie zaawansowanym', 'Doświadczenie w integracji danych z wielu źródeł danych']\n",
      "application_link: https://www.pracuj.pl/aplikuj/big-data-junior-warszawa,oferta,1003895688?apid=8fe0b301-87d0-4c0c-894d-e48af987be7e&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 200 ---\n",
      "id: 253\n",
      "url: https://www.pracuj.pl/praca/ai-tech-lead-warszawa-konstruktorska-9,oferta,1003907019\n",
      "title: AI Tech Lead\n",
      "work_location: None\n",
      "validity: valid for 18 daysto 16 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Konstruktorska 9, Mokotów, WarszawaWarszawa, Masovian']\n",
      "technologies: ['Jenkins', 'GitLab', 'Terraform', 'Ansible', 'Docker', 'Kubernetes', 'AWS', 'Microsoft Azure', 'Google Cloud Platform', 'Prometheus', 'Grafana', 'Python', '.NET', 'in house']\n",
      "responsibilities: ['Lead the design and architecture of AI solutions, ensuring alignment with business goals and technical standards', 'Coordinate with project managers and stakeholders to define project scope, priorities, and timelines.', 'Familiar with latest advancements in AI and machine learning technologies, tools, and methodologies', 'Establish and enforce coding standards, best practices, and methodologies for solution development', 'Ensure that developed solutions comply with data privacy and security regulations', 'Analyze solutions to ensure they meet scalability and efficiency requirements', 'Implement continuous monitoring and improvement processes for AI systems']\n",
      "requirements: ['Understanding of full software development lifecycle, including requirements gathering, design, coding, testing, and deployment', 'Ability to design and implement APIs for model deployment and integration with other systems', 'Skills in CI/CD tools (Jenkins, GitLab CI, CircleCI)', 'Infrastructure as code tools (Terraform, Ansible, CloudFormation)', 'Knowledge of containerization technologies (Docker, Kubernetes)', 'Skilled in cloud platforms DevOps services (AWS, Azure, GCP)', 'Familiar with monitoring/logging tools (Prometheus, Grafana)', 'Proficiency in one or more of the following software languages and ecosystems (Python, .Net)', 'Ability to lead and mentor team', 'Contribute to the strategic vision for AI initiatives within the organization']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-tech-lead-warszawa-konstruktorska-9,oferta,1003907019?apid=c7c7b38a-e66a-4f80-ad2f-bb81ad8ff706&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 201 ---\n",
      "id: 254\n",
      "url: https://www.pracuj.pl/praca/data-scientist-gtech-scaled-technical-solutions-warszawa-emilii-plater-53,oferta,1003889060\n",
      "title: Data Scientist - gTech Scaled Technical Solutions\n",
      "work_location: None\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Emilii Plater 53, Śródmieście, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['Python', 'R', 'SQL']\n",
      "responsibilities: ['Lead data science aspects of client engagements in the area of marketing portfolio management.', 'Collaborate with customers to solve their problems and identify the best statistical techniques that can solve the problem, and own the development of modeling framework.', 'Engage important stakeholders to assess data and model readiness and be able to scale a proof-of-concept to a solution.', 'Work with customer and internal teams to translate data and model results into tactical and strategic insights for decision-making, and work with clients to integrate recommendations into business processes.', 'Collaborate with Product/Engineering teams to increase and optimize capabilities of the Applied DS team, employing methods which create opportunities for scale, and help to drive innovation.']\n",
      "requirements: [\"Master's degree in a quantitative discipline such as Statistics, Engineering, Sciences, or equivalent practical experience.\", '3 years of experience using analytics to solve product or business problems, coding (e.g., Python, R, SQL), querying databases or statistical analysis, or a PhD degree.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-gtech-scaled-technical-solutions-warszawa-emilii-plater-53,oferta,1003889060?apid=1d32b9d1-3f8a-497f-ade5-6e8216c22268&oca=None&acv=0\n",
      "\n",
      "--- Job Record 202 ---\n",
      "id: 255\n",
      "url: https://www.pracuj.pl/praca/data-scientist-warszawa,oferta,1003867762\n",
      "title: Data Scientist\n",
      "work_location: None\n",
      "validity: valid for a dayto 27 February 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Kubernetes', 'Docker', 'Python', 'Go', 'Java', 'C++', 'agile', 'scrum']\n",
      "responsibilities: ['Working on a tool allowing to extract information from documents (fund prospectus, invoices, term sheets, loan notices)', 'Creating, training & improving information extraction models (new or existing ones)', 'Participating to testing new extraction approaches (e.g. Donut vs Layout ML, LLM)', 'Contribute to improvement of the platform', 'Analyze large amounts of information to discover trends and patterns', 'Propose solutions and strategies to business challenges']\n",
      "requirements: ['Minimum 4 years of experience on a similar position', 'Strong statistical and quantitative analysis skills', 'Strong Machine Learning and Deep Learning skills (RNNs, LSTMs, Transformers) & skills in data cleaning, normalization, and transformation, specifically for textual data', 'Strong experience in NLP and fine knowledge of NLP Libraries is a prerequisite, experience in contributing to an Intelligent Document Processing platform', 'Model Deployment and Scaling: Experience in deploying NLP models into production environments, and scaling them to handle large volumes of data (Kubernetes, Docker)', 'Python Programming Skills', 'Ability to communicate clearly on day to day tasks and potential challenges', 'Experience working with international and remote teams', 'Proactivity and autonomy', 'Fluent English']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-warszawa,oferta,1003867762?apid=9d459f4d-fa38-46f0-8c1a-6ec6eef7cf09&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 203 ---\n",
      "id: 256\n",
      "url: https://www.pracuj.pl/praca/data-engineer-warszawa-pulawska-2,oferta,1003888987\n",
      "title: Data Engineer\n",
      "work_location: Company locationPuławska 2, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['SQL', 'Python', 'Snowflake', 'Docker', 'Jenkins', 'DBT', 'MonteCarlo', 'DataDog', 'Terraform']\n",
      "responsibilities: ['Data modeling, data mapping, and business data analysis using Jira.', 'Identifying errors and ensuring data integrity.', 'Working in an Agile environment and adapting to changing business requirements.', 'Daily SQL tasks, including Snowflake data management and issue resolution.', 'Implementing new processes within Snowflake data warehouses.', 'Object-oriented programming and scripting in Python or Java.', 'Gaining expertise in the data generated by diagnostic instruments.', 'End-to-end development of data products, including business logic implementation, data quality validation, and maintaining Collibra data documentation.']\n",
      "requirements: ['5+ years of experience in data engineering.', 'Strong proficiency in Snowflake and DBT.', 'Experience in a programming language for data pipelines (Python or R).', 'Experience  with SQL.', 'Experience  in maintaining data pipelines.', 'Experience  working with various types of storage systems (filesystem, relational, MPP, NoSQL) and different data formats (structured, unstructured, metrics, logs, etc.).', 'Experience  with data architecture concepts (data modeling, metadata management, workflow management, ETL/ELT, real-time streaming, data quality, distributed systems).', 'Experience with cloud-based data pipeline technologies (e.g., Airflow, Glue, Dataflow, Redshift, BigQuery, Lambda, S3, EBS, etc.).', 'Strong knowledge of relational databases.', 'Proficiency in data serialization formats such as JSON, XML, YAML.', 'Excellent understanding of Git, Gitflow, and DevOps tools (e.g., Docker, Bamboo, Jenkins, Terraform).', 'Strong Unix skills.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-warszawa-pulawska-2,oferta,1003888987?apid=1ae15e5e-fbab-4222-853f-17d28232077c&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 204 ---\n",
      "id: 257\n",
      "url: https://www.pracuj.pl/praca/senior-backup-engineer-commvault-wroclaw-powstancow-slaskich-9,oferta,1003898719\n",
      "title: Senior Backup Engineer (Commvault)\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 15 dnido 13 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Commvault']\n",
      "responsibilities: ['projekt związany z rozwojem oprogramowania oraz infrastruktury dla firm publicznych i rządowych, które chcą tworzyć kopie zapasowe i przywracać dane,', 'zarządzanie i utrzymywaniu systemów kopii zapasowych, w tym planowanie, monitorowanie i weryfikowanie kopii zapasowych,', 'wspieranie utrzymania infrastruktury pamięci masowej w celu zapewnienia wysokiej dostępności i wydajności,', 'przeprowadzanie regularnych testów procesów tworzenia kopii zapasowych i odzyskiwania danych w celu zapewnienia skutecznego przywracania danych,', 'zapewnienie podstawowego rozwiązywania problemów związanych z tworzeniem kopii zapasowych i pamięcią masową,', 'wspieranie starszych inżynierów w rozwiązywaniu incydentów i zapewnianiu ciągłości usług,', 'dokumentowanie i eskalowanie złożonych problemów do starszych członków zespołu.']\n",
      "requirements: ['posiadasz 3 - 5 lat doświadczenia z obszarów infrastruktury IT, Backup, Storage oraz Devops,', 'posiadasz praktyczne doświadczenie z Commvault,', 'posiadasz znajomość chmury i przechowywania danych w chmurze,', 'posiadasz silne umiejętności rozwiązywania problemów i proaktywne podejście do utrzymania niezawodności,', 'posiadasz doskonałe umiejętności komunikacji i współpracy w zespołach,', 'sprawnie komunikujesz się w j. angielskim (min. B2+).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-backup-engineer-commvault-wroclaw-powstancow-slaskich-9,oferta,1003898719?apid=f035d7fc-20b3-4cd7-aced-f505dea1451f&oca=None&acv=0\n",
      "\n",
      "--- Job Record 205 ---\n",
      "id: 258\n",
      "url: https://www.pracuj.pl/praca/mid-data-engineer-warszawa-jana-nowaka-jezioranskiego-53a,oferta,1003921567\n",
      "title: Mid Data Engineer\n",
      "work_location: Company locationJana Nowaka-Jeziorańskiego 53a, Praga-Południe, WarszawaWarszawa, Masovian\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: contract of employment, contract of mandate, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'Запрошуємо працівників з України']\n",
      "technologies: ['Microsoft Azure', 'Google Cloud Platform', 'AWS', 'Python', 'SQL', 'PostgreSQL', 'Microsoft SQL Server', 'Apache Spark', 'Databricks', 'Azure DevOps', 'Apache Airflow', 'in house', \"at the client's site\", 'you focus on a single project at a time', 'you develop several projects simultaneously', 'you can change the project', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you develop the code \"from scratch\"', 'you focus on product development', 'agile', 'scrum']\n",
      "responsibilities: ['Implementing, and optimizing modern cloud-based solutions.', 'Building and launching new data models and data pipelines.', 'Implementing best practices in data engineering including data integrity, quality, and documentation.', 'Optimization of existing analytical solutions.', 'Leading small size teams of engineers and being a role model.']\n",
      "requirements: ['2+ years of experience delivering complex data warehouse / data lake / business intelligence solutions.', '1+ years of experience working with cloud services (Azure / GCP / AWS).', 'Knowledgeable in Python.', 'Experience in SQL and data analysis, knowledge of relational databases (preferably SQL Server, PostgreSQL).', 'Knowledge of public cloud architecture, security, networking concepts and best practices (MS Azure preferred).', 'Knowledge of DWH data modeling practices and ETL/ELT development.', 'Conceptual and analytical skills.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/mid-data-engineer-warszawa-jana-nowaka-jezioranskiego-53a,oferta,1003921567?apid=0e6e04ca-4a8a-408a-b003-edd59c501064&oca=None&acv=0\n",
      "\n",
      "--- Job Record 206 ---\n",
      "id: 259\n",
      "url: https://www.pracuj.pl/praca/data-product-lead-warszawa-postepu-15,oferta,1003870152\n",
      "title: Data Product Lead\n",
      "work_location: Company locationPostępu 15, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for 2 daysto 28 February 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'manager / supervisor, team manager']\n",
      "technologies: ['QRM', 'Fusion Risk', 'OneSumX', 'in house', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'agile']\n",
      "responsibilities: ['Lead and manage a team of data professionals (developers, analysts, scientists) across multiple squads.', 'Define and execute the product vision and roadmap for ALM Data & Processing.', 'Drive the development and implementation of data marts, lakes, and products.', 'Automate data disclosure and improve the efficiency of data delivery processes.', 'Influence the strategic direction of the global organization.', 'Ensure high-quality data services within the company.', 'Collaborate with international senior stakeholders to identify and pursue new business opportunities.', 'Foster a high-performing and inclusive team culture.', 'Recruit and onboard top talent.']\n",
      "requirements: ['Proven leadership experience leading high-performing data teams (10-16 people) in a fast-paced environment.', 'Expertise with data solutions (data warehousing, data lakes, data marts, reporting), ideally within the risk domain (e.g., ALM, Interest Rate Risk).', 'Passionate about building and delivering impactful data products that drive business value.', 'Excellent communication and interpersonal skills, with the ability to effectively collaborate with stakeholders across all levels, internally and externally.', 'MSc or PhD in a relevant field (e.g., Computer Science, (Financial) Econometrics, Statistics).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-product-lead-warszawa-postepu-15,oferta,1003870152?apid=aa576be8-95ba-4e93-b9a8-aaba63ce52dc&oca=None&acv=0\n",
      "\n",
      "--- Job Record 207 ---\n",
      "id: 260\n",
      "url: https://www.pracuj.pl/praca/analityk-z-ai-voicebot-chatbot-warszawa,oferta,1003927875\n",
      "title: Analityk z AI (Voicebot, Chatbot)\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze miesiącdo 27 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['u klienta']\n",
      "responsibilities: ['Analiza wymagań zamawiającego w obszarze Voicebot i Chatbot', 'Zadbanie o identyfikację źródeł danych, modeli migracji oraz integracji z systemami Zamawiającego.', 'Analiza i rekomendacje co do wariantu biznesowego podejścia do przedmiotowego narzędzia w obszarze model licencjonowania, architektury, modeli AI', 'Przygotowanie dokumentacji merytorycznej', 'Wsparcie merytoryczne i przygotowywanie odpowiedzi na pytania dostawców']\n",
      "requirements: ['Szukamy wymiatacza, który bardzo dobrze posługuje się językiem angielskim i lubi mieć poczucie stabilności w firmie - to projekt na dłuuugie lata! Dzięki elastycznemu trybowi współpracy sam dobierzesz poziom zaangażowania. U nas możesz się rozwijać i liczyć na wsparcie przez całość trwania współpracy: od bogatego pakietu benefitów przez wsparcie przy negocjacjach aż po pomoc w znalezieniu miejsca idealnego dla Ciebie.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/analityk-z-ai-voicebot-chatbot-warszawa,oferta,1003927875?apid=5688a7cc-c9f4-4023-9886-111a301a8578&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 208 ---\n",
      "id: 261\n",
      "url: https://www.pracuj.pl/praca/architekt-danych-architektka-danych-warszawa-ksiazeca-4,oferta,1003906809\n",
      "title: Architekt Danych / Architektka Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 18 dnido 16 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Książęca 4, Śródmieście, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Enterprise Architect', 'wewnątrz organizacji', 'szkolenia wewnątrzfirmowe']\n",
      "responsibilities: ['Budowa i nadzór nad procesem Data Governance,', 'Rozwijanie i utrzymanie architektury w domenie danych,', 'Udział w planowaniu, projektowaniu i wytwarzaniu modeli danych dotyczących zarówno obecnego jak i przyszłego działania organizacji,', 'Opracowywanie i utrzymywanie wytycznych i standardów architektonicznych dotyczących danych,', 'Wdrażanie polityk bezpieczeństwa informacji,', 'Nadzorowanie rozwoju danych w organizacji,', 'Udział w utrzymywaniu architektury korporacyjnej poprzez określenie i implementowanie jego struktury i zawartości informacyjnej,', 'Definiowanie standardów jakości, określanie wymagań biznesowych i technicznych dla danych oraz ich spełnienia,', 'Monitorowanie jakości i bezpieczeństwa danych,', 'Szkolenie zespołów w zakresie zarządzania danymi.']\n",
      "requirements: ['Praktyczna znajomość oprogramowania Sparx Enterprise Architect,', 'Minimum 5-letnie doświadczenie zawodowe na stanowisku projektanta danych lub architekta rozwiązań zorientowanych na dane,', 'Znajomość rozwiązań do przetwarzania danych (bazy danych, ETL, DWH),', 'Umiejętność modelowania danych,', 'Umiejętność przystępnego komunikowania złożonych koncepcji,', 'Zdolności analitycznego i syntetycznego myślenia,', 'Umiejętność pracy w zespole.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/architekt-danych-architektka-danych-warszawa-ksiazeca-4,oferta,1003906809?apid=10484881-2937-4fa8-bc39-3004980ea5f1&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 209 ---\n",
      "id: 262\n",
      "url: https://www.pracuj.pl/praca/software-engineer-compliance-team-lodz-ogrodowa-8,oferta,1003927866\n",
      "title: Software Engineer (Compliance Team)\n",
      "work_location: None\n",
      "validity: valid for over a monthto 10 April 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Ogrodowa 8, Bałuty, ŁódźŁódź, Łódź', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['SAS', 'Python', 'SQL']\n",
      "responsibilities: ['Developing SAS, Python Codes and SQL Statements', 'Creating Dashboards within SAS Viya BI Platform Solution', 'Execute technical and functional tests to ensure software quality and that customer requirements are met', 'designing data workflows and data architecture as basis for reports and dashboards', 'automize process chains including different tools/interfaces (APIs)', 'Do debugging and bug fixing for productive solutions as well as ad hoc data analyses for our customers']\n",
      "requirements: ['Good knowledge of Python programming', 'Good knowledge of Data management', 'Basic knowledge or willingness to learn JIRA, Confluence', 'Basic knowledge or willingness to learn Automation One (UC4) or any other job scheduler', 'Basic knowledge or willingness to learn GIT', 'Nice to have: knowledge of SAS script programming']\n",
      "application_link: https://www.pracuj.pl/aplikuj/software-engineer-compliance-team-lodz-ogrodowa-8,oferta,1003927866?apid=55e8e9fd-51b9-4753-b825-ec41869e1633&oca=None&acv=0\n",
      "\n",
      "--- Job Record 210 ---\n",
      "id: 263\n",
      "url: https://www.pracuj.pl/praca/dyrektor-dyrektorka-departamentu-automatyzacji-danych-i-ai-warszawa-aleje-jerozolimskie-132,oferta,1003913182\n",
      "title: Dyrektor / Dyrektorka Departamentu Automatyzacji, Danych i AI\n",
      "work_location: None\n",
      "validity: ważna jeszcze 22 dnido 20 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: None\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Aleje Jerozolimskie 132, Ochota, WarszawaWarszawa, mazowieckie', 'dyrektor']\n",
      "technologies: ['agile', 'scrum']\n",
      "responsibilities: ['Minimum 5 lat doświadczenia w zarządzaniu zespołami IT, AI lub analizy danych', 'Doświadczenie w realizacji projektów z zakresu AI, analityki danych, automatyzacji lub innowacji cyfrowych', 'Praktyczne doświadczenie w zarządzaniu zespołami powyżej 30 osób i prowadzeniu złożonych inicjatyw technologicznych', 'Kompetencje techniczne i strategiczne: Wiedza na temat nowoczesnych technologii AI, NLP, przetwarzania danych i automatyzacji', 'Umiejętność zarządzania cyklem życia produktów AI i danych, od koncepcji po wdrożenie w administracji publicznej', 'Zrozumienie regulacji dotyczących AI, ochrony danych osobowych i zgodności prawnej', 'Doświadczenie w analizie i zarządzaniu danymi w organizacjach publicznych lub korporacyjnych', 'Kompetencje menedżerskie: Zdolność do zarządzania zespołami interdyscyplinarnymi i budowania kultury współpracy', 'Umiejętność pracy z interesariuszami na poziomie administracji rządowej i sektora publicznego', 'Doskonałe umiejętności komunikacyjne i negocjacyjne', 'Znajomość metodyk zarządzania projektami IT (Agile, Scrum, Prince2, PMI)']\n",
      "requirements: []\n",
      "application_link: https://www.pracuj.pl/aplikuj/dyrektor-dyrektorka-departamentu-automatyzacji-danych-i-ai-warszawa-aleje-jerozolimskie-132,oferta,1003913182?apid=deb3fc3c-8450-4b6a-8ba6-713f86d6f4c9&oca=None&acv=0\n",
      "\n",
      "--- Job Record 211 ---\n",
      "id: 264\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-warszawa-chlodna-51,oferta,1003927855\n",
      "title: Senior Data Engineer\n",
      "work_location: None\n",
      "validity: valid for a monthto 27 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Chłodna 51, Wola, WarszawaWarszawa, Masovian']\n",
      "technologies: ['Python', 'Azure DevOps', 'Azure Databricks', 'Azure DataFactory', 'Spark', 'PySpark', 'CI/CD', 'SQL', 'Scala', 'in house', 'you focus on a single project at a time', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'agile', 'scrum', 'fullstack developer', 'technical leader', 'devOps', 'automated test programmer', 'manual tester', 'product owner', 'business analyst', 'IT administrator']\n",
      "responsibilities: ['MLOps: Contribute to the architecture definition and establish development standards (templates, CI/CD, etc.) for projects in collaboration with data scientists.', 'GDPR: Implement and configure purge rules on the Data Lake to ensure compliance with data retention periods.', 'Supervise/monitor the proper functioning of data ingestion into the Data Lake.', 'Assist users in optimizing PySpark/Spark codes.', 'Task automation to improve monitoring / quality review']\n",
      "requirements: ['Good ability to understand technological stack and tools', 'Ability to work collaboratively within multidisciplinary teams and to help team members to learn and develop their skills', 'Autonomy, precision, pragmatism and a real capacity to deliver', 'Professional proficiency in English']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-warszawa-chlodna-51,oferta,1003927855?apid=918e3ab2-9f8d-4fe5-bb25-6c89af5618ab&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 212 ---\n",
      "id: 265\n",
      "url: https://www.pracuj.pl/praca/automation-ai-engineer-kajetany-pow-pruszkowski-klonowa-48,oferta,1003918525\n",
      "title: Automation & AI Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 24 dnido 22 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), młodszy specjalista (Junior)\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Klonowa 48, Kajetany (pow. pruszkowski)Kajetany (pow. pruszkowski), mazowieckie', 'praca stacjonarna']\n",
      "technologies: ['AI', 'wewnątrz organizacji', 'rozwijasz kilka projektów jednocześnie', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'koncentrujesz się na rozwoju produktu']\n",
      "responsibilities: ['Współtworzenie, analiza i wdrażanie innowacyjnych rozwiązań technologicznych, które automatyzują procesy biznesowe i podnoszą efektywność operacyjną.', 'Projektowanie i integracja systemów opartych na sztucznej inteligencji, usprawniających kluczowe obszary działalności, takie jak logistyka, transport czy zarządzanie danymi.', 'Monitorowanie globalnych trendów technologicznych i proponowanie wdrożeń, które wyznaczają nowe standardy w branży i pozwolą firmie na pozostanie liderem w innowacjach technologicznych', 'Tworzenie kompleksowej dokumentacji technicznej i prowadzenie szkoleń, aby wdrażane technologie były zrozumiałe i efektywnie wykorzystywane przez zespół.', 'Rozwój i optymalizacja środowiska pracy w Microsoft 365 (Teams, Outlook, OneDrive, Planner) w celu zwiększenia efektywności zespołów.', 'Projektowanie, wdrażanie i optymalizacja procesów automatyzacyjnych z wykorzystaniem platform takich jak Power Automate oraz make.com.', 'Aktywne promowanie nowoczesnych technologii w organizacji.', 'Wsparcie użytkowników i procesów z obszaru IT.']\n",
      "requirements: ['Wykształcenie wyższe w obszarze informatyki, inżynierii oprogramowania, technologii informacyjnych lub innych pokrewnych dziedzinach.', 'Minimum 3-letnie doświadczenie w realizacji projektów technologicznych, automatyzacji procesów lub wdrażaniu rozwiązań opartych na sztucznej inteligencji.', 'Znajomość i praktyczne doświadczenie w pracy z nowoczesnymi narzędziami AI z umiejętnością ich kreatywnego wykorzystania i wprowadzania do środowiska biznesowego.', 'Znajomość podstaw przynajmniej jednego z języków programowania (Python, Java, C# lub równoważny).', 'Silne umiejętności analityczne i kreatywne podejście do rozwiązywania problemów.', 'Umiejętność pracy w dynamicznym środowisku, przy jednoczesnym zachowaniu wysokiej jakości realizowanych projektów.', 'Mile widziane doświadczenie w branży logistycznej lub transportowej', 'Entuzjazm do innowacji, umiejętność samodzielnego zdobywania wiedzy i chęć bycia pionierem w wykorzystywaniu nowoczesnych technologii w biznesie.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/automation-ai-engineer-kajetany-pow-pruszkowski-klonowa-48,oferta,1003918525?apid=b8c0c8a5-9f27-4bfe-8e69-2075d27f9a9e&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 213 ---\n",
      "id: 266\n",
      "url: https://www.pracuj.pl/praca/senior-data-scientist-gtech-ads-gcs-partner-engineering-warszawa-emilii-plater-53,oferta,1003921410\n",
      "title: Senior Data Scientist - gTech Ads GCS Partner Engineering\n",
      "work_location: None\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Emilii Plater 53, Śródmieście, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['Python', 'R', 'SQL']\n",
      "responsibilities: ['Lead data science aspects of client engagements in the area of marketing portfolio management.', 'Collaborate with customers to solve their problems and identify the best statistical techniques that can solve the problem, and own the development of modeling framework.', 'Engage important stakeholders to assess data and model readiness, and be able to scale a proof-of-concept to a solution.', 'Work with customer and internal teams to translate data and model results into tactical and strategic insights for decision-making, and work with clients to integrate recommendations into business processes.', 'Collaborate with Product/Engineering teams to increase and optimize capabilities of the Applied Data Science team, and apply methods which create opportunities for scale, and help to drive innovation.']\n",
      "requirements: [\"Master's degree in a quantitative discipline such as Statistics, Engineering, Sciences, or equivalent practical experience.\", '4 years of experience using analytics to solve product or business problems, coding (e.g., Python, R, SQL), querying databases or statistical analysis, or a PhD degree.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-scientist-gtech-ads-gcs-partner-engineering-warszawa-emilii-plater-53,oferta,1003921410?apid=a331e057-3d9c-4da5-93a7-ba0c9a6e0042&oca=None&acv=0\n",
      "\n",
      "--- Job Record 214 ---\n",
      "id: 267\n",
      "url: https://www.pracuj.pl/praca/specjalista-specjalistka-ds-ai-poznan-swierzawska-1,oferta,1003927656\n",
      "title: Specjalista / Specjalistka ds. AI\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 27 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Świerzawska 1, Grunwald, PoznańPoznań, wielkopolskie']\n",
      "technologies: ['AI', 'wewnątrz organizacji']\n",
      "responsibilities: ['Budowanie kompetencji i wiedzy z zakresu AI w organizacji', 'Wsparcie pracowników w adaptacji do narzędzi AI i ich zastosowań', 'Optymalizacja procesów biznesowych poprzez AI i automatyzację', 'Zapewnianie zgodności wdrażanych rozwiązań z regulacjami prawnymi i etycznymi', 'Organizowanie szkoleń, webinarów i publikowanie poradników nt. AI', 'Monitorowanie trendów i nowinek technologicznych w branży']\n",
      "requirements: ['Wykształcenie wyższe z zakresu informatyki, matematyki, analizy danych lub pokrewnych', 'Minimum rok doświadczenia w pracy z projektami AI', 'Znajomość możliwości i ograniczeń AI oraz kluczowych pojęć', 'Znajomość języka angielskiego na poziomie B2 (materiały w języku angielskim)', 'Doświadczenie w analizie i optymalizacji procesów biznesowych', 'Doskonałe umiejętności komunikacyjne i zdolność do współpracy', 'Zdolność wyszukiwania, analizowania i weryfikowania informacji', 'Otwartość na zmiany i chęć do ciągłego rozwoju']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-specjalistka-ds-ai-poznan-swierzawska-1,oferta,1003927656?apid=c39be812-a9b9-42ca-bd3f-c241ce8ace46&oca=None&acv=0\n",
      "\n",
      "--- Job Record 215 ---\n",
      "id: 268\n",
      "url: https://www.pracuj.pl/praca/etl-developer-warszawa-dluga-29,oferta,1003888510\n",
      "title: ETL Developer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 10 dnido 08 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['ETL', 'Talend', 'SQL', 'Oracle', 'UNIX', 'Microsoft SQL Server', 'DBT', 'u klienta', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu']\n",
      "responsibilities: ['Tworzenie i rozwijanie procesów ETL za pomocą narzędzia Talend', 'Testowanie procesów ETL, aby zapewnić poprawności danych i ich zgodności z wymaganiami biznesowymi', 'Współpraca z zespołem w celu dostarczenia odpowiednich rozwiązań ETL', 'Rozwiązywanie występujących problemów związanych z błędami w procesach ETL', 'Prowadzenie dokumentacji technicznej']\n",
      "requirements: ['Co najmniej 2-3 lata doświadczenia w pracy jako ETL Developer', 'Dobra znajomość: ETL, Talend, SQL', 'Doświadczenie z: Oracle DB i/lub MSSQL Server', 'Podstawowa znajomość systemu UNIX', 'Język angielski (min B2)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/etl-developer-warszawa-dluga-29,oferta,1003888510?apid=d372ded3-60e8-4023-a454-9bc25e90edf9&oca=None&acv=0\n",
      "\n",
      "--- Job Record 216 ---\n",
      "id: 269\n",
      "url: https://www.pracuj.pl/praca/etl-developer-katowice-brynowska-72,oferta,1003927600\n",
      "title: ETL Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 27 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca stacjonarna, praca zdalna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Brynowska 72, KatowiceKatowice, śląskie']\n",
      "technologies: ['SQL Server Integration Services', 'Pentaho', 'ETL', 'Python', 'Java', 'C#', 'PowerBI', 'wewnątrz organizacji', 'koncentrujesz się na jednym projekcie']\n",
      "responsibilities: ['monitorowanie oraz utrzymywanie istniejących procesów ETL, identyfikowanie oraz rozwiązywanie problemów z obszaru Hurtowni Danych', 'współpraca z zespołami analitycznymi i biznesowymi w celu zrozumienia potrzeb dotyczących danych i przekształcania ich w sprawne procesy ETL', 'analiza, modelowanie i mapowanie danych pomiędzy różnymi systemami i bazami danych', 'tworzenie i utrzymywanie dokumentacji technicznej dla procesów ETL', 'udział w projektach wdrożeń nowych systemów, narzędzi i rozwiązań w zakresie integracji danych, w tym w międzynarodowym środowisku']\n",
      "requirements: ['min. 3 lata doświadczenia w pracy na podobnym stanowisku', 'znajomość narzędzi ETL (w szczególności SSIS, Pentaho) (warunek konieczny)', 'bardzo dobra znajomość SQL oraz baz danych (relacyjne i nierelacyjne)', 'doświadczenie w pracy z dużymi wolumenami danych oraz ich optymalizacji (warunek konieczny)', 'umiejętność analitycznego myślenia i rozwiązywania problemów', 'znajomość zagadnień związanych z hurtowniami danych i modelowaniem danych', 'komunikatywność i umiejętność współpracy w zespole', 'znajomość języka angielskiego pozwalająca na swobodną komunikację z zagranicznymi interesariuszami']\n",
      "application_link: https://www.pracuj.pl/aplikuj/etl-developer-katowice-brynowska-72,oferta,1003927600?apid=776f5776-70bb-4a57-aef3-370861a19c98&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 217 ---\n",
      "id: 270\n",
      "url: https://www.pracuj.pl/praca/machine-learning-engineer-in-samsung-ads-project-warszawa-plac-europejski-1,oferta,1003864084\n",
      "title: Machine Learning Engineer in Samsung Ads Project\n",
      "work_location: None\n",
      "validity: valid for 14 daysto 12 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['plac Europejski 1, Wola, WarszawaWarszawa, Masovian', 'contract of employment', 'More than one vacancy']\n",
      "technologies: ['TensorFlow', 'PyTorch', 'Spark', 'Python', 'AWS', 'Snowpark', 'Github Actions', 'Grafana', 'Airflow', 'Snowflake']\n",
      "responsibilities: ['Develop, test, deploy, and maintain data, scalable low-latency machine learning products and pipelines supporting ML products considering factors such as the nature of the data, the complexity of the problem, and the available computational resources.', \"Validate the model's performance on unseen data, ensuring that it generalizes well and does not overfit the training data. Conduct rigorous testing to identify and address potential issues, such as bias or fairness concerns.\", 'Design and develop the next generation machine learning platform to support thousands of model training pipelines concurrently and thousands of billions of daily batch predictions.', 'Research the latest machine learning platform technologies pushing the boundaries of what is currently possible with ML and keep up-to-date with industry trends and developments.', 'Experiment with new ML platforms tailored to our environment and create quick prototypes / proof-of-concepts.', 'Streamline model deployment, unit testing, integration testing, and stress testing and ensure engineering quality.', 'Support automation of the ML pipeline using CI/CD principles, promoting consistency, reproducibility, and agility.', 'Work with Data Scientists to introduce new ML platform features, help streamline the model development process, and reduce the lead time for model production.', 'Closely work with different internal ML teams (e.g., Data Scientists and MLOps teams) to improve codebase and product health.', 'Depending of your skills and experience you will have a chance to technically lead people']\n",
      "requirements: ['Degree in Computer Science or related field (Data Science, Big Data, Mathematics, Physics, etc.).', 'At least 2 years of proven industry experience in machine learning projects.', 'Strong programming skills in Python and data-science libraries (pandas, scikit-learn).', 'Working knowledge of SQL and relational databases, ideally Snowflake.', 'Expert knowledge of data analysis, statistics, data mining, machine learning or Big Data processing area.', 'Experience with mainstream ML libraries (TensorFlow/PyTorch, Scikit-Learn, PySpark etc.).', 'Familiarity with data structures, CI/CD pipelines, software engineering principles and robust code testing.', 'Collaborating in a team using code versioning tools like Git.', 'Ability to summarize and present the finding results to business/product stakeholders.', 'Openness to learn new technologies and adapt to technological stack.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/machine-learning-engineer-in-samsung-ads-project-warszawa-plac-europejski-1,oferta,1003864084?apid=062ded66-1bd0-445e-afb9-e04d82094052&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 218 ---\n",
      "id: 271\n",
      "url: https://www.pracuj.pl/praca/administrator-hurtowni-danych-warszawa-aleja-armii-ludowej-26,oferta,1003927414\n",
      "title: Administrator Hurtowni Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 27 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Aleja Armii Ludowej 26, Śródmieście, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Bash', 'SQL', 'Shell', 'SAS', 'Oracle', 'u klienta']\n",
      "responsibilities: ['Utrzymanie i rozwój procesów zasilania i raportowania Hurtowni Danych,', 'Nadzór nad procesami kontroli jakości danych i realizacja wdrożeń w obszarze Hurtowni Danych,', 'Zapewnienie wsparcia użytkownikom', 'Budowa narzędzi wspomagających pracę developerów Hurtowni Danych (SAS/SAS Viya/Oracle/inne technologie), udział w pracach optymalizujących']\n",
      "requirements: ['Masz wyższe wykształcenie z zakresu IT lub pokrewne (lub ostatnie lata studiów),', 'Posiadasz dobrą znajomość systemów operacyjnych klasy Unix/Linux,', 'Znasz języki programowania Shell (BASH/SH), SQL', 'Posiadasz praktyczną znajomość technologii SAS Institute (SAS Data Integration Studio, SAS Management Console, SAS Viya),', 'Posiadasz min. 3 lata doświadczenia zawodowego w wymienionych technologiach']\n",
      "application_link: https://www.pracuj.pl/aplikuj/administrator-hurtowni-danych-warszawa-aleja-armii-ludowej-26,oferta,1003927414?apid=11f2f8b4-6443-42b3-9fab-f3aba35620cb&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 219 ---\n",
      "id: 272\n",
      "url: https://www.pracuj.pl/praca/programista-programistka-jawornik-pow-myslenicki-360,oferta,1003894672\n",
      "title: Programista / Programistka\n",
      "work_location: None\n",
      "validity: ważna jeszcze 14 dnido 12 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Jawornik 360, Jawornik (pow. myślenicki)Jawornik (pow. myślenicki), małopolskie']\n",
      "technologies: ['SQL', 'Delphi', 'Affinity Designer']\n",
      "responsibilities: ['Tworzenie i rozwijanie oprogramowania w środowisku Delphi.', 'Utrzymanie i modernizację istniejących aplikacji opartych na Delphi.', 'Projektowanie i implementację nowych modułów oraz funkcjonalności.', 'Rozwiązywanie problemów technicznych oraz optymalizację wydajności aplikacji.', 'Współpracę z zespołem w celu realizacji projektów zgodnie z założeniami biznesowymi.']\n",
      "requirements: ['Minimum 1 rok doświadczenia w programowaniu w Delphi.', 'Znajomość bibliotek VCL oraz komponentów Delphi.', 'Doświadczenie w integracji z bazami danych (np. MySQL, MariaDB).', 'Dobra znajomość języka SQL i relacyjnych baz danych.', 'Umiejętność analitycznego myślenia oraz rozwiązywania problemów.', 'Umiejętność pracy w zespole, komunikatywność i samodzielność.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/programista-programistka-jawornik-pow-myslenicki-360,oferta,1003894672?apid=d530f294-6b19-486c-913e-7dfcf56b53bd&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 220 ---\n",
      "id: 273\n",
      "url: https://www.pracuj.pl/praca/programista-programistka-python-lomianki-kolejowa-44,oferta,1003908755\n",
      "title: Programista / Programistka Python\n",
      "work_location: None\n",
      "validity: ważna jeszcze 18 dnido 16 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kolejowa 44, ŁomiankiŁomianki, mazowieckie']\n",
      "technologies: ['Python', 'JavaScript', 'Visual Studio Code', 'Git', 'QT Framework']\n",
      "responsibilities: ['tworzenie podstawy komunikacji międzysystemowej oraz z urządzeniami systemowymi', 'rozwiązywanie bieżących problemów w kodzie oraz optymalizacja wydajności kodu', 'projektowanie i wdrażanie rozwiązań dla różnych problemów biznesowych związanych m.in. z obsługą klienta, efektywnością i optymalizacją procesów', 'współuczestniczenie w projektach z wykorzystaniem AI', 'dokumentowanie procesu tworzenia oprogramowania i utrzymanie aktualnej dokumentacji   projektowej', 'realizacja innowacyjnych projektów w obszarze inteligentnej automatyzacji procesów', 'programowanie rozwiązań w obiektowym Python’ie', 'określanie architektury rozwiązań']\n",
      "requirements: ['dobra znajomość języka Python z obsługą obiektów', 'znajomość systemu kontroli wersji GIT', 'znajomość języków webowych: HTML, CSS, JavaScript', 'znajomość języków systemowych skryptowych typu Shell', 'znajomość działania i tworzenia funkcji asynchronicznych', 'zdolność do samodzielnej analizy i rozwiązywania problemów', 'chęć nauki i rozwijania się w obszarze nowoczesnych technologii IT', 'znajomość języka angielskiego w stopniu umożliwiającym swobodne posługiwanie się dokumentacją techniczną w tym języku', 'wykształcenie wyższe techniczne', 'minimum 3-letnie doświadczenie w programowaniu rozwiązań w Python', 'znajomość zagadnień elektrotechnicznych', 'umiejętności analityczne, kreatywność i samodzielność w poszukiwaniu rozwiązań']\n",
      "application_link: https://www.pracuj.pl/aplikuj/programista-programistka-python-lomianki-kolejowa-44,oferta,1003908755?apid=5fc34b13-e26e-4fef-b2cf-c4f421795f69&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 221 ---\n",
      "id: 274\n",
      "url: https://www.pracuj.pl/praca/programista-programistka-javascript-lomianki-kolejowa-44,oferta,1003908752\n",
      "title: Programista / Programistka JavaScript\n",
      "work_location: None\n",
      "validity: ważna jeszcze 18 dnido 16 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Kolejowa 44, ŁomiankiŁomianki, mazowieckie']\n",
      "technologies: ['JavaScript', 'Python', 'HTML', 'Git', 'SQL', 'QT Framework']\n",
      "responsibilities: ['opracowanie algorytmów usprawniających działanie systemu', 'rozwijanie oprogramowania w JavieScript, HTML, CSS z komunikacją międzysystemową', 'naprawianie błędów', 'udział we wszystkich aspektach rozwoju oprogramowania, w tym analizie wykonalności wymagań, projektowaniu oraz implementacji oprogramowania zgodnie z założeniami architektonicznymi wypracowanymi w dotychczasowej realizacji systemu.', 'wyszukiwanie rozwiązań problemów oraz gotowego oprogramowania Open Source spełniającego potrzebne funkcje', 'pisanie i dokumentowanie kodu źródłowego', 'przeprowadzanie code review oraz refaktoryzacji kodu', 'usuwanie błędów w oprogramowaniu oraz optymalizacja oprogramowania pod kątem wydajności i dostępności.', 'dbanie o wysoką jakość wytwarzanych rozwiązań', 'zapewnianie spójności z elementami systemu tworzonymi przez innych członków zespołu', 'wyszukiwanie oraz zapoznawanie się z nowymi narzędziami i technologiami potrzebnymi do realizacji wymagań', 'tworzenie dokumentacji technicznej oraz dokumentacji kodu', 'współuczestniczenie w projektach z wykorzystaniem AI']\n",
      "requirements: ['wykształcenie wyższe techniczne', 'min. 3letnie doświadczenie w programowaniu w języku JavaScript lub pokrewnym TypeScript', 'doświadczenie w tworzeniu aplikacji webowych oraz ugruntowana wiedza z zakresu inżynierii oprogramowania', 'znajomość języka SQL oraz doświadczenie w projektowaniu, implementacji i optymalizacji relacyjnych baz danych', 'dobra znajomość języka Python z obsługą obiektów', 'znajomość systemu kontroli wersji GIT', 'znajomość działania i tworzenia funkcji asynchronicznych', 'znajomość zagadnień elektrotechnicznych', 'otwarty umysł, chęć nauki i rozwoju zawodowego', 'znajomość języka angielskiego w stopniu umożliwiającym swobodne posługiwanie się dokumentacją techniczną w tym języku']\n",
      "application_link: https://www.pracuj.pl/aplikuj/programista-programistka-javascript-lomianki-kolejowa-44,oferta,1003908752?apid=e85e6b41-a377-43d5-babc-791127c3747c&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 222 ---\n",
      "id: 275\n",
      "url: https://www.pracuj.pl/praca/big-data-engineer-in-samsung-ads-project-warszawa-plac-europejski-1,oferta,1003878790\n",
      "title: Big Data Engineer in Samsung Ads Project\n",
      "work_location: None\n",
      "validity: valid for 5 daysto 03 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['plac Europejski 1, Wola, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['Python', 'Scala', 'Spark', 'Kubernetes', 'Airflow', 'SQL', 'Snowflake Data Cloud', 'AWS', 'Java', 'Docker', 'Rest APIs', 'GoLang', 'GraphQL']\n",
      "responsibilities: ['Collaborate with data scientists, product managers, and other engineers to refine and improve data processing systems', 'Design and maintain scalable and optimized data pipelines for efficient collection, processing, and storage data', 'Collaborate with data scientists, product managers, and other engineers to refine and improve attribution methodologies', 'Participate in R&D projects in the area of Cloud Services', 'Establish and follow best coding practices', 'Work closely with stakeholders to understand and translate business requirements into technical solutions', 'Conduct A/B testing and performance analysis to validate and iterate on attribution models.', 'Ensure software quality: create and maintain unit, integration and functional tests, participate in code review', 'Report tasks progress']\n",
      "requirements: ['At least 5 years of commercial experience in developing Python, Java or Scala', 'Proficient in working with big data technologies (e.g., Hadoop, Spark and MapReduce)', 'Knowledge of database (SQL/NoSQL) technologies (Snowflake, PostgreSQL, MySQL, DynamoDB)', 'Experience in working with Kubernetes and stream data processing frameworks frameworks (Flink, Apache Ignite)', 'Experience in optimizing SQL queries', 'Experience in working with large datasets', 'Hands-on experience with orchestration tools like Airflow or similar', 'Experience in AWS', 'Knowledge of Design Patterns', 'Knowledge of version control system: Git', 'Proficiency in English, enough to communicate and understand technical documentation', 'University degree in Computer Sciences, Telecommunication and/ or similar']\n",
      "application_link: https://www.pracuj.pl/aplikuj/big-data-engineer-in-samsung-ads-project-warszawa-plac-europejski-1,oferta,1003878790?apid=b34e64d8-eeb8-4c18-9adb-a614c5573c22&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 223 ---\n",
      "id: 276\n",
      "url: https://www.pracuj.pl/praca/technical-research-associate-ai-katowice-zabrska-17,oferta,1003906271\n",
      "title: Technical Research Associate - AI\n",
      "work_location: None\n",
      "validity: valid for 17 daysto 15 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Zabrska 17, KatowiceKatowice, Silesian', 'contract of employment', 'full office work', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Microsoft Excel', 'Jira', 'Asana']\n",
      "responsibilities: ['Work collaboratively in a fast-paced environment', 'Execute different Use Cases collecting Data in support of AI engine', 'Conduct Data Analysis on how the AI performs comparatively to User data', 'Provide feedback on how to improve the AI', 'Create and Execute Prompts', 'Learn new software programs on the job', 'Providing supporting documentation when the AI fails']\n",
      "requirements: ['Excellent communication skills (written and oral)', 'Strong Organization and Time Management Skills', 'Experience in assisting in system troubleshooting & finding resolutions', 'Preferred prior work experience or college studies in AI', 'Technical aptitude in one or more of the following: spreadsheets (eg, Excel, Google Sheets), email/calendar (eg Outlook, Gmail), project management (eg, JIRA, Asana)', 'Ability to gain new skills and knowledge through hands-on experience', 'Keen eye for detail', 'Demonstrated ability to work independently', 'Strong time management skills', 'Exemplify the quality of doing \"get it done attitude,\" including a high level of accountability, transparency, and teamwork first & foremost']\n",
      "application_link: https://www.pracuj.pl/aplikuj/technical-research-associate-ai-katowice-zabrska-17,oferta,1003906271?apid=82549b0c-d1ff-4c18-add2-212df0b5729c&oca=None&acv=0\n",
      "\n",
      "--- Job Record 224 ---\n",
      "id: 277\n",
      "url: https://www.pracuj.pl/praca/mlops-expert-warszawa-rondo-ignacego-daszynskiego-1,oferta,1003920786\n",
      "title: MLOps Expert\n",
      "work_location: None\n",
      "validity: ważna jeszcze 24 dnido 22 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior), ekspert\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['rondo Ignacego Daszyńskiego 1, Wola, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Python', 'Kubernetes', 'MLflow', 'Kubeflow', 'Jenkins', 'ArgoCD', 'GitLab', 'Docker', 'OpenShift', 'Linux', 'Unix', 'Bash', 'Java', 'Groovy', 'wewnątrz organizacji', 'agile', 'waterfall']\n",
      "responsibilities: ['konfiguracja i rozwój narzędzi Dev Ops / ML Ops (np. MLflow, Kubeflow),', 'automatyzacja czynności i procesów wchodących w skład CI/CD (Jenkins, Argo CD)', 'utrzymywanie i rozbudowa istniejącej bazy kodu w języku Python 3', 'zarządzanie platformą kontenerową (Kubernetes, OKD, Openshift)', 'optymalizacja rozwiązań ML', 'współpraca z inżynierami oprogramowania, Dev Ops, analitykami, architektami rozwiązań', 'współpraca z Data Scientists i osobami tworzącymi modele ML', 'udział w projektach i wsparcie analityczne nowych rozwiązań', 'utrzymanie i definiowanie odpowiedniej dokumentacji (proceduralnej i technicznej)']\n",
      "requirements: ['doświadczenie zawodowe w sektorze ubezpieczeniowym lub finansowym', 'Java / Groovy']\n",
      "application_link: https://www.pracuj.pl/aplikuj/mlops-expert-warszawa-rondo-ignacego-daszynskiego-1,oferta,1003920786?apid=27a059be-d64c-41a6-a35f-f0d2ba0565c3&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 225 ---\n",
      "id: 278\n",
      "url: https://www.pracuj.pl/praca/ml-ai-engineer-warszawa-pulawska-180,oferta,1003905978\n",
      "title: ML/AI Engineer\n",
      "work_location: None\n",
      "validity: valid for 17 daysto 15 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Puławska 180, Mokotów, WarszawaWarszawa, Masovian', 'Запрошуємо працівників з України']\n",
      "technologies: ['Python', 'Azure', 'Databricks', 'PySpark', 'Scala', 'R', 'Java', 'Azure DevOps', 'GitHub', 'GitLab']\n",
      "responsibilities: ['Building high-performing, scalable, enterprise-grade ML/AI applications in cloud environment.', 'Working with Data Science, Data Engineering and Cloud teams to implement Machine Learning models into production.', 'Practical and innovative implementations of ML/AI automation, for scale and efficiency.', 'Design, delivery and management of industrialized processing pipelines.', 'Defining and implementing best practices in ML models life cycle and ML operations.', 'Implementing AI/MLOps frameworks and supporting Data Science teams in best practices.', 'Gathering and applying knowledge on modern techniques, tools and frameworks in the area of ML Architecture and Operations.', 'Gathering technical requirements & estimating planned work.', 'Presenting solutions, concepts and results to internal and external clients.', 'Being Technical Leader on ML projects, defining task, guidelines and evaluating results.', 'Creating technical documentation.', 'Supporting and growing junior engineers.']\n",
      "requirements: ['Good understanding of ML/AI concepts: types of algorithms, machine learning frameworks, model efficiency metrics, model life-cycle, AI architectures.', 'Good understanding of Cloud concepts and architectures as well as working knowledge with selected cloud services, preferably Azure.', 'Experience in programming ML algorithms and data processing pipelines using Python.', 'At least 4 years of experience in production ready code development.', 'Experience in designing and implementing data pipelines.', 'Practical experience with implementing ML solutions on Azure ML and/or Databricks.', 'Good communication skills.', 'Ability to work in team and support others.', 'Taking responsibility for tasks and deliverables.', 'Great problem-solving skills and critical thinking.', 'Fluency in written and spoken English.', 'Practical experience with other programming languages: PySpark, Scala, R, Java is a plus.', 'Practical experience with tools like AirFlow, ADF or Kubeflow is a plus.', 'Good understanding of CI/CD and DevOps concepts, and experience in working with selected tools (preferably GitHub Actions, GitLab or Azure DevOps) is a plus.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ml-ai-engineer-warszawa-pulawska-180,oferta,1003905978?apid=c93f7e17-abef-4dbb-ad1b-885e8c1aec43&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 226 ---\n",
      "id: 279\n",
      "url: https://www.pracuj.pl/praca/data-engineer-delivery-experience-warszawa-zelazna-51,oferta,1003888057\n",
      "title: Data Engineer - Delivery Experience\n",
      "work_location: None\n",
      "validity: valid for 9 daysto 07 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Żelazna 51, Wola, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['Google Cloud Platform', 'SQL', 'Python', 'PySpark', 'Airflow', 'Azure']\n",
      "responsibilities: ['You will be actively responsible for developing and maintaining processes for handling large volumes of data', 'You will be streamlining and developing the data architecture that powers analytical products and work along a team of experienced analysts', 'You will be monitoring and enhancing quality and integrity of the data', 'You will manage and optimize costs related to our data infrastructure and data processing on GCP']\n",
      "requirements: ['Have at least 3 years of experience as Data Engineer and working with large datasets', 'Have experience with cloud providers (GCP preferred)', 'Are highly proficient in SQL', 'Have strong understanding of data modeling and cloud DWH architecture', 'Have experience in designing and maintaining ETL/ELT processes', 'Are capable of optimizing cost and efficiency of data processing', 'Are proficient in Python for working with large data sets (using PySpark or Airflow)', 'Use good practices (clean code, code review, CI/CD)', 'Have a high degree of autonomy and take responsibility for developed solutions', 'Have English proficiency on at least B2 level', 'Like to share knowledge with other team members']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-delivery-experience-warszawa-zelazna-51,oferta,1003888057?apid=3ff6dba9-2e08-41c9-ae59-98e3881b18b0&oca=None&acv=0\n",
      "\n",
      "--- Job Record 227 ---\n",
      "id: 280\n",
      "url: https://www.pracuj.pl/praca/advanced-cloud-developer-gdansk,oferta,1003869460\n",
      "title: Advanced Cloud Developer\n",
      "work_location: None\n",
      "validity: valid for a dayto 27 February 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['GdańskGdańsk, Pomeranian', 'contract of employment']\n",
      "technologies: ['.NET', 'Kubernetes', 'Docker', 'MS SQL Server']\n",
      "responsibilities: ['Designing and implementing modules and layers of an enterprise-grade software system', 'Providing feasibility studies and high level estimations of engineering effort required to complete a project', 'Assisting and mentoring development engineers in enhancing solutions with cloud capabilities, QA engineers in designing and carrying-out tests', 'Analyzing and troubleshooting software problems reported by internal QA team or external customers', 'Participating in the full life cycle of development from specification and design through implementation, testing and support']\n",
      "requirements: ['Bachelor of Science in Computer Science, Software Engineering, or Computer Engineering', 'Minimum 5 years of experience as a Cloud Engineer or similar role', 'Proficiency in designing and developing .NET/Cloud systems', 'Strong experience in working with cloud platforms (AWS/Azure)', 'Strong experience in containerization and orchestration platforms (Kubernetes, Docker, Podman, Buildah)', 'Experience in working with relational databases (MS SQL Server)', 'Experience in Cloud Migration desired, DevOps, Microservices', 'Strong understanding of software systems development and system architecture principles']\n",
      "application_link: https://www.pracuj.pl/aplikuj/advanced-cloud-developer-gdansk,oferta,1003869460?apid=fd9c58eb-0465-486a-be57-620342620897&oca=None&acv=0\n",
      "\n",
      "--- Job Record 228 ---\n",
      "id: 281\n",
      "url: https://www.pracuj.pl/praca/it-data-business-solutions-analyst-commercial-sales-warszawa-bobrowiecka-8,oferta,1003917837\n",
      "title: IT & DATA Business Solutions Analyst Commercial Sales\n",
      "work_location: None\n",
      "validity: valid for 23 daysto 21 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Bobrowiecka 8, Mokotów, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['Salesforce']\n",
      "responsibilities: ['Provide expert technical support and consultancy for CRM solutions, with a strong emphasis on SalesForce.', 'Manage daily operations of applications to ensure availability, effectiveness, and alignment with business requirements.', 'Offer functional user support based on internal SLAs, KPIs, and global IT&Data Governance standards.', 'Participate in or lead projects in close collaboration with business teams.', 'Continuously develop your knowledge of business processes and applications, ensuring thorough application documentation and user ownership.', 'Contribute to the development and alignment of IT&Data corporate standards.', 'Engage in supplier negotiations and work to measure and improve supplier performance.']\n",
      "requirements: ['A passion for IT solutions and a keen interest in how they transform business processes.', 'At least one year of experience in CRM implementation and support, with a strong focus on SalesForce.', 'Experience in an international environment, allowing you to navigate diverse business contexts effectively.', 'A curious and self-driven personality, with a strong openness to new ideas and creativity.', 'Well-developed communication, teamwork, and self-organization skills.', 'Fluency in English, both spoken and written.', 'Fluent Polish is a must for this role.', 'Project management experience, which is highly desirable and considered a significant asset.', 'A background in the medical business is also an advantage.', \"If you are a dynamic individual with a strong technical background and the ability to drive CRM solutions in line with Danone's global strategy, we would love to hear from you. Join us as an IT&Data Business Solutions Analyst and contribute to the evolution of our IT landscape, exceeding business expectations.\"]\n",
      "application_link: https://www.pracuj.pl/aplikuj/it-data-business-solutions-analyst-commercial-sales-warszawa-bobrowiecka-8,oferta,1003917837?apid=c4c87736-13ff-4993-b496-73ac58be0745&oca=None&acv=0\n",
      "\n",
      "--- Job Record 229 ---\n",
      "id: 282\n",
      "url: https://www.pracuj.pl/praca/machine-learning-ops-engineer-gdynia-luzycka-8c,oferta,1003923577\n",
      "title: Machine Learning Ops Engineer\n",
      "work_location: None\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: full office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Łużycka 8c, Mały Kack, GdyniaGdynia, Pomeranian', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['Python', 'Bash', 'Jenkins', 'GitLab CI', 'ArgoCD', 'Docker', 'Kubernetes', 'Kubeflow', 'MLflow', 'AWS', 'Azure', 'Google Cloud Platform', 'Prometheus', 'Grafana', 'ELK Stack', 'Go', 'Rust', 'Apache Airflow', 'Databricks', 'Kafka', 'Spark', 'TensorFlow Serving', 'Triton Inference Server', 'in house']\n",
      "responsibilities: ['Deploy, monitor, and manage machine learning models in production using MLOps best practices.', 'Automate model retraining, versioning, and deployment pipelines using CI/CD workflows.', 'Ensure scalability, reliability, and reproducibility of ML models in cloud or on-prem environments.', 'Design and implement end-to-end ML pipelines, including data ingestion, preprocessing, training, and inference.', 'Optimize and maintain data pipelines for feature engineering and model retraining.', 'Use tools like Airflow, Kubeflow, MLflow, or SageMaker Pipelines to orchestrate workflows.', 'Deploy ML workloads on AWS, leveraging services like SageMaker, Databricks, Kubernetes, and Vertex AI.', 'Optimize cloud resource utilization, cost management, and performance.', 'Implement containerization and orchestration using Docker, Kubernetes, and AWS Fargate.', 'Set up real-time model monitoring, logging, and alerting for performance tracking.', 'Implement model drift detection and automate retraining strategies.', 'Ensure models meet latency, throughput, and accuracy requirements.', 'Enforce data governance, security, and access control policies for ML models and data pipelines.', 'Ensure compliance with GDPR, HIPAA, and other industry regulations.', 'Implement authentication and encryption mechanisms for data and model security.', 'Work closely with data scientists, ML engineers, software engineers, and cloud architects.', 'Collaborate with DevOps teams to integrate ML workloads into broader CI/CD pipelines.', 'Participate in Agile workflows (sprint planning, stand-ups, retrospectives).']\n",
      "requirements: ['Bachelor’s or Master’s degree in Computer Science, Data Science, Machine Learning, or a related field.', '4+ years of experience in MLOps, DevOps for ML, or Cloud-based ML Engineering.', 'Strong programming skills in Python and Bash (knowledge of Go or Rust is a plus).', 'Hands-on experience with CI/CD tools (Jenkins, GitLab CI, ArgoCD).', 'Experience with containerization (Docker) and orchestration (Kubernetes, Kubeflow, MLflow).', 'Strong knowledge of cloud platforms (AWS, Azure, GCP) and ML services (SageMaker, Vertex AI).', 'Familiarity with monitoring and logging tools (Prometheus, Grafana, ELK Stack).', 'Understanding of machine learning workflows, model lifecycle management, and data pipelines.', 'B2+ English proficiency, with strong documentation and communication skills.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/machine-learning-ops-engineer-gdynia-luzycka-8c,oferta,1003923577?apid=7fc702bb-6fa1-4702-9bc2-65ff8a7e5fc5&oca=None&acv=0\n",
      "\n",
      "--- Job Record 230 ---\n",
      "id: 283\n",
      "url: https://www.pracuj.pl/praca/machine-learning-engineer-gdynia-luzycka-8c,oferta,1003923574\n",
      "title: Machine Learning Engineer\n",
      "work_location: None\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: full office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Łużycka 8c, Mały Kack, GdyniaGdynia, Pomeranian', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['Python', 'Git', 'AWS', 'SageMaker', 'Lambda', 'Step Functions', 'Glue', 'Redshift', 'Athena', 'QuickSight', 'Tableau', 'Kibana', 'Docker', 'Kubernetes', 'in house']\n",
      "responsibilities: ['Develop and help in optimizing ML models for predictive analytics, anomaly detection, and AI-driven insights.', 'Assist in designing and prototyping of a scalable ML pipelines for data preprocessing, feature engineering, training, and inference.', 'Deploy, test and monitor ML models in production, ensuring performance, scalability, and reliability.', 'Leverage cloud services (AWS) for distributed training, model hosting, and data storage.', 'Collaborate with cross-functional teams, including data scientists, software engineers, and product owners.', 'Optimize data pipelines and infrastructure to support high-performance ML workloads.', 'Ensure compliance with MLOps best practices, including CI/CD, model versioning, and monitoring.', 'Stay up to date with the latest advancements in AI/ML, deep learning, and cloud-based ML solutions.']\n",
      "requirements: ['Master of Science in Engineering in Computer Science, Computer Engineering, Software Engineering, Computer Information Science, Electrical Engineering, or equivalent.', 'At least 4 years of active professional experience in software development.', '2+ years of experience in AI/ML, data intelligence, or cloud-based solutions.', 'Experience in Python.', 'Experience in other scripting languages is a plus.', 'Strong understanding of machine learning workflows, data processing, and AI-driven insights.', 'Hands-on experience with version control (Git).', 'Experience working in SAFe/Agile environments (Scrum/Kanban) and collaborating with Product Owners and cross-functional teams.', 'Good problem-solving, debugging, and performance optimization skills.', 'B2+ English proficiency, with strong communication and teamwork abilities.', 'Experience working across time zones and cultures.', 'Strong ownership attitude.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/machine-learning-engineer-gdynia-luzycka-8c,oferta,1003923574?apid=2c886d59-d07c-4fba-8a4d-d628f29763d7&oca=None&acv=0\n",
      "\n",
      "--- Job Record 231 ---\n",
      "id: 284\n",
      "url: https://www.pracuj.pl/praca/principal-machine-learning-engineer-gdynia-luzycka-8c,oferta,1003923575\n",
      "title: Principal Machine Learning Engineer\n",
      "work_location: Company locationŁużycka 8c, Mały Kack, GdyniaGdynia, Pomeranian\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: full office work, home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['Python', 'TypeScript', 'C++', 'TensorFlow', 'PyTorch', 'Scikit-learn', 'Apache Spark', 'Kafka', 'Flink', 'Elasticsearch', 'AWS', 'Ansible', 'Terraform', 'Cisco', 'Juniper Apstra', 'in house']\n",
      "responsibilities: ['Design and develop AI/ML models for network traffic analysis, anomaly detection, and predictive maintenance.', 'Implement real-time analytics for detecting cyber threats, QoS deviations, and performance bottlenecks.', 'Optimize ML models for low-latency, high-throughput processing in network environments.', 'Build data pipelines for structured and unstructured networking data, including logs, telemetry, and packet flows.', 'Engineer network-specific features to improve model accuracy and explainability.', 'Work with big data platforms (Spark, Databricks, Kafka, Redshift) for scalable data processing.', 'Deploy ML models in cloud (AWS) and edge computing environments for real-time processing.', 'Optimize AI-driven SDN (Software-Defined Networking) and NFV (Network Function Virtualization) solutions.', 'Integrate ML pipelines with network monitoring tools (Wireshark, Prometheus, NetFlow, SNMP, OpenTelemetry).', 'Stay up to date with emerging AI/ML applications in networking (5G, Wi-Fi 6/7, SD-WAN, intent-based networking).', 'Explore and implement new AI-driven approaches for network automation and self-healing networks.', 'Collaborate with industry leaders, standards organizations (IEEE, IETF, BBF), and open-source communities.', 'Optimize data pipelines and infrastructure to support high-performance ML workloads.', 'Work closely with network engineers, cybersecurity teams, and cloud architects to develop ML-driven solutions.', 'Provide technical guidance to product teams, ensuring ML solutions align with business objectives and network constraints.', 'Contribute to technical documentation, whitepapers, and industry presentations on AI/ML in networking.']\n",
      "requirements: ['Master’s or PhD in Computer Science, Machine Learning, Networking, or a related field.', '8+ years of experience in AI/ML development, with a focus on network analytics, automation, or cybersecurity.', 'Strong programming skills like Python, TypeScript, C++ with experience in ML frameworks (TensorFlow, PyTorch, Scikit-learn).', 'Expertise in networking technologies, including TCP/IP, SDN, NFV, BGP, MPLS, Wi-Fi, and network security protocols.', 'Experience with big data processing and real-time analytics (Apache Spark, Kafka, Flink, Elasticsearch).', 'Hands-on experience with AWS ML services.', 'Familiarity with network monitoring and performance management tools (Wireshark, SNMP, NetFlow, OpenTelemetry).', 'Strong problem-solving skills and ability to work with cross-functional teams.', 'C1+ English proficiency, with excellent communication and technical documentation skills.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/principal-machine-learning-engineer-gdynia-luzycka-8c,oferta,1003923575?apid=891cb7e9-658b-4f07-8c80-366efe56fa4c&oca=None&acv=0\n",
      "\n",
      "--- Job Record 232 ---\n",
      "id: 285\n",
      "url: https://www.pracuj.pl/praca/senior-machine-learning-engineer-gdynia-luzycka-8c,oferta,1003923573\n",
      "title: Senior Machine Learning Engineer\n",
      "work_location: Company locationŁużycka 8c, Mały Kack, GdyniaGdynia, Pomeranian\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: full office work, home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['Python', 'AWS', 'SageMaker', 'Lambda', 'Step Functions', 'Glue', 'Redshift', 'Athena', 'Git', 'QuickSight', 'Tableau', 'Kibana', 'Docker', 'Kubernetes', 'AWS Fargate', 'in house']\n",
      "responsibilities: ['Develop and optimize ML models for predictive analytics, anomaly detection, and AI-driven insights.', 'Design and prototype scalable ML pipelines for data preprocessing, feature engineering, training, and inference.', 'Deploy, test and monitor ML models in production, ensuring performance, scalability, and reliability.', 'Leverage cloud services (AWS) for distributed training, model hosting, and data storage.', 'Collaborate with cross-functional teams, including data scientists, software engineers, and product owners.', 'Optimize data pipelines and infrastructure to support high-performance ML workloads.', 'Ensure compliance with MLOps best practices, including CI/CD, model versioning, and monitoring.', 'Stay up to date with the latest advancements in AI/ML, deep learning, and cloud-based ML solutions.']\n",
      "requirements: ['Master of Science in Engineering in Computer Science, Computer Engineering, Software Engineering, Computer Information Science, Electrical Engineering, or equivalent.', 'At least 8 years of active professional experience in software development.', '3+ years of experience in AI/ML, data intelligence, or cloud-based solutions.', 'Proficiency in Python, with experience in developing scalable, cloud-native applications.', 'Experience in other scripting languages is a plus.', 'Strong experience with AWS cloud services, including SageMaker, Lambda, Step Functions, Glue, Redshift, and Athena.', 'Experience in Databricks is a plus.', 'Experience developing microservices and serverless applications.', 'Strong understanding of machine learning workflows, data processing, and AI-driven insights.', 'Hands-on experience with CI/CD pipelines and version control (Git).', 'Experience working in SAFe/Agile environments (Scrum/Kanban) and collaborating with Product Owners and cross-functional teams.', 'Excellent problem-solving, debugging, and performance optimization skills.', 'B2+ English proficiency, with strong communication and teamwork abilities.', 'Experience working across time zones and cultures.', 'Strong ownership attitude.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-machine-learning-engineer-gdynia-luzycka-8c,oferta,1003923573?apid=0985e697-26f7-4434-8931-a866219e30f1&oca=None&acv=0\n",
      "\n",
      "--- Job Record 233 ---\n",
      "id: 286\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-business-intelligence-k-m-warszawa-postepu-15,oferta,1003908599\n",
      "title: Specjalista ds. Business Intelligence (k/m)\n",
      "work_location: None\n",
      "validity: ważna jeszcze 18 dnido 16 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Postępu 15, Mokotów, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL']\n",
      "responsibilities: ['projektowanie, tworzenie i utrzymywanie modeli danych oraz raportów w Power BI,', 'transformacja danych z różnych źródeł z użyciem Power Query,', 'tworzenie miar biznesowych w DAX,', 'projektowanie warstwy wizualnej dla raportów w Power BI,', 'tworzenie zapytań do Hurtowni Danych z użyciem SQL,', 'budowa i utrzymywanie raportów w środowisku Reporting Services,', 'zbieranie wymagań biznesowych na dane dostępne w Hurtowni Danych, udział w procesie budowy nowych modeli,', 'tworzenie i utrzymywanie dokumentacji modeli i raportów.']\n",
      "requirements: ['praktyczna znajomość Power BI / DAX / Power Query,', 'wiedza i doświadczenie w zakresie koncepcji Business Intelligence,', 'praktyczna znajomość zasad wydajnego budowania modeli danych,', 'znajomość najlepszych praktyk w zakresie wizualizacji danych oraz dbałość o szczegóły i estetykę,', 'praktyczna znajomość MS SQL Server,', 'umiejętność łączenia danych z różnych źródeł i wyciągania z nich wniosków,']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-business-intelligence-k-m-warszawa-postepu-15,oferta,1003908599?apid=f9bbe238-dc6c-489f-9ae8-8b6ba68b4f22&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 234 ---\n",
      "id: 287\n",
      "url: https://www.pracuj.pl/praca/data-engineer-warszawa,oferta,1003874847\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 3 dnido 01 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL', 'Python', 'u klienta', 'branżowe platformy e-learningowe']\n",
      "responsibilities: ['uczestnictwo w projekcie dot. branży bankowej. Wykorzystywany stos technologiczny: Python, PySpark, Scala, Hadoop, SQL,', 'projektowanie architektury i wdrażanie nowoczesnych systemów przetwarzania danych w obszarze Big Data, w tym systemów wykorzystujących analitykę strumieniową i uczenie maszynowe,', 'zapewnienie wysokiej jakości procesu rozwoju i wdrożenia,', 'tworzenie dokumentacji technicznej,', 'badanie i wdrażanie nowych rozwiązań (R&D),', 'współpraca z innymi zespołami w celu wsparcia developmentu,', 'praca w modelu hybrydowym: 2 razy w miesiącu praca z biura w Warszawie,']\n",
      "requirements: ['masz minimum 2 lata doświadczenia w branży IT,', 'masz minimum rok doświadczenia w programowaniu procesów Spark w Pythonie lub Scali,', 'znasz koncepcję Big Data, Hurtowni Danych i Zarządzania Danymi,', 'masz doświadczenie w pracy z Hadoop,', 'znasz różne formaty danych: JSON, PARQUET, ORC, AVRO,', 'pracujesz z bazami danych i systemami przetwarzania danych, np. hive, kudu, hbase,', 'znasz SQL,', 'masz doświadczenie w integracji danych z wielu źródeł,', 'komunikujesz się w języku angielskim,']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-warszawa,oferta,1003874847?apid=40e23025-37a1-44ea-8e14-3a4022ccd4b2&oca=None&acv=0\n",
      "\n",
      "--- Job Record 235 ---\n",
      "id: 288\n",
      "url: https://www.pracuj.pl/praca/data-engineer-azure-warszawa,oferta,1003890378\n",
      "title: Data Engineer (Azure)\n",
      "work_location: None\n",
      "validity: ważna jeszcze 10 dnido 08 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL', 'Python', 'u klienta', 'wymiana wiedzy technicznej w firmie']\n",
      "responsibilities: ['Tworzenie, optymalizacja i wdrażanie procesów ETL oraz modeli danych w środowisku chmurowym.', 'Weryfikacja jakości i integralności danych, testowanie oraz optymalizacja wydajności procesów.', 'Projektowanie i implementacja rozwiązań do migracji danych z systemów lokalnych do chmury.', 'Aktywny udział w optymalizacji i automatyzacji przepływów danych.', 'Współpraca z zespołami biznesowymi i IT.']\n",
      "requirements: ['Minimum 4-letnie doświadczenia w pracy z danymi, w tym ETL, DWH i Data Lake.', 'Mile widziane doświadczenie w migracji systemów on-premise do chmury, szczególnie z Databricks.', 'Znajomość SQL i Pythona w zakresie przetwarzania, analizy i automatyzacji danych.', 'Doświadczenie w pracy z dużymi zbiorami danych oraz ich modelowaniu.', 'Znajomość języka angielskiego (B2).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-azure-warszawa,oferta,1003890378?apid=a0b3350a-776c-45d2-8d83-08bb70d776ea&oca=None&acv=0\n",
      "\n",
      "--- Job Record 236 ---\n",
      "id: 289\n",
      "url: https://www.pracuj.pl/praca/big-data-engineer-warszawa,oferta,1003874799\n",
      "title: Big Data Engineer\n",
      "work_location: Company locationWarszawaWarszawa, Masovian\n",
      "validity: valid for 3 daysto 01 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['Python', 'Apache Spark', 'SQL', 'Microsoft Azure', 'Apache Kafka', 'Apache Flink', 'HDFS', 'NoSQL', 'Hive', 'HBase', 'Cassandra', 'ElasticSearch', 'Scala', 'agile', 'scrum']\n",
      "responsibilities: ['Building pipelines for processing big amounts of medical data', 'Participating in system design, development and maintenance', 'Working closely with the data analysts to identify and provide the required data', 'Contributing to the definition and adoption of technical standards', 'Creating comprehensive automated unit and integration tests']\n",
      "requirements: ['5+ years of experience with Big Data Stream processing (Python, Apache Spark)', 'Knowledge of SQL and transactional databases', 'Previous experience with Azure', 'Experience with one or more of Apache Spark, Apache Kafka, Apache Flink, HDFS', 'Knowledge of the NoSQL database such as Hive, HBase, Cassandra, ElasticSearch or others', 'Bachelor’s Degree in Computer Science or another related field', 'Familiarity with Agile methodologies']\n",
      "application_link: https://www.pracuj.pl/aplikuj/big-data-engineer-warszawa,oferta,1003874799?apid=98b6df00-fdd4-4e26-8bfb-4d6e7b90e517&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 237 ---\n",
      "id: 290\n",
      "url: https://www.pracuj.pl/praca/platform-engineer-support-team-katowice,oferta,1003866640\n",
      "title: Platform Engineer – Support Team\n",
      "work_location: None\n",
      "validity: None\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['KatowiceKatowice, śląskie', 'ważna 6 godzin']\n",
      "technologies: ['Kubernetes']\n",
      "responsibilities: ['Manage and troubleshoot Kubernetes clusters across on-premises and cloud environments, ensuring smooth daily operations.', 'Support the deployment and upkeep of GPU workloads on NVIDIA hardware using Kubernetes.', 'Assist in maintaining infrastructure automation and implementing Infrastructure as Code (IaC) principles.', 'Collaborate with internal teams to implement solutions that are stable and scalable.', 'Act as a primary point of contact for Kubernetes support, helping to resolve issues and manage vendor and stakeholder relationships.', 'Build and maintain monitoring solutions to ensure the health of Kubernetes clusters, with tools like Prometheus or similar.', 'Implement automation tools such as ArgoCD, HELM, and Terraform to streamline deployment and management processes.', 'Provide training and documentation to support teams, ensuring a smooth knowledge transfer and reliable operations.']\n",
      "requirements: ['Experience with Kubernetes in on-premises and cloud settings, comfortable managing and troubleshooting clusters.', 'Familiarity with AWS and cloud-native services.', 'Working knowledge of IaC tools like Terraform to assist with infrastructure management.', 'Basic understanding of monitoring tools and techniques, ensuring infrastructure health.', 'Experience in Linux systems and their role in infrastructure management.', 'Excellent troubleshooting skills in a distributed, service-oriented environment.', 'Strong communication skills, with a focus on customer service and collaboration.', 'An eagerness to learn and grow in the role, with potential to advance into more complex project-based work.', 'Preferred qualifications:', 'Familiarity with ArgoCD, HELM, and other Kubernetes-native tools.', 'Experience with Single Sign-On (SSO) and cloud integration services.', 'Exposure to CI/CD processes and modern deployment practices.', 'Comfortable working with relational and non-relational databases.', 'Interest in learning about Windows Server automation and related tools.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/platform-engineer-support-team-katowice,oferta,1003866640?apid=d650f942-9719-4f14-9fe3-77fc9093b3dd&oca=None&acv=0\n",
      "\n",
      "--- Job Record 238 ---\n",
      "id: 291\n",
      "url: https://www.pracuj.pl/praca/fullstack-developer-react-native-next-js-python-ai-healthtech-gdynia-aleja-zwyciestwa-96-98,oferta,1003926981\n",
      "title: Fullstack Developer (React Native, Next.js, Python) – AI HealthTech\n",
      "work_location: Company locationAleja Zwycięstwa 96/98, Redłowo, GdyniaGdynia, Pomeranian\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['React Native', 'Next.js', 'Python', 'agile', 'scrum']\n",
      "responsibilities: ['Develop and maintain the web application (Next.js) and mobile app (React Native)', 'Build and optimize the backend in Python', 'Integrate and work with LLMs (or at least manage their integration)', 'Collaborate closely with the project manager and designer']\n",
      "requirements: ['Knowledge of LLMs (Large Language Models) or experience integrating them', 'Experience with React Native, Next.js, and Python (at least 5 years)', 'Availability to work in a US time zone', 'Fluent in English due to the international team']\n",
      "application_link: https://www.pracuj.pl/aplikuj/fullstack-developer-react-native-next-js-python-ai-healthtech-gdynia-aleja-zwyciestwa-96-98,oferta,1003926981?apid=46e6be65-73f1-48fa-b820-b68aad82f7b9&oca=None&acv=0\n",
      "\n",
      "--- Job Record 239 ---\n",
      "id: 292\n",
      "url: https://www.pracuj.pl/praca/big-data-architect-warszawa,oferta,1003912125\n",
      "title: Big Data Architect\n",
      "work_location: None\n",
      "validity: ważna jeszcze 21 dnido 19 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['u klienta', 'wymiana wiedzy technicznej w firmie']\n",
      "responsibilities: ['Współpraca z zespołami biznesowymi i IT w celu identyfikacji wymagań dotyczących danych i opracowywania rozwiązań spełniających te potrzeby.', 'Implementacja i zarządzanie rozwiązaniami Big Data oraz hurtowniami danych, optymalizując je pod kątem wydajności i skalowalności.', 'Projektowanie i rozwój kompleksowych architektur danych, uwzględniających potrzeby biznesowe i technologiczne organizacji.', 'Tworzenie i utrzymywanie modeli danych, zapewniając ich spójność, skalowalność i wydajność w całym przedsiębiorstwie.', 'Nadzorowanie procesów integracji danych, w tym ETL (Extract, Transform, Load) i zapewnianie jakości danych.', 'Zarządzanie bezpieczeństwem danych i zapewnianie zgodności z regulacjami branżowymi i prawnymi.']\n",
      "requirements: ['Minimum 5 lat komercyjnego doświadczenia na stanowisku Big Data Architect lub podobnym.', 'Znajomość zagadnień związanych z projektowaniem i wdrażaniem złożonych systemów IT wraz z integracją.', 'Doświadczenie w opracowywaniu modeli oraz analiz architektonicznych, tworzeniu architektury biznesowej.', 'Znajomość branży energetycznej potwierdzonej co najmniej 2 letnim doświadczeniem - must have.', 'Doświadczenie projektowe zdobyte w projektach IT (doradczych lub wdrożeniowych).', 'Znajomość rozwiązań i technologii w obszarze Big Data, Data Science, DWH, ETL.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/big-data-architect-warszawa,oferta,1003912125?apid=6b1bb6e4-2707-4f16-bd31-965fb41ebfc9&oca=None&acv=0\n",
      "\n",
      "--- Job Record 240 ---\n",
      "id: 293\n",
      "url: https://www.pracuj.pl/praca/administrator-postgresql-gdansk,oferta,1003926957\n",
      "title: Administrator PostgreSQL\n",
      "work_location: None\n",
      "validity: ważna jeszcze 13 dnido 11 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['GdańskGdańsk, pomorskie']\n",
      "technologies: ['PostgreSQL', 'SQL', 'Oracle', 'Bash', 'Python', 'Git', 'Ansible', 'Terraform']\n",
      "responsibilities: ['Zarządzanie i optymalizacja baz danych PostgreSQL w środowiskach Linux i Windows', 'Monitorowanie wydajności i rozwiązywanie problemów z bazami danych', 'Optymalizacja zapytań i projektowanie schematów baz danych', 'Zarządzanie replikacją, klastrami i partycjonowaniem w dużych środowiskach PostgreSQL', 'Konfiguracja wysokiej dostępności (HA), strategie backupui odzyskiwania danych', 'Zapewnienie zgodności z wymaganiami RPO/RTO', 'Wdrażanie i audytowanie zabezpieczeń baz danych oraz przeprowadzanie ocen podatności na zagrożenia', 'Zarządzanie dostępem użytkowników i uprawnieniami', 'Tworzenie skryptów automatyzujących zadania administracyjne (np. w Bash, Python)', 'Integracja z procesami CI/CD, automatyzacja z wykorzystywaniem Terraform i Ansible', 'Proaktywne monitorowanie baz danych przy użyciu narzędzi takich jak Grafana', 'Analiza i eliminacja wąskich gardeł wydajności, szybkie reagowanie na incydenty', 'Planowanie i przeprowadzanie migracji baz danych, w tym do chmury (np. Amazon RDS)', 'Wykonywanie aktualizacji, poprawek i migracji baz danych', 'Wspieranie zespołów deweloperskich w kwestiach związanych z bazami danych oraz współpraca z zespołami DevOps i infrastruktury', 'Kontakt z klientami, analiza potrzeb i prezentacja rekomendowanych rozwiązań', 'Tworzenie i utrzymywanie dokumentacji baz danych', 'Szkolenie innych członków zespołu w zakresie administracji PostgreSQL']\n",
      "requirements: ['Minimum 5 lat doświadczenia w administracji bazami danych PostgreSQL w środowisku produkcyjnym', 'Zaawansowana znajomość architektury PostgreSQL', 'Umiejętność projektowania, wdrażania i zarządzania klastrami PostgreSQL o wysokiej dostępności (HA) \\x0bi odporności na awarie (DR)', 'Doświadczenie w optymalizacji wydajności baz danych, w tym tuningu zapytań SQL i konfiguracji serwera', 'Biegłość w administracji systemami Linux, w tym zarządzanie serwerami, skrypty powłoki i automatyzacja zadań', 'Praktyczna znajomość narzędzi do monitorowania i analizy wydajności baz danych (np. pgBadger, pg_stat_statements)', 'Doświadczenie w planowaniu i przeprowadzaniu migracji danych, szczególnie z innych systemów bazodanowych (np. Oracle, MS SQL) do PostgreSQL', 'Umiejętność tworzenia i zarządzania skryptami do automatyzacji zadań administracyjnych (np. w Bash, Python)', 'Znajomość najlepszych praktyk w zakresie bezpieczeństwa baz danych, w tym zarządzanie dostępem, szyfrowanie danych i audyt', 'Doświadczenie w pracy z systemami kontroli wersji (np. Git) i narzędziami CI/CD', 'Znajomość narzędzi do automatyzacji infrastruktury (np. Ansible, Terraform)', 'Doświadczenie w pracy z bazami danych w środowisku chmurowym (np. Amazon RDS dla PostgreSQL)', 'Umiejętność analizy i rozwiązywania złożonych problemów z bazami danych', 'Znajomość podstaw programowania, szczególnie w kontekście tworzenia funkcji i procedur składowanych \\x0bw PostgreSQL', 'Wykształcenie wyższe', 'Skuteczność w działaniu', 'Odpowiedzialność biznesowa', 'Nastawienie na ciągły rozwój oraz czerpanie z różnych perspektyw', 'Kreowanie innowacji']\n",
      "application_link: https://www.pracuj.pl/aplikuj/administrator-postgresql-gdansk,oferta,1003926957?apid=b1d2c2c2-1102-41cf-a65c-8f4ff3871103&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 241 ---\n",
      "id: 294\n",
      "url: https://www.pracuj.pl/praca/analityk-systemowy-hurtownie-danych-warszawa,oferta,1003926929\n",
      "title: Analityk Systemowy (hurtownie danych)\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: None\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'praca stacjonarna']\n",
      "technologies: ['SQL', 'UML', 'Enterprise Architect', 'scrum']\n",
      "responsibilities: ['Identyfikacja i analiza źródeł danych, ich struktury oraz przepływów w ramach hurtowni danych,', 'Projektowanie i optymalizacja procesów Data Quality,', 'Analiza systemowa warstwy logicznej i fizycznej modelu danych przy użyciu notacji UML,', 'Projektowanie i rekomendowanie zmian w procesach ETL w celu efektywnego zasilania struktur danych na potrzeby raportowania i zaawansowanej analityki,', 'Tworzenie i aktualizowanie dokumentacji systemowej,', 'Konsultowanie i proponowanie rozwiązań systemowych we współpracy ze zleceniodawcami oraz zespołem realizacyjnym,', 'Wsparcie analityczne na etapie projektowania, implementacji i testowania systemu.']\n",
      "requirements: ['Minimum 3-letnie doświadczenie na podobnym stanowisku,', 'Doświadczenie w pracy nad projektami dotyczącymi hurtowni danych,', 'Wiedza i praktyka w analizie oraz projektowaniu przekształceń danych w przepływach między warstwami hurtowni danych,', 'Doświadczenie w optymalizacji wydajności baz danych,', 'Znajomość relacyjnych baz danych oraz języka SQL,', 'Podstawowa wiedza z zakresu przetwarzania ETL,', 'Umiejętność modelowania danych w notacji UML,', 'Zdolność do rozwiązywania problemów i szybkiego uczenia się,', 'Umiejętność tworzenia dokumentacji wymagań oraz analiz,', 'Znajomość narzędzia Enterprise Architect na poziomie umożliwiającym swobodne korzystanie,', 'Znajomość języka polskiego na poziomie pozwalającym na swobodną komunikację werbalną i pisemną.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/analityk-systemowy-hurtownie-danych-warszawa,oferta,1003926929?apid=fa7d897b-81b9-4964-96ca-d930b5560cd9&oca=None&acv=0\n",
      "\n",
      "--- Job Record 242 ---\n",
      "id: 295\n",
      "url: https://www.pracuj.pl/praca/starszy-programista-big-data-warszawa,oferta,1003926923\n",
      "title: Starszy Programista Big Data\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: None\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Scala', 'Python', 'Hadoop', 'SQL', 'Linux', 'MongoDB', 'Cassandra', 'HBase', 'MS SSIS', 'Pentaho', 'scrum']\n",
      "responsibilities: ['Tworzenie i optymalizacja rozwiązań bazujących na językach Scala/Python.', 'Praca w środowisku Apache Hadoop (dystrybucja Cloudera) – Spark, Hive, HDFS, HBase.', 'Tworzenie i rozwój rozwiązań Big Data z wykorzystaniem Apache Hadoop.', 'Techniczne wsparcie testów w obszarze testów jednostkowych, integracyjnych oraz UAT (User Acceptance Testing).', 'Dokumentowanie kodu oraz przepływu danych.']\n",
      "requirements: ['Praktyczna znajomość poparta minimum 5-letnim doświadczeniem w programowaniu w językach Scala/Python.', 'Komercyjne doświadczenie w realizacji projektów związanych z rozproszonymi systemami oraz stosami technologicznymi Hadoop.', 'Minimum 5-letnie doświadczenie w pracy z rozwiązaniami klasy Big Data z wykorzystaniem Apache Hadoop & Family (Spark, Hive, HDFS, HBase).', 'Doświadczenie w pracy z nierelacyjnymi bazami danych NoSQL (MongoDB, Cassandra, HBase lub podobne).', 'Doświadczenie komercyjne w zakresie czuwania nad wydajnością rozwiązań opartych na Hadoop/Spark.', 'Doświadczenie w realizacji projektów integracji systemów na dużą skalę.', 'Doświadczenie w pracy z RDBMS oraz bardzo dobra znajomość SQL.', 'Znajomość integracji z usługami RESTful i SOAP/ESB.', 'Praktyczna wiedza z zakresu systemów Linux i języków skryptowych.', 'Umiejętność tworzenia oprogramowania od opisu algorytmu po implementację rozwiązania.', 'Znajomość języka polskiego na poziomie umożliwiającym swobodną komunikację werbalną i pisemną.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/starszy-programista-big-data-warszawa,oferta,1003926923?apid=a374bb1f-64ab-4d57-a004-8959a83e2d57&oca=None&acv=0\n",
      "\n",
      "--- Job Record 243 ---\n",
      "id: 296\n",
      "url: https://www.pracuj.pl/praca/etl-developer-gdansk,oferta,1003912060\n",
      "title: ETL Developer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 21 dnido 19 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['ETL', 'Azure Databricks', 'T-SQL', 'Microsoft Power BI', 'Azure Data Factory']\n",
      "responsibilities: ['Projektowanie i wdrażanie procesów przetwarzania danych (ETL),', 'Projektowanie i praca z modelami danych,', 'Weryfikacja zgodności wdrożonych rozwiązań z wytycznymi,', 'Zapewnianie wsparcia technicznego przedstawicielom IT i biznesu,', 'Współpraca z architektami/programistami/inżynierami, uzgadnianie rozwiązań i rozwiązywanie ewentualnych problemów,', 'Automatyzacja zadań i procesów realizowanych w środowisku chmurowym.']\n",
      "requirements: ['Co najmniej 4 lata doświadczenia na podobnym stanowisku,', 'Doświadczenie pracy w środowisku chmurowym: Azure.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/etl-developer-gdansk,oferta,1003912060?apid=cfc6d332-aec8-410f-847b-4dca6ba387b6&oca=None&acv=0\n",
      "\n",
      "--- Job Record 244 ---\n",
      "id: 297\n",
      "url: https://www.pracuj.pl/praca/manager-ai-ml-interior-sensing-krakow-powstancow-wielkopolskich-13,oferta,1003911991\n",
      "title: Manager AI/ML Interior Sensing\n",
      "work_location: None\n",
      "validity: valid for 21 daysto 19 March 2025\n",
      "contract_type: None\n",
      "employment_type: None\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Powstańców Wielkopolskich 13, Podgórze, KrakówKraków, Lesser Poland', 'contract of employment', 'part time', 'team manager']\n",
      "technologies: []\n",
      "responsibilities: ['Plan and control the quality and schedule of all allocated Machine Learning projects, ensuring the team meets project targets.', 'Plan, monitor, and control capabilities of teams; drive solutions', 'Manage the team in terms of planning, executing, and adjusting to changes in the project.', 'Monitor project execution progress via metrics, key performance indicators.', 'Plan skills development & workload of the team members.', 'Assign the project competency lead.', 'Be active participant in communication with stakeholders, including internal and external customers.', 'Drive problem resolution process.', 'Provide the vision of the team development to the senior management.', 'Perform engineering group members performance rating.', 'Care for good team spirit, motivation, and company image.', 'Permanent and regular developing of professional qualifications.', 'Plan and manage the budget of assigned cost center(s)', 'Take an active role in hiring process of new team members if applicable', 'Care for confidential information and company’s property.']\n",
      "requirements: ['Bachelor or Master of Science degree in engineering', 'Ability to speak, read and write English', 'Ability to speak other foreign languages is nice to have', 'Honesty in relation with company, clients and co-operators', 'Methodical approach to problem solving', 'Prior experience in automotive industry', 'Prior experience in Computer Vision and/or Machine Learning', 'Willingness to travel as required', 'Flexible approach to working hours', 'Commitment to ongoing projects and implementing improvements', 'Solid understanding of artificial intelligence, model building and evaluation', 'Strong knowledge of at least one common ML framework PyTorch, Tensorflow)', 'Experience in leading a team of software engineers']\n",
      "application_link: https://www.pracuj.pl/aplikuj/manager-ai-ml-interior-sensing-krakow-powstancow-wielkopolskich-13,oferta,1003911991?apid=19f61363-c53a-4f2c-982d-b1bf1ea9185f&oca=None&acv=0\n",
      "\n",
      "--- Job Record 245 ---\n",
      "id: 298\n",
      "url: https://www.pracuj.pl/praca/machine-learning-engineer-data-engineer-gdansk,oferta,1003926828\n",
      "title: Machine Learning Engineer / Data Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['GdańskGdańsk, pomorskie']\n",
      "technologies: ['Python', 'Hadoop', 'Spark', 'SQL', 'Git', 'Bitbucket', 'Hive']\n",
      "responsibilities: ['Tworzenie, trenowanie, testowanie i wdrażanie modeli Machine Learning / Data Science', \"Rozwój i optymalizacja pipeline'ów przetwarzania danych na dużą skalę\", 'Praca z bazami danych i SQL (T-SQL, PL/SQL, Spark SQL)', 'Integracja rozwiązań ML z ekosystemem Big Data (Hadoop, Spark, Hive)', 'Współpraca z zespołem w zakresie architektury danych i wdrożeń MLOps', 'Zarządzanie kodem w Git / Bitbucket', 'Tworzenie oraz optymalizacja rozproszonych systemów przetwarzania danych w czasie zbliżonym do rzeczywistego', 'Wsparcie zespołu w integracji systemów (microservices, REST API, WebSockets, async methods)']\n",
      "requirements: ['5+ lat doświadczenia w Pythonie', '4+ lata doświadczenia w Big Data (Hadoop, Spark, przetwarzanie dużych zbiorów danych)', '4+ lata doświadczenia w projektach Machine Learning (trening, testowanie, wdrażanie modeli)', '3+ lata zaawansowanej znajomości SQL (T-SQL, PL/SQL, Spark SQL)', 'Doświadczenie w systemach kontroli wersji (Git / Bitbucket)', 'Znajomość przetwarzania danych w czasie zbliżonym do rzeczywistego (Hadoop, Hive)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/machine-learning-engineer-data-engineer-gdansk,oferta,1003926828?apid=953b79cb-4aec-41cc-8f6c-a132d9da3f3b&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 246 ---\n",
      "id: 299\n",
      "url: https://www.pracuj.pl/praca/global-trade-solutions-data-analyst-krakow-kapelanka-42a,oferta,1003908370\n",
      "title: Global Trade Solutions Data Analyst\n",
      "work_location: None\n",
      "validity: valid for 18 daysto 16 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['Microsoft Excel', 'Python']\n",
      "responsibilities: ['Drive the analytics agenda for GTS, constantly expanding the range of fields in which analytical insight adds value to decision-making.', 'Work with the Regional head of BM to establish a centre of excellence in analytics and move away from data/MI provider.', 'Supporting Sales performance management, Digital, Client Experience to providing informed and independent data-driven expertise and insight.', 'Develop dashboards using different technologies (e.g. excel, data visualisation etc.).', 'Delivering fair outcomes for our customers and ensure own conduct maintains the orderly and transparent operation of financial markets.', \"Leading and encouraging constructive cross-country and cross-business teamwork by demonstrating collaboration and matrix management in action and taking prompt action to address any activities and behaviours that are not consistent with HSBC's diversity policy and/or the best interests of the business and its customers.\"]\n",
      "requirements: ['University degree in business or related discipline, with experience in portfolio management or in a financial services industry.', 'Experience in SAS or Python, Excel, Qlicksense and/or other business intelligence tool development.', 'Ability to deliver high quality reports, presentations and communications to Senior Stakeholders under strict deadlines.', 'Experience in solving business problems with data driven insights.', 'Ability to influence and facilitate strong collaboration across business areas.', 'Knowledge and understanding of the Global Trade Solutions business and broader construct of HSBC’s Global Businesses a positive. Experience in Business Management, COO Office or Strategy roles a positive.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/global-trade-solutions-data-analyst-krakow-kapelanka-42a,oferta,1003908370?apid=63d9df12-87e6-47a2-b7d6-71ade1fe5160&oca=None&acv=0\n",
      "\n",
      "--- Job Record 247 ---\n",
      "id: 300\n",
      "url: https://www.pracuj.pl/praca/data-scientist-warszawa,oferta,1003887629\n",
      "title: Data Scientist\n",
      "work_location: None\n",
      "validity: valid for 9 daysto 07 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian']\n",
      "technologies: ['Python', 'Numpy', 'Scipy', 'Pandas', 'Matplotlib', 'Hadoop', 'Apache Spark', 'Java', 'Scala', 'Linux', 'SQL', 'agile', 'waterfall']\n",
      "responsibilities: ['Develop predictive models and data-driven solutions', 'Analyze data to support projects in fraud prevention, churn analysis, and NLP', 'Collaborate with business teams to understand requirements and deliver insights', 'Identify data sources both within and outside the organization', 'Present product visions and engage stakeholders in the creation process', 'Ensure high-quality statistical analysis and machine learning model development', 'Work with large-scale data using Hadoop and Apache Spark', 'Implement machine learning algorithms and maintain model performance', 'Write and optimize queries in SQL for data analysis', 'Follow best practices in DevOps and Linux administration']\n",
      "requirements: ['Strong knowledge of machine learning concepts and statistical analysis', 'Proficiency in Python and libraries such as Numpy, Scipy, Pandas, and Matplotlib', 'Experience with Hadoop and Apache Spark for big data processing', 'Familiarity with machine learning libraries in Java/Scala', 'Ability to collaborate with cross-functional teams and present insights', 'Strong problem-solving and critical-thinking skills', 'Ability to identify and extract valuable data from multiple sources', 'Knowledge of DevOps principles and Linux administration basics', 'Excellent communication skills and business acumen', 'Familiarity with SQL for querying and data manipulation']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-warszawa,oferta,1003887629?apid=0a362587-3e48-4d91-b2d4-60396061df7c&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 248 ---\n",
      "id: 301\n",
      "url: https://www.pracuj.pl/praca/data-engineer-warszawa,oferta,1003905585\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 17 dnido 15 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Google Cloud Platform', 'Python', 'SQL', 'BigQuery', 'Apache Airflow', 'AWS', 'u klienta', 'wymiana wiedzy technicznej w firmie']\n",
      "responsibilities: ['Tworzenie i optymalizacja struktur oraz modeli danych wspierających przetwarzanie, analizę i przechowywanie danych z różnych źródeł.', 'Monitorowanie procesów przetwarzania danych oraz rozwiązywanie problemów i optymalizacja wydajności systemu.', 'Łączenie wewnętrznych i zewnętrznych źródeł danych, zapewniając ich wysoką jakość, dostępność i spójność.', 'Współpraca z zespołami inżynierii, analityki danych, marketingu i produktu.', 'Wdrażanie algorytmów analizy danych i przewidywania wyników.']\n",
      "requirements: ['Minimum 5 letnie doświadczenie komercyjne w roli Data Engineera.', 'Zaawansowana wiedza i doświadczenie w pracy z chmurą GCP.', 'Doświadczenie w obszarze Data Science i Machine Learning.', 'Biegła znajomość języka Python, SQL oraz BigQuery.', 'Bardzo dobra znajomość Airflow.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-warszawa,oferta,1003905585?apid=042d18b0-88e5-4307-b766-cfa412fa24bb&oca=None&acv=0\n",
      "\n",
      "--- Job Record 249 ---\n",
      "id: 302\n",
      "url: https://www.pracuj.pl/praca/senior-esg-data-specialist-krakow-kapelanka-42a,oferta,1003884571\n",
      "title: Senior ESG Data Specialist\n",
      "work_location: None\n",
      "validity: valid for 8 daysto 06 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: []\n",
      "responsibilities: ['Ensure the end-to-end data strategy for the value stream is documented, aligned to Wholesale and HSBC ESG data strategy, and communicated to the key stakeholders and sub-value streams.', 'Be responsible for documenting and ensure data flows and data lineage for the areas are documented and maintained to ensure end-to-end data integrity.', 'Accountable for ensuring projects comply with the recommended strategy, and continuous interactions with the different implementation teams to remove / clarify any data blockers during their different phases, e.g. data analysis, data modelling or solution architecture.', 'Ensure the projects are aligned with Wholesale data policies, methodologies, systems, and processes to ensure data integrity.', 'Additionally, this job will engage in initiatives to assist business in reducing data risk and deliver savings by facilitating better utilization of data assets.', 'Ensure that business intelligence capabilities are developed and maintained to allow the value stream to maximize the benefit of data assets.', 'Ensure the programme is abreast of new developments and practises Wholesale and HSBC DAO are recommending and developing.', 'Oversee investigations and root cause analyses to identify data integrity issues.']\n",
      "requirements: ['Strong interest in data, i.e., the way they are used, represented, distributed, visualized, and stored in a business context.', 'Ability to document, present, and recommend a data strategy to senior stakeholders and influence a change of approach.', 'Hands-on ability in conceptual data model in more than one financial business domain.', 'Hands-on ability to investigate data asset issues and recommend remediation and strategic approach.', 'Ability to investigate on multiple environments and systems to recommend data sources (e.g., clouds, Hadoop, etc.).', 'Ability to review and recommend a preferred data architecture.', 'Ability to support multiple implementation pods on data queries.', 'Strong communication skills to help for negotiating, collaborating, presenting, facilitating, influencing stakeholders or project teams.', 'Experience with Agile & DevOps.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-esg-data-specialist-krakow-kapelanka-42a,oferta,1003884571?apid=b5003838-fbcb-4d26-bf4e-57d075d873ce&oca=None&acv=0\n",
      "\n",
      "--- Job Record 250 ---\n",
      "id: 303\n",
      "url: https://www.pracuj.pl/praca/data-scientist-language-ai-warszawa-aleje-jerozolimskie-158,oferta,1003923187\n",
      "title: Data Scientist - Language AI\n",
      "work_location: None\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Aleje Jerozolimskie 158, Ochota, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['Python', 'NLP', 'ML', 'GenAI', 'NoSQL', 'AWS', 'Azure', 'GCP', 'in house']\n",
      "responsibilities: ['Ideation and development of innovative AI and data solutions across the Pharma value chain, including Research & Development, Product Supply, Medical Office and Commercial', 'Develop digital products that leverage Large Language Models (LLMs), proprietary and open-source ones, in connection with internal and external data sources and agentic tools, using techniques like Retrieval Augmented Generation (RAG)', 'Design and implement complex NLP and GenAI algorithms to extract value from large amounts of unlabeled text data (clinical notes, regulatory documents, medical literature), building and taking responsibility for end-to-end products from the notebook to production-ready code, from data cleaning to deployment', 'Work with complex AI codebases in Python using libraries such as LangChain, LangGraph, LlamaIndex, Docling, Unstructured, LiteLLM, LangFuse, OpenAI and FastAPI, accessing a range of IT and data architectures and cloud platforms, including serverless architectures in AWS and infrastructure as code', 'Be a team-player that loves to collaborate, influence, and learn from a diverse group of people, and enjoys working with a culture of respect and support to others', 'Collaborate with data engineers, cloud engineers, machine learning engineers, software developers, UX/UI designers, a large IT organization and business domain experts in multidisciplinary projects using agile methodologies to deliver digital products into the organization effectively', 'Work in a dynamic and product-oriented mindset, understanding user needs and setting priorities to match expectations of stakeholders', 'Incorporate aspects of responsible use of AI and consider ethical and environmental implications from your work', 'Be an advocate of best software development practices, including secure development principles, functional testing and documentation', 'Communicate effectively and present analysis and visualizations tailored to your audience', 'Support data science capability building across the team and wider organization']\n",
      "requirements: ['Master’s or Ph.D. in AI, Computer Science, Statistics, Mathematics, or a related field', 'Minimum of 3 years industry experience (preferably pharma) or very strong academic records leading the delivery of groundbreaking machine learning projects', 'Proficiency in Python development for data analysis and machine learning, including expertise in the common libraries and frameworks for NLP, ML and GenAI', 'Advanced knowledge of statistical methods and techniques', 'Knowledge in SQL, relational databases, and NoSQL databases', 'Experience in hypothesis testing, regression analysis, clustering, and classification', 'Familiarity with the managed services by cloud providers (AWS, Azure, GCP)', 'Strong analytical and problem-solving skills', 'Ability to evaluate assumptions and limitations of data', 'Excellent verbal and written communication skills', 'Experience working collaboratively in interdisciplinary teams', 'Familiarity with agile development practices']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-language-ai-warszawa-aleje-jerozolimskie-158,oferta,1003923187?apid=31c6eb17-18e5-4016-8d2b-7881a76e6de5&oca=None&acv=0\n",
      "\n",
      "--- Job Record 251 ---\n",
      "id: 304\n",
      "url: https://www.pracuj.pl/praca/ai-engineer-krakow,oferta,1003887435\n",
      "title: AI Engineer\n",
      "work_location: None\n",
      "validity: valid for 9 daysto 07 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KrakówKraków, Lesser Poland']\n",
      "technologies: ['NLP', 'Machine Learning', 'MLOps']\n",
      "responsibilities: ['Analyze available AI models and recommend the optimal solution with justification', 'Assist in selecting the appropriate hardware and infrastructure for the AI model deployment', 'Install, configure, and train the AI model', 'Support the team in implementing the model within our infrastructure', 'Provide documentation and recommendations for the further development of the AI system']\n",
      "requirements: ['4+ years of experience in AI/ML or related fields', 'Experience with NLP and Machine Learning models, particularly in content classification', 'Ability to select and implement AI models based on business requirements', 'Knowledge of model training processes and optimization', 'Experience working with on-premises AI environments (without using public cloud services)', 'Ability to choose the right hardware for specific AI model requirements', 'Proficiency in programming languages used in AI', 'Strong ability to manage work independently and lead a project from A to Z']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-engineer-krakow,oferta,1003887435?apid=782d4335-20cc-4aa5-8897-895cfde0e912&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 252 ---\n",
      "id: 305\n",
      "url: https://www.pracuj.pl/praca/call-center-technical-expert-ai-poznan-szyperska-14,oferta,1003905337\n",
      "title: Call Center Technical Expert (AI)\n",
      "work_location: None\n",
      "validity: ważna jeszcze 17 dnido 15 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: None\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Szyperska 14, Stare Miasto, PoznańPoznań, wielkopolskie', 'ekspert']\n",
      "technologies: ['AI', 'Salesforce']\n",
      "responsibilities: ['Identyfikacja innowacji technologicznych, ocena wykonalności i wdrożenie rozwiązań IT – głównie w obszarze AI,', 'Odpowiedzialność za planowanie, zarządzanie i realizację projektów w obszarze call center,', 'Optymalizacja i rozwój procesów w celu zwiększenia efektywności i zadowolenia klientów,', 'Kierowanie i motywowanie zespołu call center oraz zapewnienie wysokiej jakości usług,', 'Tworzenie, monitorowanie i analiza wskaźników wydajności (KPI), zbieranie, sprawdzanie i analiza wskaźników ekonomicznych oraz tych specyficznych dla projektów,', 'Koordynacja i komunikacja z wewnętrznymi i zewnętrznymi interesariuszami,', 'Identyfikacja i przedstawienie krytycznych czynników sukcesu,', 'Merytoryczna i dyscyplinarna odpowiedzialność za kadrę kierowniczą oraz ich rozwój,', 'Planowanie, rozwój i zarządzanie działaniami up- i cross-sellingowymi,', 'Odpowiedzialność za koszty i rozwój sprzedaży projektu.']\n",
      "requirements: ['Umiejętności i wiedza dotycząca technologii, takich jak narzędzia raportowe, infrastruktura IT i wdrażanie systemów do połączeń wirtualnych – warunek konieczny', 'Doświadczenie w pracy z narzędziami AI, takimi jak AI Voice Agents i innymi – warunek konieczny', 'Znajomość Salesforce i innych programów wykorzystywanych w call center - mile widziana,', 'Bardzo dobra znajomość języka niemieckiego w mowie i piśmie na poziomie minimum C1- warunek konieczny,', 'Dobra znajomość języka angielskiego,', 'Wykształcenie wyższe w obszarze IT lub o profilu handlowo-finansowym,', 'Wieloletnie doświadczenie w branży call center, doświadczenie w sprzedaży,', 'Umiejętność zarządzania zmianą,', 'Bardzo dobra znajomość MS Office (Word, Excel, Outlook, PowerPoint),', 'Doświadczenie w prezentacjach i moderowaniu spotkań będzie dodatkową zaletą,', 'Wysoka orientacja na usługi, klientów i serwis,', 'Umiejętność komunikacji i rozwiązywania konfliktów oraz umiejętność pracy w zespole,', 'Orientacja na cele i wytrwałość oraz elastyczność i zdolność adaptacji,', 'Odporność na stres, umiejętność podejmowania decyzji,', 'Gotowość do podróży służbowych (zagranicznych).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/call-center-technical-expert-ai-poznan-szyperska-14,oferta,1003905337?apid=b0913022-9abe-42c4-b68d-db06ee928a2f&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 253 ---\n",
      "id: 306\n",
      "url: https://www.pracuj.pl/praca/scala-spark-big-data-developer-wroclaw-ruska-3,oferta,1003926425\n",
      "title: Scala/Spark Big Data Developer\n",
      "work_location: Company locationRuska 3, Stare Miasto, WrocławWrocław, Lower Silesia\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['Linux', 'Spark', 'Scala', 'Git', 'in house', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product']\n",
      "responsibilities: ['Design, implement, and maintain petabyte-scale Big data pipelines using Scala, Spark, Kubernetes and a lot of other tech', 'Optimize – working with Big data is very specific, sometimes it’s IO/CPU-bound, depending on the process, we need to figure out a faster way of doing things. At least empirical knowledge of calculation complexity, as in Big data, even simple operations, when you multiply by the size of the dataset can be costly', 'Conduct Proof of Concept (PoC) for enhancements', 'Writing great and performant Big Data Scala code', 'Cooperate with other Big data teams', 'Work with technologies like AWS, Kubernetes, Airflow, EMR, Hadoop, Linux/Ubuntu, Kafka, and Spark', 'Use Slack and Zoom for communication']\n",
      "requirements: ['2+ years of experience with Linux', 'Solid knowledge of Linux (bash, threads, IPC, filesystems; being power-user is strongly desired, understanding how OS works so you can benefit from performance optimizations in production but also in daily workflows)', '1+ years of experience with Spark,  primarily using Scala for Big data processing (that includes understanding of how Spark works and why)', 'Huge need to drive projects of the future, improve stuff, risk taking mindset - covered by examples', 'Great communication skills (you can drive end-to-end projects, and guide dev team members)', 'Professional working proficiency in English (both oral and written)', 'Understanding of HTTP API communication patterns (HTTP/REST/RPC) and protocol', 'Good software debugging skills, not only prints, but also using debugger', 'Deep understanding of at least one technical area (please let us know which one is this and prepare a story of the biggest battle story about this you had)', 'Quite good understanding of Git']\n",
      "application_link: https://www.pracuj.pl/aplikuj/scala-spark-big-data-developer-wroclaw-ruska-3,oferta,1003926425?apid=d557bbd4-cf7d-4a23-a380-be09cbf3d85b&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 254 ---\n",
      "id: 307\n",
      "url: https://www.pracuj.pl/praca/python-software-engineer-for-big-data-warszawa-chlodna-51,oferta,1003926409\n",
      "title: Python Software Engineer for Big Data\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Chłodna 51, Wola, WarszawaWarszawa, Masovian', 'More than one vacancy']\n",
      "technologies: ['Python', 'English', 'PySpark', 'Databricks', 'Azure DevOps', 'Azure Pipelines', 'in house', 'you focus on a single project at a time', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'agile', 'scrum', 'fullstack developer', 'technical leader', 'devOps', 'automated test programmer', 'manual tester', 'product owner', 'business analyst', 'IT administrator']\n",
      "responsibilities: ['Create and implement a data processing solution from scratch for data scientists working on pricing models', 'Design and execute pipelines for the preparation and cleaning of datasets to ensure high-quality data for model training', 'Establish and maintain good programming standards to ensure consistency and quality in the data solutions developed', 'Work closely with data scientists/actuaries to understand their data needs and ensure the solution meets those requirements']\n",
      "requirements: ['High initiative and proactivity', 'Critical thinking', 'Experience in working for multinational financial institutions is a plus', 'Ability to work autonomously', 'Professional proficiency in English']\n",
      "application_link: https://www.pracuj.pl/aplikuj/python-software-engineer-for-big-data-warszawa-chlodna-51,oferta,1003926409?apid=4805cf5b-e2b0-4dc0-9653-784a5d9c60f3&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 255 ---\n",
      "id: 308\n",
      "url: https://www.pracuj.pl/praca/ai-ml-engineer-warszawa-zabkowska-31,oferta,1003911540\n",
      "title: AI/ML Engineer\n",
      "work_location: Company locationZąbkowska 31, Praga-Północ, WarszawaWarszawa, Masovian\n",
      "validity: valid for 21 daysto 19 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'employer does not require CV']\n",
      "technologies: ['Python', 'PyTorch', 'TensorFlow', 'Keras', 'scikit-learn', 'LangChain', 'LlamaIndex', 'Haystack', 'Microsoft Azure', 'OpenAI', 'AWS', 'Google Cloud Platform', 'C#', 'Java', 'Node.js.', \"at the client's site\", 'agile']\n",
      "responsibilities: ['Spearhead the design, development, and deployment of sophisticated AI/ML models, focusing on areas like Natural Language Processing (NLP), Computer Vision, and broader Machine Learning.', 'Leverage your strong software engineering background to guide agile teams and ensure the timely delivery of complex, impactful projects.', 'Architect and maintain cost-effective, high-performing, and scalable AI/ML solutions.', 'Apply your analytical mindset and problem-solving skills to tackle challenging technical hurdles.', 'Collaborate effectively with clients and stakeholders, clearly communicating project requirements and progress.', 'Embrace a growth-oriented approach, continuously seeking opportunities for personal and professional advancement.', \"Communicate fluently in English (both written and spoken) as you'll be working directly with clients in the UK and US.\", 'Provide technical leadership and mentorship to other team members.']\n",
      "requirements: ['Expertise in Python for machine learning and data science.', 'Practical experience with ML/DL frameworks like PyTorch, TensorFlow, Keras and scikit-learn.', 'Familiarity with Large Language Model (LLM) frameworks such as LangChain, LlamaIndex, or Haystack.', 'Proficiency with cloud-based ML/AI services (e.g., Azure, OpenAI, AWS, GCP).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-ml-engineer-warszawa-zabkowska-31,oferta,1003911540?apid=b3c83263-cf16-49ed-be96-a69ccabee389&oca=None&acv=0\n",
      "\n",
      "--- Job Record 256 ---\n",
      "id: 309\n",
      "url: https://www.pracuj.pl/praca/machine-learning-engineer-warszawa-chlodna-51,oferta,1003926390\n",
      "title: Machine Learning Engineer\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior), expert\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Chłodna 51, Wola, WarszawaWarszawa, Masovian']\n",
      "technologies: ['Azure', 'Docker', 'Kubernetes', 'MLFlow', 'Pandas', 'PowerBI', 'Langchain', 'CI/CD', 'FastAPI', 'Python', 'in house', 'you have influence on the technological solutions applied', 'you develop the code \"from scratch\"', 'you focus on product development', 'agile', 'scrum', 'product owner', 'project manager', 'IT administrator']\n",
      "responsibilities: ['Create continuous integration templates tailored for model development ensuring version control, testing, and reproducibility of our actuarial pricing models and datasets.', \"Close work with members of the ML Engineering team and actuaries to audit and optimize the reliability and scalability of the actuaries' model training pipelines.\", 'Develop effective monitoring strategies to track the performance, reliability, and efficiency of the system.', 'Manage the end-to-end operation of the AI platform to guarantee high availability, responsive performance, and secure data handling during document ingestion and processing.', 'Oversee the integration and management of cloud resources to optimize cost, performance, and compliance with security standards, thereby enabling continuous innovation on the platform.']\n",
      "requirements: [\"Bachelor's or Master's degree in Mathematics, Computer Science, Machine Learning, or related field.\", 'Mastery over Data Science frameworks (pandas, pyspark, sklearn and shap) and MLOPS frameworks (MLFlow, Kedro/Airflow, Hyperopt/Optuna and Great Expectations) in Python.', 'Experience with building GenAI agentic workflows using Langchain or smolagents.', 'Basic familiarity with Dashboarding tools (PowerBI/Tableau).', 'Strong understanding of DevOps methodologies (CI/CD) and experience implementing Github Actions (or similar) workflows.', 'Experience with serving models with APIs using Flask or FastAPI.', 'Experience with cloud platforms (e.g., AWS, Azure, GCP) and containerization (e.g., Docker, Kubernetes).', 'Extremely high attention to detail and rigor.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/machine-learning-engineer-warszawa-chlodna-51,oferta,1003926390?apid=b7e33f12-95e7-4e4c-af20-48abffc9cdf1&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 257 ---\n",
      "id: 310\n",
      "url: https://www.pracuj.pl/praca/data-platforms-technical-lead-data-innovations-unit-warszawa-marynarska-12,oferta,1003907904\n",
      "title: Data Platforms Technical Lead - Data Innovations Unit\n",
      "work_location: None\n",
      "validity: valid for 18 daysto 16 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Marynarska 12, Mokotów, WarszawaWarszawa, Masovian', 'manager / supervisor, team manager', 'Запрошуємо працівників з України']\n",
      "technologies: ['AWS', 'GCP', 'NoSQL', 'Kubernetes', 'agile']\n",
      "responsibilities: ['Lead the design and development of data platform solutions', 'Create and maintain technical standards and best practices', 'Make decisions on architecture, tools, and frameworks for data ingestion, processing and analytics', 'Ensure alignment with enterprise technology and security standards', 'Implement and manage cloud-based (e.g. AWS, GCP) and on-prem data & analytics platforms', 'Ensure compliance with DevOps practices and privacy & security policies', 'Optimize platform performance and cost-efficiency, troubleshoot complex technical issues', 'Team & Stakeholder Management:', 'Oversee and mentor a team of data engineers and developers', 'Foster a collaborative and learning-oriented environment', 'Work closely with stakeholders to align technical efforts with business objectives', 'Create and maintain product backlogs and roadmaps', 'Estimate and plan work, providing status updates to stakeholders']\n",
      "requirements: ['Extensive experience (4+ years) in designing and delivering cloud or hybrid data platforms', 'Strong background in data engineering and analytics', 'Proficiency in cloud technologies (AWS, GCP)', 'Experience with Data Warehousing, Big Data technologies and NoSQL databases', 'Strong leadership and communication skills', 'Ability to translate complex technical concepts for non-technical stakeholder']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-platforms-technical-lead-data-innovations-unit-warszawa-marynarska-12,oferta,1003907904?apid=2ad1f466-35fc-44b0-9f05-bc71a4f1b5d4&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 258 ---\n",
      "id: 311\n",
      "url: https://www.pracuj.pl/praca/snowflake-architect-warszawa-grzybowska-87,oferta,1003926357\n",
      "title: Snowflake Architect\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Grzybowska 87, Wola, WarszawaWarszawa, Masovian']\n",
      "technologies: ['SQL', 'Python', 'Snowflake Data Cloud', 'Teradata', 'Microsoft Power BI', 'agile']\n",
      "responsibilities: ['This role will play a crucial role in redefining and executing our client’s integration strategy by deploying the full range of new Integration Services across the entire organization', 'You will be gathering business requirements from customers and translating them into a well-architected recommendation', 'The role involves defining high-quality integration solutions that meet customer needs, follow best practices, and encourage reusability', 'This role aims to transform the perception of Integration within the organization, establishing Integration Services as a trusted partner and enabler for the business and its domains']\n",
      "requirements: ['Experience implementing and operating Snowflake solutions', 'Excellent understanding of the data analytics stack and workflow, from ETL processes to data platform design, and the use of BI and analytics tools', 'Familiarity with BI and ETL/ELT solutions', 'Hands-on experience in technical roles leveraging SQL andPython to develop, operate, and maintain data analytics solutions', 'Extensive knowledge and hands-on experience with large-scale database technologies such as Snowflake, Teradata, and others', 'Expertise in implementing data security measures, access controls, and architecture specifically within the Snowflake platform']\n",
      "application_link: https://www.pracuj.pl/aplikuj/snowflake-architect-warszawa-grzybowska-87,oferta,1003926357?apid=e409cd5e-7d3f-4c5b-b8a8-7b14b2ad83eb&oca=None&acv=0\n",
      "\n",
      "--- Job Record 259 ---\n",
      "id: 312\n",
      "url: https://www.pracuj.pl/praca/data-engineer-azure-etl-warszawa,oferta,1003926309\n",
      "title: Data Engineer (Azure, ETL)\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatów']\n",
      "technologies: ['Microsoft Azure', 'ETL', 'Microsoft SQL Server', 'Netezza', 'Python', 'Spark', 'Databricks', 'Informatica', 'Microsoft Power BI', 'u klienta', 'koncentrujesz się na jednym projekcie', 'możesz zmienić projekt', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'dokumentacja']\n",
      "responsibilities: [\"Poszukujemy Data Engineera, który dołączy do naszego zespołu i wesprze nas w budowie i optymalizacji pipeline'ów danych w Azure i Databricks.\"]\n",
      "requirements: ['4-5 lat doświadczenia w roli Data Engineera', 'Znajomość Azure Databricks i platformy Azure Cloud', 'Dobra znajomość ETL, doświadczenie w pracy z bazami danych (SQL Server, Netezza)', 'Dobre umiejętności w Python i Spark']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-azure-etl-warszawa,oferta,1003926309?apid=be4e843c-5b01-4dc2-b525-0531ad17e274&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 260 ---\n",
      "id: 313\n",
      "url: https://www.pracuj.pl/praca/senior-machine-learning-engineer-mlops-engineer-warszawa,oferta,1003926263\n",
      "title: Senior Machine Learning Engineer / MLOps Engineer\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian']\n",
      "technologies: ['Python', 'Unix', 'Linux', 'Spark', 'Hadoop', 'Hive', 'NoSQL', 'Keras', 'TensorFlow', 'PyTorch', 'Airflow', 'Kubeflow', 'Google Cloud Platform', 'Microsoft Azure']\n",
      "responsibilities: ['Design and improve machine learning solutions in Python, ensuring robust performance for massive cloud-based applications on platforms like Google Cloud and Azure', 'Implement automated deployment workflows using tools such as Airflow, Kubeflow, Jenkins, and handle Docker containers and Kubernetes for efficient scaling and orchestration', 'Engage with sizable datasets, ensuring effective processing and assimilation for LLM applications while resolving technical issues to boost system performance', 'Develop and maintain RESTful APIs to facilitate seamless integration of AI and data services across various systems', 'Cooperate with multi-disciplinary teams to deliver scalable, high-quality solutions, and stay informed about the latest developments in machine learning, cloud technologies, and AI methodologies']\n",
      "requirements: ['Strong knowledge of Machine Learning and Deep Learning algorithms', 'Expertise in Python, including libraries such as pandas, scikit-learn, TensorFlow, and PyTorch', 'Background in cloud development (Google Cloud or Azure) and deployment tools (e.g., Airflow, Kubeflow)', 'Proficiency in Docker, Kubernetes, and Unix/Linux for automation', 'Strong problem-solving and troubleshooting abilities', 'Experience handling large datasets and building RESTful APIs']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-machine-learning-engineer-mlops-engineer-warszawa,oferta,1003926263?apid=22ceafd5-3772-4999-96a6-9687ed7810a1&oca=None&acv=0\n",
      "\n",
      "--- Job Record 261 ---\n",
      "id: 314\n",
      "url: https://www.pracuj.pl/praca/data-engineer-pyspark-wroclaw,oferta,1003926262\n",
      "title: Data Engineer (PySpark)\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WrocławWrocław, Lower Silesia']\n",
      "technologies: ['PySpark', 'Python', 'SQL', 'Microsoft Azure']\n",
      "responsibilities: ['Design scalable and resilient data architectures on Azure, leveraging services like Azure Data Lake Storage, Azure SQL Database, and Azure Data Factory', 'Evaluate and implement new tools and technologies to enhance data processing capabilities', 'Lead the development and optimization of complex data pipelines using PySpark and Azure Databricks', 'Provide technical guidance and mentorship to junior developers, ensuring best practices and high-quality code', 'Identify and drive opportunities for process improvement and automation', 'Stay abreast of industry trends and emerging technologies to ensure the organization remains competitive in big data and cloud solutions', 'Solve complex data challenges using innovative approaches and critical thinking', 'Collaborate with business stakeholders, data scientists, and engineers to understand strategic goals and deliver data-driven solutions']\n",
      "requirements: ['At least 2 years of experience in big data processing and cloud technologies', 'Hands on experience with PySpark, Python, and SQL', 'Experience with Azure cloud services, including Azure Databricks, Data Lake, and Azure Data Factory would be beneficial', 'Excellent communication and interpersonal skills, with the ability to influence and drive change']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-pyspark-wroclaw,oferta,1003926262?apid=2e6d0881-d0f6-47f9-8d32-af0e99ffa4ae&oca=None&acv=0\n",
      "\n",
      "--- Job Record 262 ---\n",
      "id: 315\n",
      "url: https://www.pracuj.pl/praca/data-visualization-specialist-krakow,oferta,1003887060\n",
      "title: Data Visualization Specialist\n",
      "work_location: None\n",
      "validity: valid for 9 daysto 07 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KrakówKraków, Lesser Poland', 'Запрошуємо працівників з України']\n",
      "technologies: ['Tableau', '(Pandas', 'Numpy', 'Pyspark', 'in house', 'you focus on a single project at a time', 'you develop several projects simultaneously', 'you can change the project', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'agile', 'scrum', 'waterfall']\n",
      "responsibilities: ['Developing data visualizations in the form of advanced dashboards and reports', 'Understanding infrastructure requirements and best practices to support a Tableau deployment', 'Data Source Management – ensuring data integrity, updating data as needed', 'Managing Project site on Tableau Server', 'Documenting processes, data sources and dashboards', 'Analyze & drive data sharing best practices around user access', 'Strong stakeholder management and influencing skills']\n",
      "requirements: ['Proficiency in Tableau', '3+ years’ work experience in data analysis', 'Ability to query and display large data sets while maximizing the performance of Tableau workbooks', 'Working knowledge of Tableau administrator/architecture', 'Ability to work collaboratively with cross-functional teams and stakeholders at all levels', 'A high degree of mathematical competence', 'Analytically minded']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-visualization-specialist-krakow,oferta,1003887060?apid=8460cead-b3ed-48ae-99f6-c83a9450cae8&oca=None&acv=0\n",
      "\n",
      "--- Job Record 263 ---\n",
      "id: 316\n",
      "url: https://www.pracuj.pl/praca/data-engineer-warszawa,oferta,1003887041\n",
      "title: Data engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 9 dnido 07 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL', 'Python', 'Microsoft Azure', 'u klienta', 'wymiana wiedzy technicznej w firmie']\n",
      "responsibilities: ['Zaprojektowanie modeli danych i udokumentowanie wszelkich zmian zgodnie ze standardami i obowiązującym procesem projektowym.', 'Aktywne i wnikliwe słuchanie, które wspomoże przełożenie wymagań biznesowych na koncepcje rozwiązań informatycznych platform danych.', 'Łączenie świata technologicznego i biznesowego, dzięki czemu możliwa będzie wzajemna komunikacja.', 'Aktywna współpraca z jednostkami biznesowymi przy projektowaniu optymalnych przepływów danych.', 'Rozpoznanie danych i zrozumienie ich przy użyciu Pythona czy SQL.', 'Inicjowanie procesów optymalizacji.']\n",
      "requirements: ['Minimum 5 lat doświadczenia jako Data Engineer, a mile widziane, jeśli kluczowe kompetencje masz poparte certyfikatami lub ukończonymi kursami.', 'Dobra znajomość SQL (umiejętność tworzenia złożonych zapytań i wykorzystania funkcji analitycznych baz danych) oraz Pythona (obróbka i łączenie danych).', 'Łatwość w nawiązywaniu i utrzymywaniu kontaktów zarówno z odbiorcami/dostawcami wewnętrznymi, jak i zewnętrznymi.', 'Co najmniej 3-letnie doświadczenie w pracy z technologią Databricks i z chmurą Azure.', 'Doświadczenie w pracy analitycznej przy budowie procesów ETL, DWH, Data Lake.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-warszawa,oferta,1003887041?apid=5bdb2f01-4471-4c0b-92b6-2d87a070feec&oca=None&acv=0\n",
      "\n",
      "--- Job Record 264 ---\n",
      "id: 317\n",
      "url: https://www.pracuj.pl/praca/programista-sql-warszawa,oferta,1003926204\n",
      "title: Programista SQL\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: None\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'praca stacjonarna']\n",
      "technologies: ['SQL']\n",
      "responsibilities: ['Projektowanie rozwiązań bazodanowych', 'Modelowanie struktur danych', 'Implementacja procesów bazodanowych', 'Testowanie oraz wdrażanie tworzonych rozwiązań', 'Diagnozowanie i usuwanie błędów programistycznych', 'Optymalizacja oraz poprawa wydajności działających procesów', 'Praca w środowiskach  Visual Studio, SQL Server']\n",
      "requirements: ['Wykształcenie wyższe w zakresie nauk IT lub pokrewnych', 'Umiejętności analitycznych , rozumienie procesów biznesowych i ich poprawnego mapowania', 'Minimum 2 lata doświadczenia w programowaniu w językach  T-sql,  PISql,  PgPISql', 'Znajomość relacyjnych baz danych', 'Znajomość tematyki hurtowni danych', 'Znajomość podstaw programowania obiektowego', 'Umiejętność identyfikacji, analizowania oraz rozwiązywania problemów']\n",
      "application_link: https://www.pracuj.pl/aplikuj/programista-sql-warszawa,oferta,1003926204?apid=ab20d8e3-3aa7-4929-8857-e8c99cfe956c&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 265 ---\n",
      "id: 318\n",
      "url: https://www.pracuj.pl/praca/python-data-engineer-warszawa-dluga-29,oferta,1003922668\n",
      "title: Python Data Engineer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 25 dnido 23 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Python', 'Microsoft Azure', 'MongoDB', 'Docker', 'Kubernetes', 'Git', 'u klienta', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'agile', 'DevOps']\n",
      "responsibilities: [\"Tworzenie i utrzymywanie dużej skali pipeline'ów danych i umożliwianie ich integracji i wdrażania w skalowalnych środowiskach chmury Azure\", 'Wprowadzanie innowacji w celu zwiększenia wydajności komponentów technicznych', 'Dbanie o wydajne wdrażanie i skalowalność rozwiązań', 'Współpraca z innymi członkami zespołów produktowych (np. DevOps, Data Engineers, Data Scientists, Cloud Engineers, Solution Architects, and Python Engineers)', 'Prowadzenie dokumentacji technicznej', 'Praca w metodologii Agile']\n",
      "requirements: ['Co najmniej 3 lata doświadczenia w pracy na podobnych stanowiskach', \"Dobra znajomość Python'a, w tym bibliotek do przetwarzania i analizy danych\", \"Doświadczenie w tworzeniu i utrzymywaniu Data Pipeline'ów na dużą skalę przy użyciu języka Python\", 'Znajomość plaftormy chmurowej Azure oraz usług do przechowywania, przetwarzania i wdrażania danych (np. Azure Data Lake, Azure Functions, Azure DevOps)', 'Doświadczenie w pracy z bazami danych NoSQL (najlepiej MongoDB)', 'Znajomość metodologii i narzędzi DevOps jak np. Docker, Kubernetes, Git', 'Język angielski umożliwiający swobodną komunikację (min B2/C1)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/python-data-engineer-warszawa-dluga-29,oferta,1003922668?apid=9a6c0dfc-19e5-44b7-a909-d530922acef9&oca=None&acv=0\n",
      "\n",
      "--- Job Record 266 ---\n",
      "id: 319\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-warszawa-dluga-29,oferta,1003922653\n",
      "title: Senior Data Engineer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 25 dnido 23 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Python', 'R', 'DBT', 'SQL', 'Snowflake', 'ETL', 'Airflow', 'Glue', 'Dataflow', 'Elasticsearch', 'Redshift', 'BigQuery', 'Lambda', 'S3', 'EBS', 'u klienta', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu']\n",
      "responsibilities: ['Wykorzystywanie standardów inżynierii danych, aby rozwijać solidne i łatwe w utrzymaniu pipeliny danych', 'Stosowanie najlepszych praktyk branżowych w zakresie magazynowania danych, zarządzania danymi i zarządzania cyklem życia danych', 'Analizuj i organizuj pipeliny pozyskiwania surowych danych', 'Wsparcie starszych interesariuszy biznesowych w definiowaniu nowych przypadków użycia produktów danych i ich wartości', 'Modelowanie danych']\n",
      "requirements: ['Co najmniej 4 lata na podobnym stanowisku', 'Bardzo dobra znajomość tworzenia data pipelinów z wykorzystaniem Python lub R', 'Dobra znajomość DBT, SQL', 'Znajomość: Snowflake, narzędzi ETL', 'Znajomość Airflow, Glue, Dataflow lub innych rozwiązań do obsługi danych w chmurze - Elastic, Redshift, BigQuery, Lambda, S3, EBS itp.', 'Bardzo dobra znajomość angielskiego (min B2/C1)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-warszawa-dluga-29,oferta,1003922653?apid=ddfbd8b6-653b-4a64-a939-b00549001698&oca=None&acv=0\n",
      "\n",
      "--- Job Record 267 ---\n",
      "id: 320\n",
      "url: https://www.pracuj.pl/praca/internship-program-dan-one-in-it-data-global-data-analytics-project-management-warszawa-bobrowiecka-8,oferta,1003907686\n",
      "title: Internship Program DAN One in IT&Data Global Data&Analytics (Project Management)\n",
      "work_location: None\n",
      "validity: valid for 18 daysto 16 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Bobrowiecka 8, Mokotów, WarszawaWarszawa, Masovian', 'contract of employment', 'trainee']\n",
      "technologies: ['in house']\n",
      "responsibilities: ['Helping with Projects: You’ll be a part of our team, helping us keep our projects on track.', 'Collecting Data: You’ll gather the data we need for our projects. Don’t worry, we’ll guide you on what to collect!', 'Organizing Meetings: You’ll help us set up our team meetings. This means deciding when and where we meet etc.', 'Other Tasks: You’ll also help with other tasks related to our projects. We promise it won’t be boring!', 'So, are you ready to jump in and start learning with us? Hit that apply button! We can’t wait to meet you']\n",
      "requirements: ['Fluent in English: You’re comfortable communicating in English, both written and spoken.', 'Proactive: You don’t wait for things to happen, you make them happen! You’re always looking for ways to improve and get things done.', 'Organized: You know how to manage your time and tasks effectively. You can prioritize your work and meet deadlines.', 'Proficient in PowerPoint & Excel: You’re comfortable using PowerPoint to create engaging presentations and Excel for data analysis and reporting.', 'Analytical: You’re good at looking at data and spotting trends, patterns, and insights.', 'Team Player: You work well with others and believe that teamwork makes the dream work!', 'Curious: You’re always eager to learn and grow, and you’re not afraid to ask questions.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/internship-program-dan-one-in-it-data-global-data-analytics-project-management-warszawa-bobrowiecka-8,oferta,1003907686?apid=0dda27db-8395-405a-88a8-ce265ebb4459&oca=None&acv=0\n",
      "\n",
      "--- Job Record 268 ---\n",
      "id: 321\n",
      "url: https://www.pracuj.pl/praca/python-developer-warszawa,oferta,1003904918\n",
      "title: Python Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 17 dnido 15 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna, praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['opencv', 'Python', 'GitLab', 'Linux', 'Docker', 'C++', 'ROS2', 'mavros', 'mavproxy', 'TensorFlow', 'elastyczny czas pracy']\n",
      "responsibilities: ['Projektowanie, programowanie i wdrażanie algorytmów przetwarzania obrazów pochodzących z jednostki latającej', 'Testowanie i walidacja przygotowanego oprogramowania', 'Opracowywanie dokumentacji technicznej tworzonego kodu', 'Szkolenie użytkowników w zakresie obsługi przygotowanych systemów', 'Przygotowywanie raportów na temat wydajności i funkcjonalności tworzonych algorytmów', 'Współpraca integracyjna z systemami VMS kamer CCTV', 'Przewodzenie (lider zespołu)', 'Realizowanie innych zadań związanych z systemami wizyjnymi, zgodnie z potrzebami organizacji w tym integracji z innymi elementami projektu']\n",
      "requirements: ['4-letnie doświadczenie w programowaniu w Pythonie z dobrą znajomością API do programowania współbieżnego - multiprocessing', 'umiejętność identyfikowania wąskich gardeł w szybkim procesowaniu danych wizualnych - procesowanie obrazu real-time', 'umiejętność projektowania niezawodnych aplikacji z bardzo dobrą propagacją wyjątków i logowania błędów', 'dobra znajomość paradygmatu programowania funkcyjnego', 'umiejętność tworzenia oprogramowania w cyklu TDD', 'znajomość zasad tworzenia i implementacji architektur z zachowaniem separacji interfejsów', 'umiejętność przewidywania rozwoju aplikacji w oparciu o wymagania określane przez szefa produktu', 'cierpliwość, sumienność, precyzja - przeciwieństwo hype-driven development', 'przewodzenie i udzielanie instrukcji młodszym członkom zespołu, planowanie pracy', 'zarządzanie bazą kodu w repozytorium git z zachowaniem zasad dobrej praktyki programistycznej - process pull request/code review', 'wyobraźnia przestrzenna, trzeba co najmniej lubić problemy geometryczne', 'znajomość biblioteki open-cv', 'mile widziane doświadczenie w przetwarzaniu analitycznym obrazu w tempie ~20fps', 'mile widziane doświadczenie w technologiach stosowanych w robotyce: ROS2, mavros, mavproxy', 'bardzo dobra znajomość systemu Linux, umiejętność sprawnego rozwiązywania problemów sieciowych', 'mile widziane doświadczenie w wizji komputerowej - np. trenowania modeli YOLO, pracowania ze zbiorami danych treningowych']\n",
      "application_link: https://www.pracuj.pl/aplikuj/python-developer-warszawa,oferta,1003904918?apid=c01b651d-6ca3-44f1-9606-c499562237fb&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 269 ---\n",
      "id: 322\n",
      "url: https://www.pracuj.pl/praca/data-engineer-warszawa,oferta,1003904906\n",
      "title: Data Engineer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 17 dnido 15 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['SQL', 'Terraform', 'Docker', 'Kubernetes', 'Python', 'Microsoft Azure', 'Azure DevOps', 'u klienta', 'wymiana wiedzy technicznej w firmie']\n",
      "responsibilities: ['Współpraca i Agile w kooperacji z architektami danych, inżynierami AI oraz zespołami biznesowymi w metodyce Scrum.', 'Przetwarzanie danych w czasie rzeczywistym z wykorzystaniem rozwiązań strumieniowych i aktualizacji modeli AI.', 'Budowa i optymalizacja potoków danych (ETL/ELT) dla integracji, transformacji i przechowywania danych.', 'Zapewnienie jakości danych poprzez mechanizmy walidacji, wykrywania anomalii i monitorowania.', 'Automatyzacja i CI/CD w celu usprawnienia przepływów danych i wdrażania rozwiązań DevOps.', 'Tworzenie i implementacja skalowalnych Data Hubów wspierających potrzeby organizacji.']\n",
      "requirements: ['Minimum 3-letnie doswiadczenie na stanowisku Data Engineer lub podobne.', 'Praktyczna znajomość ETL/ELT i przetwarzania danych obejmująca projektowanie potoków oraz strumieniowe przetwarzanie danych.', 'Biegłość w SQL i zapewnieniu jakości danych z umiejętnością automatycznej walidacji i monitorowania danych.', 'Znajomość DevOps & automatyzacji z wykorzystaniem CI/CD, Terraform, Docker, Kubernetes/AKS.', 'Znajomość programowania w Pythonie lub innym języku odpowiednim dla inżynierii danych.', 'Doświadczenie z chmurą i usługami Azure (Data Factory, ADLS, Azure SQL).', 'Znajomość języka angielskiego na poziomie minimum B2 w mowie i piśmie.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-warszawa,oferta,1003904906?apid=a15e8ccf-7fa7-4e52-96f6-6acde308e17c&oca=None&acv=0\n",
      "\n",
      "--- Job Record 270 ---\n",
      "id: 323\n",
      "url: https://www.pracuj.pl/praca/application-manager-with-ai-katowice-chorzowska-146,oferta,1003922499\n",
      "title: Application Manager with AI\n",
      "work_location: None\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Chorzowska 146, KatowiceKatowice, Silesian', 'contract of employment', 'expert']\n",
      "technologies: ['Microsoft Azure', 'Microsoft Power Platform', 'Microsoft Power BI', 'SAP']\n",
      "responsibilities: ['Lead the design and development of AI-driven applications using low-code/no-code platforms for rapid deployment', 'Manage the full lifecycle of AI integration from conceptualization to deployment and continuous improvement', 'Focus on Microsoft Power Platform solutions (RPA, Power Apps, Chatbots) and prepare technical designs aligned with business needs', 'Ensure compliance with data protection regulations and AI ethics, and provide training and support to end-users', 'Proficient in creating and working with APIs for seamless software integration, while staying updated on trends in low-code/no-code technology and AI']\n",
      "requirements: ['Degree in Information Technology or Business Administration with 1-3 years of professional experience', 'High proficiency in English and German', 'Strong understanding of Artificial Intelligence and machine learning solutions', 'Proven experience with low-code/no-code platforms including AI features, MS Azure, IT tools (Microsoft Power Platform), reporting tools (Power BI), process mining tools (SAP Signavio) and understanding of IT architecture (SAP BTP)', 'Strong project management skills and ability to lead cross-functional projects', 'Entrepreneurial mindset, strong analytical skills, and excellent communication skills for driving change and collaborating with business streams and service functions']\n",
      "application_link: https://www.pracuj.pl/aplikuj/application-manager-with-ai-katowice-chorzowska-146,oferta,1003922499?apid=9c6b66d7-6bee-4b3b-b6eb-984e5e402be0&oca=None&acv=0\n",
      "\n",
      "--- Job Record 271 ---\n",
      "id: 324\n",
      "url: https://www.pracuj.pl/praca/ai-expert-nlp-ml-relocation-to-belgium-belgium,oferta,11172800\n",
      "title: AI Expert (NLP, ML) - Relocation to Belgium\n",
      "work_location: None\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior), expert\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Belgium, Belgiumabroad']\n",
      "technologies: ['AWS', 'Microsoft Azure', 'Google Cloud Platform', 'GitHub', 'GitLab', 'Docker', 'scrum', 'backend developer', 'frontend developer', 'devOps', 'automated test programmer', 'product owner', 'project manager', 'graphic designer', 'architect / technical leader support']\n",
      "responsibilities: ['Developing and optimizing AI/ML and NLP models using advanced algorithms.', 'Deploying AI solutions in the cloud and utilizing containerization and version control.', 'Analyzing, modeling, and processing data, including exploration and segmentation.', 'Tracking new AI technologies and supporting their integration with IT systems.']\n",
      "requirements: ['At least 5 years of professional experience in AI development, including machine learning and NLP.', 'Proven expertise in Deep Learning models and NLP techniques.', 'Strong command of machine learning techniques and algorithms such as k-NN, Naïve Bayes, SVM, Decision Forests, Neural Networks, and AI frameworks.', 'Ability to adapt to evolving AI technologies and methodologies.', 'Hands-on experience in data preparation and cleaning for AI and machine learning models.', 'Minimum 3 years of experience with cloud services (AWS, Azure, GCP) and containerization tools (e.g., Docker) for AI/ML application deployment.', 'Minimum 2 years of experience with version control systems (GitHub/GitLab) and ETL tools (e.g., Data Services, Talend, etc.).', 'Strong technical proficiency in database design, data modeling, data mining, and segmentation techniques.', 'Relocation to Belgium required.', 'Very good English and Polish.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-expert-nlp-ml-relocation-to-belgium-belgium,oferta,11172800?apid=9d632e8c-5aba-4cd1-9c8c-90c1b4273490&oca=None&acv=0\n",
      "\n",
      "--- Job Record 272 ---\n",
      "id: 325\n",
      "url: https://www.pracuj.pl/praca/data-engineer-poznan,oferta,1003886622\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 9 dnido 07 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['PoznańPoznań, wielkopolskie', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['PL/SQL', 'Azure Databricks', 'DWH', 'Python', 'agile', 'scrum']\n",
      "responsibilities: ['Projektowanie i testowanie rozwiązań chmurowych', 'Udział w migracji danych do chmury', 'Współpraca z zespołem w planowaniu strategii migracyjnej', 'Implementacja oraz optymalizacja procesów ETL', 'Tworzenie i zarządzanie hurtowniami danych (DWH)', 'Projektowanie oraz implementacja skryptów i danych testowych']\n",
      "requirements: ['Min. 3 lata doświadczenia jako Data Engineer', 'Znajomośc podstaw ETL', 'Podstawowa znajomość PL/SQL', 'Doświadczenie z Azure databricks', 'Średnia znajmość technologii DWH']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-poznan,oferta,1003886622?apid=e344638b-1688-4885-aa83-20adad49dbe5&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 273 ---\n",
      "id: 326\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-wdrozen-i-integracji-ai-gdansk-swietokrzyska-73,oferta,1003886611\n",
      "title: Specjalista ds. wdrożeń i integracji AI\n",
      "work_location: None\n",
      "validity: ważna jeszcze 9 dnido 07 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Świętokrzyska 73, GdańskGdańsk, pomorskie']\n",
      "technologies: ['wewnątrz organizacji', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne']\n",
      "responsibilities: ['Odkrywanie i analiza: Szukanie narzędzi AI i sprawdzanie, gdzie mogą pomóc naszym działom.', 'Wdrożenia: Wprowadzanie wybranych rozwiązań i współpraca z zespołem IT.', 'Optymalizacja: Monitorowanie działania wdrożonych rozwiązań i wprowadzanie usprawnień.']\n",
      "requirements: ['Zainteresowanie AI: Masz pasję do sztucznej inteligencji i chęć uczenia się nowych rzeczy.', 'Podstawowa znajomość technologii: Nawet jeśli nie jesteś programistą, mile widziana jest znajomość API (np. REST) i podstawowe umiejętności techniczne.', 'Chęć do pracy zespołowej: Potrafisz komunikować się i współpracować z innymi, nawet jeśli dopiero zdobywasz doświadczenie.', 'Proaktywne podejście: Nie boisz się nowych wyzwań i chcesz rozwijać swoje kompetencje.', 'Wykształcenie wyższe']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-wdrozen-i-integracji-ai-gdansk-swietokrzyska-73,oferta,1003886611?apid=cef61637-5007-493c-a115-08aef157241e&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 274 ---\n",
      "id: 327\n",
      "url: https://www.pracuj.pl/praca/administrator-baz-danych-rzeszow-olchowa-14,oferta,1003896320\n",
      "title: Administrator Baz Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 14 dnido 12 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Olchowa 14, RzeszówRzeszów, podkarpackie', 'Szukamy wielu kandydatówwakaty: 2']\n",
      "technologies: ['Oracle', 'PostgreSQL', 'Unix', 'Linux', 'Bash', 'Microsoft Azure', 'AWS', 'MongoDB', 'MySQL']\n",
      "responsibilities: ['Zapewnienie właściwego poziomu świadczenia usług IT w obszarze baz danych dla naszych Klientów', 'Administracja, utrzymanie oraz konfiguracja baz danych w zintegrowanym środowisku biznesowym „Asseco BooX”', 'Planowanie i wdrażanie aktualizacji oraz poprawek bezpieczeństwa dla baz danych', 'Obsługa, analiza zgłoszeń i incydentów związanych z działaniem baz danych (praca w systemie ticketowym)', 'Odpowiedzialność za realizację zadań związanych z bazami danych, m.in. takich jak przygotowanie środowisk, migracja danych, backup, upgrade', 'Współtworzenie dokumentacji technicznej oraz bazy wiedzy', 'Pomoc w zakresie projektowania i zarządzania bazą danych', 'Wsparcie zespołów aplikacyjnych w optymalizacji wydajności i rozwiązywaniu problemów', 'Nadawanie uprawnień']\n",
      "requirements: ['Doświadczenie w zakresie pracy z bazami danych (Oracle, PostgreSQL)', 'Doświadczenie w pracy w środowisku chmury Oracle Cloud Infrastructure', 'Doświadczenie w współpracy z administratorami infrastruktury i twórcami aplikacji w zakresie realizacji projektów oraz rozwiązywania zadań serwisowych', 'Doświadczenie w optymalizacji baz pod kątem wydajności i stabilności', 'Umiejętność rozwiązywania problemów i incydentów zgłaszanych przez Klientów oraz działy wewnętrzne', 'Doświadczenie w monitorowaniu baz danych, znajomość narzędzi monitorujących', 'Znajomość systemów operacyjnych rodziny Unix/Linux', 'Tworzenie skryptów powłoki Bash']\n",
      "application_link: https://www.pracuj.pl/aplikuj/administrator-baz-danych-rzeszow-olchowa-14,oferta,1003896320?apid=f08fba23-56ac-40a7-abed-9dbc40b28237&oca=None&acv=0\n",
      "\n",
      "--- Job Record 275 ---\n",
      "id: 328\n",
      "url: https://www.pracuj.pl/praca/sql-developer-warszawa-lucka-15,oferta,1003925834\n",
      "title: SQL Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: umowa o pracę, umowa zlecenie, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Łucka 15, Wola, WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatówwakaty: 3']\n",
      "technologies: ['SQL', 'PL/SQL', 'Oracle', 'u klienta', 'możesz zmienić projekt', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'agile', 'scrum', 'kanban']\n",
      "responsibilities: ['projektowanie, rozwijanie i zarządzanie bazami danych,', 'tworzenie zapytań SQL oraz procedur składowanych,', 'optymalizacja baz danych w celu poprawy wydajności,', 'tworzenie raportów i analiz danych,', 'utrzymywanie integralności danych oraz zapewnienie bezpieczeństwa danych,', 'monitorowanie i rozwiązywanie problemów związanych z bazą danych,', 'dokumentowanie procesów oraz procedur związanych z bazą danych.']\n",
      "requirements: ['posiadasz przynajmniej 4-letnie doświadczenie w pracy z technologiami SQL (PL/SQL, Oracle Database),', 'masz doświadczenie w pracy z hurtowniami danych (DWH) i procesami ETL,', 'masz wiedzę i rozumiesz zagadnienia dotyczące optymalizacji procesów i zapytań SQL, w tym zaawansowanej optymalizacji zapytań w Oracle oraz analizy planów wykonania zapytań,', 'znasz narzędzia diagnostyczne Oracle,', 'posiadasz umiejętność dekompozycji procesów oraz stosowania dobrych praktyk w modelowaniu danych i tworzeniu wydajnych zapytań,', 'znasz zagadnienia związane z Oracle Partitioning, Oracle Data Guard oraz Oracle Sharding,', 'znasz narzędzia wspierające wytwarzanie oprogramowania opartego o bazę danych, takie jak git, Liquibase/Flyway, Maven.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/sql-developer-warszawa-lucka-15,oferta,1003925834?apid=3bd30541-1de2-4c46-8055-a5b2495413b9&oca=None&acv=0\n",
      "\n",
      "--- Job Record 276 ---\n",
      "id: 329\n",
      "url: https://www.pracuj.pl/praca/starszy-specjalista-starsza-specjalistka-ds-infrastruktury-it-storage-san-katowice,oferta,1003925829\n",
      "title: Starszy Specjalista / Starsza Specjalistka ds. Infrastruktury IT (Storage/SAN)\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['KatowiceKatowice, śląskie']\n",
      "technologies: ['Xorux', 'SAN']\n",
      "responsibilities: ['Zapewnienie ciągłości działania usług biznesowych, infrastruktury i zasobów IT', 'Troubleshooting oraz monitorowanie poprawności działania sieci SAN oraz infrastruktury Storage', 'Bieżąca administracja, konfiguracja i aktualizacja firmware urządzeń Storage, SAN', 'Bezpieczne i wydajne zarządzanie zasobami dyskowymi', 'Konfiguracja oraz utrzymanie połączeń protokołu ISL']\n",
      "requirements: ['Kilkuletnie doświadczenie pracy w IT- szczególnie w obszarze sieci SAN', 'Doświadczenie w realizacji zadań z zakresu bezpieczeństwa informacji tj. administracja środowiskiem SAN i infrastrukturą Storage', 'Umiejętność optymalizacji pracy poprzez wdrażanie i utrzymywanie automatyzacji powtarzalnych procesów', 'Odpowiedzialna postawa oraz zaangażowanie w efektywną realizację celów i zadań']\n",
      "application_link: https://www.pracuj.pl/aplikuj/starszy-specjalista-starsza-specjalistka-ds-infrastruktury-it-storage-san-katowice,oferta,1003925829?apid=1d87510e-b935-486f-9165-b04b8dcda0f8&oca=None&acv=0\n",
      "\n",
      "--- Job Record 277 ---\n",
      "id: 330\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-business-intelligence-warszawa-aleje-jerozolimskie-142b,oferta,1003925802\n",
      "title: Specjalista ds. Business Intelligence\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), młodszy specjalista (Junior)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Aleje Jerozolimskie 142b, Ochota, WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatów']\n",
      "technologies: ['Microsoft Power BI', 'SQL', 'wewnątrz organizacji']\n",
      "responsibilities: ['Rozwój narzędzi IT służących do planowania i raportowania budżetu w Spółce,', 'Automatyzacja procesów raportowych w Spółce,', 'Wizualizacja raportów w oparciu o Power BI Desktop,', 'Wdrażanie i usprawnianie narzędzi raportowych,', 'Optymalizacja, utrzymanie i rozwijanie hurtowni danych,', 'Optymalizacja przetwarzania danych (procesy ETL, raporty, modele semantyczne Power BI),', 'Opracowywanie i utrzymywanie baz danych oraz procesów integracji danych.']\n",
      "requirements: ['Wykształcenie wyższe w obszarze informatyki i ekonometrii, finansów i rachunkowości lub pokrewne,', 'Minimum roczne doświadczenie związane z wykorzystaniem i rozwojem narzędzi IT w obszarze finansów i rachunkowości,', 'Bardzo dobra znajomość pakietu Ms Office, w tym także znajomość VBA,', 'Umiejętność tworzenia scenariuszy i przypadków testowych,', 'Dobra organizacja pracy, komunikatywność, zdolności analityczne i umiejętność wyciągania wniosków z dużych zbiorów danych,', 'Praktyczna wiedza z zakresu optymalizacji zapytań SQL,', 'Znajomość środowiska Power BI Service/Fabric z punktu widzenia projektanta rozwiązań Business Intelligence.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-business-intelligence-warszawa-aleje-jerozolimskie-142b,oferta,1003925802?apid=b9e97165-5edb-48ec-8895-5e940c4841a0&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 278 ---\n",
      "id: 331\n",
      "url: https://www.pracuj.pl/praca/full-stack-developer-ai-genai-solutions-warszawa,oferta,1003910734\n",
      "title: Full Stack Developer (AI & GenAI Solutions)\n",
      "work_location: None\n",
      "validity: valid for 21 daysto 19 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian']\n",
      "technologies: ['JavaScript', 'TypeScript', 'AI', 'agile', 'scrum']\n",
      "responsibilities: ['You’ll be using state-of-the-art AI technologies to build intelligent and innovative solutions.', 'You’ll be developing and implementing high-quality applications that integrate AI, Generative AI, and LLMs (Large Language Models).', 'You’ll own your work, influencing the project’s architecture, strategy, and AI-driven features.', 'You’ll have the freedom to experiment and improve the product by leveraging OpenAI API, different LLM models, and chatbot frameworks.', 'You’ll be joining a team of customer-focused Full Stack Engineers and UX/UI designers, driving one of the most exciting AI-powered products on the Polish IT market.']\n",
      "requirements: ['At least 3 years of programming experience (Senior Level: 5+ years).', 'Strong knowledge of JavaScript or TypeScript (Senior Level: very good knowledge of both).', 'Experience with JS frameworks (we use Express.js and React).', 'Hands-on experience in integrating AI/GenAI models into applications (e.g., OpenAI API, LangChain, Hugging Face, or other LLM providers).', 'Experience in building AI-powered chatbots or virtual assistants.', 'Knowledge of RESTful API design using best practices and design patterns.', 'Understanding of asynchronous programming and experience with message brokers (RabbitMQ, Redis).', 'Ability to write clean, testable, and scalable code.', 'Experience in Agile and Scrum methodologies (we work in Scrum, by the way).', 'Strong collaboration skills, working closely with business stakeholders and AI/ML teams.', '(For Senior Level) Ability to take ownership, driving AI-enhanced services from concept to deployment.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/full-stack-developer-ai-genai-solutions-warszawa,oferta,1003910734?apid=229b5d0f-6ad8-4f3d-85b5-675303a7381d&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 279 ---\n",
      "id: 332\n",
      "url: https://www.pracuj.pl/praca/lead-engineer-gis-data-engineering-krakow-jana-dekerta-24,oferta,1003873423\n",
      "title: Lead Engineer (GIS, Data Engineering)\n",
      "work_location: Company locationJana Dekerta 24, Podgórze, KrakówKraków, Lesser Poland\n",
      "validity: valid for 3 daysto 01 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'manager / supervisor', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Python', 'GitLab', 'GitHub', 'AWS', 'Microsoft Azure', 'JavaScript', 'in house', 'you focus on a single project at a time', 'you develop several projects simultaneously', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'you focus on code maintenance', 'agile']\n",
      "responsibilities: ['Designing and architecting scalable, maintainable, and efficient software solutions', 'Making high-level decisions regarding system design, technology stacks, and software development methodologies', 'Writing and reviewing high-quality, robust, and secure code', 'Ensuring adherence to coding standards, best practices, and company policies', 'Provide technical guidance and mentorship to team members']\n",
      "requirements: ['Expert knowledge of ArcGIS Pro, ArcGIS Enterprise, and ArcGIS Online', 'Experience working with location data (e.g. maps, addresses, POIs, etc. in various schemas (GeoJSON, FGDB, etc.)) and using packages such as ArcPy and GeoPandas', 'Demonstrated expertise in Python programming, with the ability to write clean, efficient, and maintainable code for complex systems and a deep understanding of advanced Python concepts (e.g., asynchronous programming, memory optimization, and design patterns)', 'Experience with common data stores ( relational databases, distributed databases)', 'Experience with CI/CD systems such as GitLab, GitHub,', 'Experience with ETL processes and tooling (AWS Glue, FME, etc.)', 'Excellent problem-solving and analytical abilities', 'Proven experience in data engineering projects', 'Strong communication and collaboration skills in English']\n",
      "application_link: https://www.pracuj.pl/aplikuj/lead-engineer-gis-data-engineering-krakow-jana-dekerta-24,oferta,1003873423?apid=76cf2153-e172-424d-9ca6-5493c81e346d&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 280 ---\n",
      "id: 333\n",
      "url: https://www.pracuj.pl/praca/senior-developer-microsoft-power-platform-warszawa-aleja-jana-pawla-ii-80,oferta,1003910688\n",
      "title: Senior Developer – Microsoft Power Platform\n",
      "work_location: None\n",
      "validity: ważna jeszcze 21 dnido 19 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['aleja Jana Pawła II 80, Śródmieście, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Microsoft Power Platform', 'Dataverse', 'JavaScript', 'TypeScript', 'C#', 'Power Fx', 'u klienta', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'project manager']\n",
      "responsibilities: ['Zaawansowana znajomość Microsoft Power Platform: Power Apps (Canvas Apps, Model-driven Apps), Power Automate, Power BI, Power Virtual Agents.', 'Znajomość Dataverse (Common Data Service - CDS) oraz integracji z innymi źródłami danych (SQL, SharePoint, Azure).', 'Programowanie w JavaScript/TypeScript, C# oraz Power Fx.', 'Znajomość Power Automate Desktop (RPA) oraz tworzenia automatyzacji w chmurze.', 'Integracja Power Platform z systemami zewnętrznymi poprzez API, OData, Web Services.', 'Tworzenie niestandardowych komponentów PCF (PowerApps Component Framework).', 'Umiejętność optymalizacji aplikacji Power Apps oraz zarządzania środowiskami Power Platform.', 'Zarządzanie bezpieczeństwem i uprawnieniami w Power Platform (DLP, role, polityki).', 'Doświadczenie w DevOps dla Power Platform (ALM, Git, CI/CD).', 'Projektowanie dashboardów i raportów w Power BI z uwzględnieniem modelowania danych i DAX.', 'Umiejętność analizy wymagań biznesowych i ich przełożenia na rozwiązania w Power Platform.', 'Zdolność do współpracy z zespołem oraz interesariuszami.', 'Praca zgodnie z metodykami Agile/Scrum.', 'Angielski B2+.']\n",
      "requirements: ['Certyfikaty Microsoft (np. **MB-400, MB-600, PL-).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-developer-microsoft-power-platform-warszawa-aleja-jana-pawla-ii-80,oferta,1003910688?apid=87cff757-ce34-455e-a2aa-881532e75075&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 281 ---\n",
      "id: 334\n",
      "url: https://www.pracuj.pl/praca/data-analyst-warszawa,oferta,1003925697\n",
      "title: Data Analyst\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian']\n",
      "technologies: ['SQL', 'Microsoft Power BI', 'Tableau', 'Looker', 'Python', 'R', 'Spark', 'Hadoop', 'AWS', 'AWS Redshift', 'Google BigQuery', 'Azure Synapse', 'in house', 'agile', 'scrum']\n",
      "responsibilities: ['Collect, clean, and analyze large datasets to identify trends and patterns.', 'Develop and maintain reports, dashboards, and data visualizations using tools like Power BI, Tableau, or Excel.', 'Work with stakeholders to define key performance indicators (KPIs) and deliver insights to support business objectives.', 'Perform statistical analysis to provide insights and recommendations for decision-making.', 'Collaborate with IT and data engineering teams to ensure data integrity and availability.', 'Build and optimize SQL queries to extract and manipulate data from databases.', 'Present findings to stakeholders in a clear and concise manner.', 'Document data analysis processes and methodologies for future reference.']\n",
      "requirements: ['3+ years of experience in data analysis or a related field.', 'Proficiency in SQL for querying and data manipulation.', 'Experience with data visualization tools such as Power BI, Tableau, or Looker.', 'Strong analytical skills and proficiency in Excel (including advanced functions and pivot tables).', 'Knowledge of statistical analysis and data modeling techniques.', 'Familiarity with programming languages such as Python or R for data analysis.', 'Strong problem-solving skills and attention to detail.', 'Excellent communication skills to present complex data in an understandable way.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-analyst-warszawa,oferta,1003925697?apid=189e8feb-e57b-44d8-9723-3e2062fc19a4&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 282 ---\n",
      "id: 335\n",
      "url: https://www.pracuj.pl/praca/data-scientist-warszawa,oferta,1003925695\n",
      "title: Data Scientist\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian']\n",
      "technologies: ['Python', 'R', 'TensorFlow', 'PyTorch', 'Microsoft Power BI', 'Tableau', 'Matplotlib', 'Spark', 'Hadoop', 'Databricks', 'AWS', 'Microsoft Azure', 'Google Cloud Platform', 'in house', 'agile', 'scrum']\n",
      "responsibilities: ['Analyze large and complex datasets to uncover actionable insights and trends.', 'Develop and implement machine learning models and statistical algorithms to solve business challenges.', 'Collaborate with cross-functional teams to understand business requirements and translate them into data science projects.', 'Design and optimize predictive models and recommendation systems.', 'Visualize data insights and present findings to stakeholders through dashboards and reports.', 'Work with data engineers to ensure data availability, quality, and integrity for analysis.', 'Conduct A/B testing and experiments to validate hypotheses and measure the impact of changes.', 'Stay updated with the latest advancements in data science and machine learning technologies.']\n",
      "requirements: ['3+ years of experience in data science or a related field.', 'Proficiency in programming languages such as Python or R for data analysis and modeling.', 'Strong knowledge of machine learning frameworks (e.g., TensorFlow, PyTorch, scikit-learn).', 'Experience with SQL for querying and managing large datasets.', 'Familiarity with data visualization tools like Power BI, Tableau, or Matplotlib.', 'Knowledge of statistical analysis and hypothesis testing.', 'Experience with big data platforms (e.g., Spark, Hadoop, Databricks).', 'Strong problem-solving skills and ability to work in a fast-paced environment.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-warszawa,oferta,1003925695?apid=dcec799d-d485-416c-a545-1891c695de7a&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 283 ---\n",
      "id: 336\n",
      "url: https://www.pracuj.pl/praca/big-data-engineer-warszawa,oferta,1003925694\n",
      "title: Big Data Engineer\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian']\n",
      "technologies: ['Apache Spark', 'Hadoop', 'Kafka', 'Flink', 'Python', 'Scala', 'Java', 'SQL', 'PostgreSQL', 'MongoDB', 'Cassandra', 'AWS', 'Microsoft Azure', 'Google Cloud Platform', 'Apache Airflow', 'Docker', 'Kubernetes', 'Apache Kafka Streams', 'Kinesis', 'Pulsar', 'Microsoft Power BI', 'Tableau', 'Looker', 'in house', 'you have influence on the technological solutions applied', 'you have influence on the product', 'agile', 'scrum']\n",
      "responsibilities: ['Design, develop, and maintain big data pipelines to process and analyze large datasets.', 'Implement data ingestion, processing, and storage solutions using big data frameworks such as Apache Spark, Hadoop, and Kafka.', 'Optimize data pipelines for performance, scalability, and fault tolerance.', 'Collaborate with data scientists, analysts, and other stakeholders to ensure data availability and usability.', 'Develop and maintain data storage solutions such as HDFS, Amazon S3, Google Cloud Storage, or Azure Data Lake.', 'Ensure data quality and integrity through automated testing and validation processes.', 'Monitor and troubleshoot big data infrastructure to ensure optimal performance and reliability.', 'Document technical solutions, workflows, and best practices.']\n",
      "requirements: ['Proficiency in big data technologies such as Apache Spark, Hadoop, Kafka, or Flink.', 'Strong programming skills in languages like Python, Scala, or Java.', 'Experience with SQL and NoSQL databases such as PostgreSQL, MongoDB, or Cassandra.', 'Familiarity with cloud platforms such as AWS, Azure, or Google Cloud, including their big data services (e.g., EMR, BigQuery, Databricks).', 'Knowledge of data modeling, ETL processes, and data pipeline orchestration tools like Apache Airflow, Luigi, or Dagster.', 'Strong understanding of distributed computing principles and parallel processing.', 'Experience with containerization tools such as Docker and orchestration tools like Kubernetes.', 'Strong problem-solving skills and ability to troubleshoot large-scale data systems.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/big-data-engineer-warszawa,oferta,1003925694?apid=303f3845-894f-4588-b346-a51846f578fd&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 284 ---\n",
      "id: 337\n",
      "url: https://www.pracuj.pl/praca/etl-developer-warszawa,oferta,1003925692\n",
      "title: ETL Developer\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian']\n",
      "technologies: ['SQL', 'Snowflake Data Cloud', 'AWS Glue', 'Azure Data Factory', 'Google Dataflow', 'Python', 'Shell', 'Bash', 'Spark', 'Kafka', 'Hadoop', 'MongoDB', 'Cassandra', 'Git', 'in house', 'you have influence on the technological solutions applied', 'you have influence on the product', 'agile', 'scrum']\n",
      "responsibilities: ['Design, develop, and optimize ETL pipelines to extract, transform, and load data from multiple data sources.', 'Collaborate with data architects and business analysts to gather and understand data requirements.', 'Implement and maintain data integration workflows using ETL tools such as Informatica, Talend, SSIS, or Apache NiFi.', 'Perform data validation and quality checks to ensure data integrity and accuracy.', 'Troubleshoot and resolve issues related to ETL processes and data flows.', 'Monitor and enhance the performance of ETL jobs to meet business SLA requirements.', 'Maintain and document technical solutions, including data mappings, workflows, and procedures.', 'Work closely with other data team members to support data warehouse and data lake initiatives.']\n",
      "requirements: ['Proficiency in SQL for querying and transforming data.', 'Hands-on experience with ETL tools such as Informatica, Talend, SSIS, or similar.', 'Strong knowledge of data modeling techniques, including star schema and snowflake schema.', 'Experience in data integration and data warehousing concepts.', 'Familiarity with cloud platforms (e.g., AWS Glue, Azure Data Factory, Google Dataflow) for ETL processes.', 'Strong problem-solving skills and the ability to troubleshoot complex data issues.', 'Experience with scripting languages such as Python, Shell, or Bash for automation.', 'Excellent communication and collaboration skills to work effectively with cross-functional teams.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/etl-developer-warszawa,oferta,1003925692?apid=05aa45c9-b444-4a58-9fe2-ed9c039b356a&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 285 ---\n",
      "id: 338\n",
      "url: https://www.pracuj.pl/praca/power-bi-developer-warszawa,oferta,1003925690\n",
      "title: Power BI Developer\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian']\n",
      "technologies: ['Microsoft Power BI', 'SQL', 'Power Query', 'Microsoft Azure', 'Python', 'R', 'Databricks', 'Spark', 'Git', 'in house', 'you have influence on the technological solutions applied', 'you have influence on the product', 'agile', 'scrum']\n",
      "responsibilities: ['Design, develop, and deploy Power BI dashboards and reports based on business requirements.', 'Connect to various data sources, including SQL databases, Excel, and cloud services, to create comprehensive BI solutions.', 'Develop and maintain Power BI datasets, dataflows, and queries to ensure efficient and accurate data models.', 'Implement DAX calculations and measures to enable advanced data analysis and insights.', 'Optimize Power BI reports for performance and usability, ensuring quick load times and responsive user experiences.', 'Collaborate with stakeholders to gather requirements and translate them into actionable dashboards and reports.', 'Perform data analysis and create meaningful visualizations to support business strategies and operational decisions.', 'Conduct user training and provide ongoing support for Power BI tools and solutions.', 'Stay updated with the latest Power BI features and best practices to continuously enhance BI solutions.']\n",
      "requirements: ['Proficiency in Power BI development, including report building, data modeling, and visualization.', 'Strong knowledge of DAX (Data Analysis Expressions) for creating complex calculations and measures.', 'Experience with SQL for querying and transforming data.', 'Familiarity with data integration tools and methods for connecting Power BI to various data sources.', 'Strong understanding of data modeling concepts, including star schema and snowflake schema.', 'Experience with Power BI Service for publishing and managing dashboards.', 'Knowledge of data governance and security in Power BI, including row-level security (RLS).', 'Strong analytical and problem-solving skills.', 'Excellent communication and collaboration abilities.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/power-bi-developer-warszawa,oferta,1003925690?apid=01031c5d-ca46-4308-b0dc-d1ed23a71d27&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 286 ---\n",
      "id: 339\n",
      "url: https://www.pracuj.pl/praca/business-intelligence-developer-warszawa,oferta,1003925689\n",
      "title: Business Intelligence Developer\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian']\n",
      "technologies: ['SQL', 'Microsoft Power BI', 'Tableau', 'QlikView', 'Snowflake', 'Redshift', 'Azure Synapse', 'Python', 'R', 'AWS', 'Microsoft Azure', 'Google Cloud Platform', 'Spark', 'Hadoop', 'Databricks', 'Git', 'in house', 'you have influence on the technological solutions applied', 'you have influence on the product', 'agile', 'scrum']\n",
      "responsibilities: ['Design, develop, and maintain BI solutions, including dashboards, reports, and data visualizations.', 'Collaborate with stakeholders to gather requirements and translate them into technical BI solutions.', 'Build and optimize ETL pipelines to ensure efficient data flow from various sources to data warehouses.', 'Develop and maintain data models to support reporting and analytics needs.', 'Create interactive dashboards and reports using tools like Power BI, Tableau, or QlikView.', 'Perform data analysis to provide actionable insights and support decision-making processes.', 'Ensure data accuracy and consistency through data validation and quality checks.', 'Monitor and improve the performance of BI systems and applications.', 'Stay updated with the latest BI tools, technologies, and best practices.']\n",
      "requirements: ['Proficiency in SQL for querying and managing data in relational databases.', 'Experience with ETL tools such as Informatica, Talend, or SSIS.', 'Hands-on experience with BI tools like Power BI, Tableau, QlikView, or similar.', 'Strong understanding of data modeling concepts, including star schema and snowflake schema.', 'Familiarity with data warehousing technologies such as Snowflake, Redshift, or Azure Synapse.', 'Experience with scripting languages like Python or R for data analysis and automation.', 'Strong problem-solving skills and attention to detail.', 'Good communication and collaboration abilities.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/business-intelligence-developer-warszawa,oferta,1003925689?apid=ad10d371-02c3-48af-aa7e-27f8cf9e4362&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 287 ---\n",
      "id: 340\n",
      "url: https://www.pracuj.pl/praca/storage-administrator-warszawa,oferta,1003925606\n",
      "title: Storage Administrator\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian']\n",
      "technologies: ['Veeam', 'Commvault', 'Rubrik', 'AWS', 'Microsoft Azure', 'Google Cloud Platform', 'Kubernetes', 'Ansible', 'Terraform', 'PowerShell', 'in house', 'agile', 'scrum']\n",
      "responsibilities: ['Install, configure, and maintain enterprise storage systems (e.g., NetApp, Dell EMC, HPE, Pure Storage).', 'Manage SAN (Storage Area Network), NAS (Network Attached Storage), and other storage technologies.', 'Monitor and optimize storage performance and capacity, ensuring high availability and reliability.', 'Implement and maintain data backup, replication, and disaster recovery solutions.', 'Troubleshoot and resolve issues related to storage systems, connectivity, and data access.', 'Collaborate with IT teams to ensure seamless integration of storage solutions with servers, databases, and applications.', 'Perform regular storage system upgrades, patches, and firmware updates.', 'Document storage configurations, procedures, and best practices.', 'Ensure compliance with data retention and security policies.']\n",
      "requirements: ['3+ years of experience in storage administration or a related field.', 'Proficiency in managing SAN, NAS, and cloud-based storage solutions.', 'Strong knowledge of storage protocols such as iSCSI, Fibre Channel, NFS, and CIFS/SMB.', 'Experience with backup and replication tools (e.g., Veeam, Commvault, Rubrik).', 'Hands-on experience with monitoring tools for storage systems.', 'Familiarity with Linux and Windows Server environments.', 'Strong problem-solving skills and the ability to work under pressure.', 'Excellent communication and teamwork skills.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/storage-administrator-warszawa,oferta,1003925606?apid=66c8cd98-1a07-4fde-9bae-a825f9db92d5&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 288 ---\n",
      "id: 341\n",
      "url: https://www.pracuj.pl/praca/senior-nlp-engineer-ai-warszawa-saska-103,oferta,1003904468\n",
      "title: Senior NLP Engineer (AI)\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 17 dnido 15 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Python', 'AWS', 'TypeScript', 'wewnątrz organizacji', 'u klienta', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt']\n",
      "responsibilities: ['Rozwój i wdrażanie praktycznych rozwiązań NLP oraz Generative AI dla sektora finansowego']\n",
      "requirements: ['Doświadczenie w NLP i Generative AI budowanie komponentów NLP i wdrażanie ich w produktach', 'Silne umiejętności w Pythonie pisanie, przegląd i utrzymanie kodu produkcyjnego dla aplikacji NLP', 'Doświadczenie z AWS zwłaszcza SageMaker i Jupyter Notebooks', 'Doświadczenie w DevOps i CI/CD', 'Doświadczenie w wdrażaniu ML i LLM w środowisku produkcyjnym', 'Praca w serverless framework w AWS', 'Umiejętność przekładania problemów biznesowych na rozwiązania AI', 'BSc w dziedzinie Informatyki lub pokrewnej', 'Doskonałe umiejętności komunikacyjne', 'Samodzielność i proaktywność', 'Pasja do NLP i AI, chęć ciągłego rozwoju', 'Umiejętność pracy zespołowej']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-nlp-engineer-ai-warszawa-saska-103,oferta,1003904468?apid=fa3c6aa3-199e-49ae-9b15-77c280e36bc2&oca=None&acv=0\n",
      "\n",
      "--- Job Record 289 ---\n",
      "id: 342\n",
      "url: https://www.pracuj.pl/praca/ai-expert-senior-business-analyst-warszawa,oferta,1003910516\n",
      "title: AI Expert - Senior Business Analyst\n",
      "work_location: None\n",
      "validity: valid for 21 daysto 19 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior), expert\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian']\n",
      "technologies: ['AI', 'agile', 'scrum']\n",
      "responsibilities: ['Lead and conduct business analysis for AI-related initiatives, ensuring alignment between business objectives and technological capabilities.', 'Run workshops with business stakeholders, technical teams, and UX designers to define requirements, optimize processes, and identify AI opportunities.', 'Translate business needs into AI-enhanced user stories, technical specifications, and process documentation.', 'Identify opportunities for process automation, optimization, and innovation using AI, GenAI, and ML solutions.', 'Act as a trusted advisor, guiding the organization on the practical application of AI in business processes.', 'Support manual testing and validate AI-driven functionalities before deployment.', 'Assist in resolving production issues by analyzing data, identifying root causes, and working with IT teams on AI-powered improvements.']\n",
      "requirements: ['At least 5 years of experience in business analysis, preferably in projects involving digital transformation or AI implementation.', 'Strong understanding of AI technologies, including Generative AI and Machine Learning applications in business environments.', 'Experience in translating business requirements into AI-driven solutions.', 'Ability to identify, map, and optimize business processes using AI-driven automation.', 'Strong communication skills and the ability to engage with diverse stakeholders.', 'Proficiency in English (spoken and written) to effectively collaborate in an international environment.', 'Familiarity with Agile/Scrum methodologies—you understand how to work iteratively and deliver value in sprints.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-expert-senior-business-analyst-warszawa,oferta,1003910516?apid=2118f5f8-8c57-490e-ad28-311320487e21&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 290 ---\n",
      "id: 343\n",
      "url: https://www.pracuj.pl/praca/senior-analytics-consultant-warszawa-saska-103,oferta,1003867888\n",
      "title: Senior Analytics Consultant\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze dzieńdo 27 lutego 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Looker', 'BigQuery', 'Hive', 'Oracle', 'Python', 'dbt', 'FlinkSQL', 'wewnątrz organizacji', 'u klienta', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt']\n",
      "responsibilities: ['Zbieranie wymagań biznesowych dla rozwiązań analitycznych (raporty, analizy).', 'Doradztwo w zakresie wskaźników monitorowania wydajności biznesowej.', \"Tworzenie i monitorowanie pipeline'ów danych oraz implementacja transformacji danych (głównie SQL).\", 'Projektowanie i wdrażanie modeli danych w narzędziach BI.', 'Budowanie raportów i dashboardów.', 'Walidacja i testowanie danych.', 'Komunikowanie wyników biznesowi oraz wsparcie w ich wykorzystaniu.', 'Edukacja w zakresie znajomości danych w organizacji.', 'Śledzenie trendów i najlepszych praktyk w obszarze inżynierii analitycznej.']\n",
      "requirements: ['Minimum 5 lat doświadczenia w pracy z Looker.', 'Bardzo dobra znajomość SQL oraz przynajmniej jednego systemu DBMS (np. BigQuery, Hive, Oracle).', 'Umiejętność mapowania wymagań biznesowych na produkty analityczne (raporty, analizy).', 'Doświadczenie w projektowaniu struktur danych i relacji, dostosowanych do wymagań analitycznych.', 'Znajomość przynajmniej jednego języka programowania (preferowany Python).', 'Umiejętność pracy z różnorodnymi zestawami danych i źródłami danych.', 'Znajomość najlepszych praktyk wizualizacji danych oraz storytellingu opartego na danych.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-analytics-consultant-warszawa-saska-103,oferta,1003867888?apid=95c33250-561e-4a6a-a91b-cf50d2adf13f&oca=None&acv=0\n",
      "\n",
      "--- Job Record 291 ---\n",
      "id: 344\n",
      "url: https://www.pracuj.pl/praca/data-engineer-warszawa-saska-103,oferta,1003867887\n",
      "title: Data Engineer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze dzieńdo 27 lutego 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Python', 'Scala', 'Spark', 'Hadoop', 'Kubernetes', 'wewnątrz organizacji', 'u klienta', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt']\n",
      "responsibilities: ['Rozwój i utrzymanie platform ETL oraz przetwarzania danych (Python, Scala, Spark, HDFS, Hive).', 'Rozwój aplikacji wspierających dostęp do danych dla użytkowników biznesowych (Airflow, JupyterHub, Trino, Superset, MLFlow) z wykorzystaniem Kubernetes, Docker, ArgoCD.', 'Automatyzacja procesów CI/CD (GitLab-CI).', 'Monitorowanie platformy (Prometheus).', 'Badania, rozwój i utrzymanie komponentów platformy.', 'Realizacja polityk strategicznych związanych z technologiami oraz organizacją pracy.']\n",
      "requirements: ['Minimum 5 lat doświadczenia jako Data Engineer.', 'Biegłość w programowaniu w Pythonie oraz Scala.', 'Doświadczenie w pracy z Spark oraz systemami przesyłania komunikatów.', 'Znajomość Hadoop.', 'Praktyczna znajomość Kubernetes.', 'Solidne umiejętności programistyczne i znajomość najlepszych praktyk inżynierii oprogramowania.', 'Doświadczenie w pracy z systemami kontroli wersji (preferowany Git).', 'Umiejętność prowadzenia rozmów z klientami, identyfikowania ich potrzeb oraz proponowania rozwiązań.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-warszawa-saska-103,oferta,1003867887?apid=ca005f99-e44c-4e8f-ac47-417cf3047859&oca=None&acv=0\n",
      "\n",
      "--- Job Record 292 ---\n",
      "id: 345\n",
      "url: https://www.pracuj.pl/praca/starszy-programista-sql-warszawa,oferta,1003910379\n",
      "title: Starszy Programista SQL\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 21 dnido 19 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['SQL', 'PL/SQL', 'PostgreSQL', 'Oracle', 'Git', 'u klienta', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'project manager']\n",
      "responsibilities: ['Projektowanie i optymalizacja zapytań SQL', 'Tworzenie i optymalizacja procedur składowanych w bazach danych PostgreSQL lub Oracle', 'Praca nad strukturami bazodanowymi i integracjami w Hurtowniach Danych', 'Udział w tworzeniu procesów ETL i ich implementacja', 'Praca w zespole w środowisku CI/CD', 'Wykorzystanie systemów kontroli wersji (GIT) do zarządzania kodem']\n",
      "requirements: ['Minimum 5 lat doświadczenia w programowaniu w języku SQL lub PL/SQL', 'Doświadczenie w optymalizacji zapytań SQL (minimum 1 projekt)', 'Doświadczenie w tworzeniu procedur składowanych w bazach danych PostgreSQL lub Oracle', 'Doświadczenie w pracy z Hurtowniami Danych', 'Doświadczenie w projektowaniu struktur bazodanowych', 'Wykształcenie wyższe techniczne']\n",
      "application_link: https://www.pracuj.pl/aplikuj/starszy-programista-sql-warszawa,oferta,1003910379?apid=70969f1e-2066-41ee-9deb-5ab1d146fee8&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 293 ---\n",
      "id: 346\n",
      "url: https://www.pracuj.pl/praca/marketing-automation-developer-warszawa-komitetu-obrony-robotnikow-43,oferta,1003925401\n",
      "title: Marketing Automation Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Komitetu Obrony Robotników 43, Włochy, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Adobe Campaign', 'IBM Unica', 'Salesforce', 'GetResponse', 'Oracle Eloqua', 'Synerise', 'SQL', 'Python']\n",
      "responsibilities: ['Rekomendacje i implementacja testów A/B', 'Współpraca z inżynierami w zakresie utrzymania i rozbudowy modelu danych zorientowanych wokół Klienta', 'Współpraca z inżynierami oraz IT w zakresie utrzymania i rozwoju procesów przetwarzania danych do kampanii w chmurze', 'Współpraca z marketingiem w zakresie projektowania kampanii', 'Optymalizacja kampanii z wykorzystaniem modeli predykcyjnych', 'Rozwój systemu alertowania kampanii i procesów zależnych', 'Drążenie danych i analizy w obszarze zarządzania wartością Klientów', 'Rekomendacje do budowy strategii komunikacji oraz segmentacji i personalizacji', 'Udział w projektach firmowych związanych z integracją danych zorientowanych wokół Klienta', 'Tworzenie dokumentacji kampanii i procesów']\n",
      "requirements: ['Dyspozycyjność do pracy z biura z Warszawy min. 3 dni w tygodniu', 'Co najmniej 1,5 roku doświadczenia w budowie scenariuszy kampanii w narzędziach marketing automation (np. Adobe Campaign, IBM Unica, Salesforce, GetResponse, Oracle Eloqua, Synerise)', 'Wykształcenie wyższe, mile wdziane kierunki ścisłe takie jak Matematyka, Informatyka, Big Data, Data Science, Ekonometria lub pokrewne', 'Dobra znajomość SQL', 'Znajomość języka angielskiego na poziomie swobodnej komunikacji w mowie i piśmie (min. B2)', 'Wysoka samodzielność i odpowiedzialność', 'Umiejętności analityczne - logiczne myślenie, wnioskowanie, strukturyzowanie informacji', 'Znajomość zagadnień związanych z budową i zastosowaniem modeli predykcyjnych', 'Zdolności techniczne/programistyczne', 'Samodzielność, ale też umiejętność pracy z ludźmi zarówno z obszarów biznesowych jak i IT']\n",
      "application_link: https://www.pracuj.pl/aplikuj/marketing-automation-developer-warszawa-komitetu-obrony-robotnikow-43,oferta,1003925401?apid=3edf9a66-dfe9-4014-abe9-a11e1897f28c&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 294 ---\n",
      "id: 347\n",
      "url: https://www.pracuj.pl/praca/biostatystyk-bialystok,oferta,1003925394\n",
      "title: Biostatystyk\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['BiałystokBiałystok, podlaskie', 'praca stacjonarna']\n",
      "technologies: ['Python', 'R']\n",
      "responsibilities: ['Projektowanie analiz statystycznych: opracowywanie i realizacja zaawansowanych modeli statystycznych na potrzeby badań klinicznych i eksperymentów medycznych realizowanych na UMB.', 'Analiza danych wielowymiarowych: praca z danymi genomicznymi, proteomicznymi, metabolomicznymi i klinicznymi, w tym ich integracja w celu opracowania modeli predykcyjnych.', 'Wsparcie naukowców: doradztwo w zakresie projektowania badań, wyboru metod statystycznych oraz interpretacji wyników.', 'Opracowywanie publikacji naukowych: analiza danych do artykułów naukowych, raportów i prezentacji.', 'Rozwój narzędzi analitycznych: współpraca w projektowaniu i implementacji rozwiązań usprawniających analizę danych w projektach badawczych.']\n",
      "requirements: ['Wykształcenie wyższe (preferowane kierunki: biostatystyka, matematyka, statystyka, epidemiologia lub pokrewne).', 'Bardzo dobra znajomość metod statystycznych stosowanych w badaniach biomedycznych.', 'Dobra znajomość języka angielskiego w mowie i piśmie na poziomie komunikatywnym.', 'Umiejętność pracy z narzędziami statystycznymi i programowania (np. R, Python).', 'Doświadczenie w pracy z danymi wielowymiarowymi oraz w analizie danych z zakresu genomiki, proteomiki i metabolomiki.', 'Mile widziane doświadczenie w integracji danych omicznych z klinicznymi oraz opracowywaniu modeli predykcyjnych.', 'Umiejętność pracy w zespole, komunikatywność oraz otwartość na interdyscyplinarną współpracę.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/biostatystyk-bialystok,oferta,1003925394?apid=7f8f7b06-4366-4667-a139-93dd0811f87c&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 295 ---\n",
      "id: 348\n",
      "url: https://www.pracuj.pl/praca/bioinformatyk-bialystok,oferta,1003925393\n",
      "title: Bioinformatyk\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['BiałystokBiałystok, podlaskie', 'praca stacjonarna']\n",
      "technologies: ['Python', 'R']\n",
      "responsibilities: ['Analiza danych z zakresu genomiki, w tym danych z technologii sekwencjonowania nowej generacji (NGS), obejmujących m.in. sekwencjonowanie całogenomowe (Whole Genome Sequencing) oraz transkryptomikę (RNA-Seq).', 'Przetwarzanie i analiza danych proteomicznych, metabolomicznych oraz danych z mikromacierzy.', 'Integracja danych omicznych (genomicznych, proteomicznych i metabolomicznych) z danymi klinicznymi w celu opracowania modeli predykcyjnych.', 'Opracowywanie i wdrażanie zaawansowanych algorytmów analizy danych omicznych z', 'wykorzystaniem narzędzi bioinformatycznych.', 'Współpraca z naukowcami w zakresie przetwarzania i interpretacji danych badawczych.', 'Rozwój i implementacja narzędzi analitycznych wspierających projekty badawcze w obszarze', 'medycyny spersonalizowanej.']\n",
      "requirements: ['Zainteresowane osoby prosimy o przesłanie CV oraz listu motywacyjnego w terminie do 15.03.2025 r.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/bioinformatyk-bialystok,oferta,1003925393?apid=18d7f0fe-d465-4dbe-8c5f-3149f203dc05&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 296 ---\n",
      "id: 349\n",
      "url: https://www.pracuj.pl/praca/specjalista-specjalistka-ds-wsparcia-bi-power-bi-administrator-jaslo,oferta,1003895657\n",
      "title: Specjalista / Specjalistka ds. Wsparcia BI / Power BI Administrator\n",
      "work_location: None\n",
      "validity: ważna jeszcze 14 dnido 12 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['JasłoJasło, podkarpackie', 'praca stacjonarna']\n",
      "technologies: ['SQL']\n",
      "responsibilities: ['zarządzanie infrastrukturą Power BI (workspace’y, uprawnienia, licencje)', 'monitorowanie wydajności raportów i optymalizacja środowiska Power BI', 'tworzenie standardów i najlepszych praktyk w zakresie wizualizacji i modelowania danych', 'wspieranie zespołów Self-Service BI w korzystaniu z Analysis Services jako źródła danych', 'organizacja szkoleń i przygotowywanie materiałów edukacyjnych dla użytkowników Power BI', 'monitorowanie wykorzystania Power BI w organizacji (np. analiza logów, eliminacja duplikatów raportów)']\n",
      "requirements: ['zaawansowana znajomość Power BI, w tym administracji środowiskiem i optymalizacji raportów', 'znajomość DAX oraz umiejętność projektowania modeli opartych o Analysis Services', 'minimum podstawowa znajomość MS SQL', 'znajomość języka angielskiego umożliwiająca swobodne porozumiewanie się w mowie i piśmie', 'umiejętność współpracy z zespołami interdyscyplinarnymi w celu dostosowania rozwiązań BI do potrzeb organizacji', 'komunikatywność, umiejętność jasnego wyrażania swoich pomysłów', 'otwartość na naukę nowych narzędzi, metod i rozwiązań analitycznych']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-specjalistka-ds-wsparcia-bi-power-bi-administrator-jaslo,oferta,1003895657?apid=f5344cca-2fa3-41f5-80fa-83632eee94bf&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 297 ---\n",
      "id: 350\n",
      "url: https://www.pracuj.pl/praca/specjalista-specjalistka-ds-modelowania-danych-bi-developer-jaslo,oferta,1003895650\n",
      "title: Specjalista / Specjalistka ds. Modelowania Danych / BI Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 14 dnido 12 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['JasłoJasło, podkarpackie', 'praca stacjonarna']\n",
      "technologies: ['Python', 'SQL', 'Scala', 'Power Bi']\n",
      "responsibilities: ['projektowanie, rozwój i utrzymanie centralnych modeli danych (Databricks SQL, Analysis Services)', 'tworzenie złożonych raportów Power BI w oparciu o dane z modeli Analysis Services i MS SQL, Databricks SQL', 'przygotowywanie specyfikacji technicznych dla procesów ETL realizowanych przez zewnętrznych dostawców', 'współpraca z Zespołami Architektury i Infrastruktury i  w zakresie optymalizacji procesów przetwarzania danych w hurtowni danych', 'zapewnienie jakości i spójności danych w modelach analitycznych']\n",
      "requirements: ['znajomość Power BI, DAX', 'znajomość SQL, Python, Scala', 'umiejętność tworzenia specyfikacji technicznych i dokumentacji projektowej', 'znajomość języka angielskiego umożliwiająca swobodne porozumiewanie się w mowie i piśmie', 'umiejętność współpracy z zespołami interdyscyplinarnymi w celu dostosowania rozwiązań BI do potrzeb organizacji', 'komunikatywność, umiejętność jasnego wyrażania swoich pomysłów', 'otwartość na zdobywanie nowych umiejętności w obszarze narzędzi i technologii BI oraz Data Engineering']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-specjalistka-ds-modelowania-danych-bi-developer-jaslo,oferta,1003895650?apid=8532ebb6-6d82-4829-a17a-1fdc08ea9475&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 298 ---\n",
      "id: 351\n",
      "url: https://www.pracuj.pl/praca/data-ml-platform-tech-lead-data-engineer-warszawa,oferta,1003910313\n",
      "title: Data/ML Platform Tech Lead / Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 21 daysto 19 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian']\n",
      "technologies: ['Kafka', 'Python', 'scrum']\n",
      "responsibilities: ['You’ll be leading cutting-edge tech initiatives in the Azure Cloud, using state-of-the-art tools and methodologies.', 'You’ll own your work, influencing the project’s architecture, strategy, and direction.', 'You’ll play a key role in shaping MLOps/LLMOps pipelines, improving data availability, and ensuring seamless collaboration between analytics and IT teams.', 'You’ll be joining a team of customer-focused Data Analysts, Data Engineers, and Machine Learning Engineers to drive one of the most exciting products on the Polish insurance market.']\n",
      "requirements: ['Proven experience as a Tech Lead or a strong aspiration to take on a leadership role in cloud-based data and AI projects.', 'Commercial experience in building Data, Analytics, or MLOps/LLMOps Platforms in the Cloud (Azure preferred).', 'Hands-on expertise in machine learning model lifecycle management, deployment, and monitoring in a cloud environment.', 'Practical knowledge of open-source Big Data tools (e.g., Kafka, Airflow, Presto, Spark).', '+3 years of programming experience (Python preferred, but Java/Scala experience with a willingness to learn Python is also welcome).', 'Experience in database development, data model design, and distributed systems.', 'Strong business acumen and collaboration skills, with a proactive approach to addressing service needs and operational demands.', 'Understanding of Agile and Scrum principles (we work in Scrum, by the way).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-ml-platform-tech-lead-data-engineer-warszawa,oferta,1003910313?apid=f1bf2fb4-40f9-42d2-b9d5-cedfc4966ba3&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 299 ---\n",
      "id: 352\n",
      "url: https://www.pracuj.pl/praca/analityk-danych-data-analyst-warszawa-ostrobramska-75c,oferta,1003906981\n",
      "title: Analityk Danych (Data Analyst)\n",
      "work_location: None\n",
      "validity: ważna jeszcze 18 dnido 16 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Ostrobramska 75C, Praga-Południe, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Google Analytics', 'SQL', 'Python', 'wewnątrz organizacji']\n",
      "responsibilities: ['Analiza danych i tworzenie raportów wspierających procesy biznesowe,', 'Projektowanie oraz wdrażanie dashboardów w Looker Studio i Power BI,', 'Przetwarzanie i optymalizacja dużych zbiorów danych (BigQuery, SQL Server),', 'Tworzenie modeli analitycznych oraz prognozowanie trendów na podstawie danych,', 'Współpraca z zespołami marketingu, sprzedaży oraz IT w zakresie analizy danych,', 'Automatyzacja procesów raportowania oraz usprawnianie metod analizy danych.']\n",
      "requirements: ['Minimum 2-3 lata doświadczenia na podobnym stanowisku,', 'Bardzo dobra znajomość narzędzi Microsoft (Excel, Power BI, SQL Server),', 'Doświadczenie w pracy z narzędziami Google (BigQuery, Looker Studio, Google Analytics),', 'Znajomość SQL oraz umiejętność optymalizacji zapytań,', 'Umiejętność przetwarzania dużych zbiorów danych i wyciągania z nich wniosków,', 'Zdolność do pracy zespołowej oraz komunikatywność,', 'SQL, Azure, Python, Databricks.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/analityk-danych-data-analyst-warszawa-ostrobramska-75c,oferta,1003906981?apid=2d4fcc21-ed19-4e50-971b-7a4497efc91e&oca=None&acv=0\n",
      "\n",
      "--- Job Record 300 ---\n",
      "id: 353\n",
      "url: https://www.pracuj.pl/praca/lider-zespolu-data-analyst-warszawa-ostrobramska-75c,oferta,1003906982\n",
      "title: Lider Zespołu Data Analyst\n",
      "work_location: None\n",
      "validity: ważna jeszcze 18 dnido 16 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: None\n",
      "work_arrangement: praca zdalna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Ostrobramska 75C, Praga-Południe, WarszawaWarszawa, mazowieckie', 'kierownik / koordynator, menedżer']\n",
      "technologies: ['Google Analytics', 'SQL', 'Python', 'R', 'wewnątrz organizacji']\n",
      "responsibilities: ['Zarządzanie zespołem analityków danych i koordynowanie ich pracy,', 'Analiza danych i tworzenie raportów wspierających procesy produkcyjne i biznesowe,', 'Projektowanie oraz wdrażanie dashboardów w Power BI i Looker Studio,', 'Przetwarzanie i optymalizacja dużych zbiorów danych (SQL Server, BigQuery),', 'Tworzenie strategii analitycznych i usprawnianie procesów raportowania,', 'Ścisła współpraca z zespołami produkcji, IT oraz zarządem,', 'Wdrażanie najlepszych praktyk w zakresie analizy danych oraz automatyzacja procesów raportowania.']\n",
      "requirements: ['Minimum 5 lat doświadczenia w analizie danych, w tym 2 lata na stanowisku liderskim,', 'Bardzo dobra znajomość narzędzi Microsoft (Excel, Power BI, SQL Server),', 'Doświadczenie w pracy z narzędziami Google (BigQuery, Looker Studio, Google Analytics),', 'Zaawansowana znajomość SQL i umiejętność optymalizacji zapytań,', 'Umiejętność zarządzania zespołem i prowadzenia projektów analitycznych,', 'Silne umiejętności komunikacyjne i zdolność do współpracy międzydziałowej,', 'SQL, Azure, Python, Databricks.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/lider-zespolu-data-analyst-warszawa-ostrobramska-75c,oferta,1003906982?apid=6738a1f5-7867-4c52-a6f2-3ed5dd305bef&oca=None&acv=0\n",
      "\n",
      "--- Job Record 301 ---\n",
      "id: 354\n",
      "url: https://www.pracuj.pl/praca/senior-ml-engineer-remote-warszawa,oferta,1003925332\n",
      "title: Senior ML Engineer (Remote)\n",
      "work_location: Company locationWarszawaWarszawa, Masovian\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'Запрошуємо працівників з України']\n",
      "technologies: ['Python', 'Docker', 'Kubernetes']\n",
      "responsibilities: ['Build a scalable data platform to handle large scientific datasets, research papers, and multi-modal training data for LLM fine-tuning', 'Design efficient data pipelines for data ingestion, preprocessing, feature engineering, embedding generation, vector storage, retrieval, and model training/evaluation', 'Implement monitoring, logging, and debugging infrastructure for ML operations', 'Ensure reproducible training environments for current and future LLM experiments']\n",
      "requirements: ['5+ years of experience in developing and deploying AI and machine learning models, with a deep understanding of algorithms and data structures', 'A degree in Computer Science, Engineering, Mathematics, or a related field (or equivalent experience)', 'High proficiency in Python', 'Deep understanding and knowledge of database systems, docker and Kubernetes', 'Deep knowledge of building data platforms that manage large-scale datasets', 'History of distributed ML training infrastructure management', 'Solid understanding of system architecture, particularly in building AI-driven systems that are efficient, secure, and scalable']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-ml-engineer-remote-warszawa,oferta,1003925332?apid=918006b8-2ab8-4691-a02b-6287ce0f63e6&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 302 ---\n",
      "id: 355\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-katowice-wroclawska-54,oferta,1003904240\n",
      "title: Senior Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 17 daysto 15 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Wrocławska 54, KatowiceKatowice, Silesian']\n",
      "technologies: ['SQL', 'Airflow', 'Python', 'in house', 'agile', 'scrum']\n",
      "responsibilities: ['Develop and maintain ETL data pipelines in DBT and Airflow, transforming raw data from multiple sources into structured datasets aligned with business logic.', 'Prepare source datasets for reporting in Power BI and other analytical tools, ensuring data accuracy and consistency.', 'Optimize SQL queries to improve database performance.', 'Work alongside the Data Engineering team to maintain consistency and best practices across data processing and reporting.', 'Potentially mentor and support future team members as the team grows.']\n",
      "requirements: ['Minimum 3 years of experience as a Data Engineer or in a similar role.', 'Strong SQL skills, including the ability to write complex queries and optimize database performance.', 'Proficiency in Python for data processing, scripting, and automation.', 'Hands-on experience developing and managing data pipelines, preferably using Airflow and DBT.', 'Fluent English communication skills (working in an international environment).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-katowice-wroclawska-54,oferta,1003904240?apid=376759ad-14b6-407b-a067-bcd14a565294&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 303 ---\n",
      "id: 356\n",
      "url: https://www.pracuj.pl/praca/full-stack-developer-poznan-mikolaja-reja-2,oferta,1003925207\n",
      "title: Full Stack Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: umowa zlecenie, kontrakt B2B\n",
      "employment_type: pełny etat, część etatu\n",
      "position: specjalista (Mid / Regular), młodszy specjalista (Junior)\n",
      "work_arrangement: praca stacjonarna, praca zdalna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Mikołaja Reja 2, Jeżyce, PoznańPoznań, wielkopolskie', 'Szukamy wielu kandydatówwakaty: 3']\n",
      "technologies: ['NextJS', 'TailwindCSS', 'MongoDB', 'FastAPI', 'Python', 'TypeScript', 'Git', 'Torch/PyTorch']\n",
      "responsibilities: ['Tworzenie i rozwój aplikacji: Projektowanie i implementacja interaktywnych interfejsów użytkownika przy użyciu Next.js i Tailwind CSS, a także budowa solidnych rozwiązań backendowych opartych na FastAPI. Integracja API i systemów: Projektowanie, wdrażanie i utrzymywanie RESTful API, które umożliwiają komunikację pomiędzy różnymi modułami systemu oraz integrację z zewnętrznymi usługami.', 'Wsparcie rozwiązań AI: Współpraca przy budowie produktów wykorzystujących LLM i modele generatywne, które automatyzują procesy biznesowe.', 'Web scraping: Opracowywanie rozwiązań do pozyskiwania danych z różnych źródeł (mile widziane), co może wspierać procesy treningowe modeli generatywnych.']\n",
      "requirements: ['Znajomość NextJS/React, FastAPI oraz git.', 'Umiejętność\\xa0integracji API']\n",
      "application_link: https://www.pracuj.pl/aplikuj/full-stack-developer-poznan-mikolaja-reja-2,oferta,1003925207?apid=03775b73-de80-4db3-b135-33e4457e54c9&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 304 ---\n",
      "id: 357\n",
      "url: https://www.pracuj.pl/praca/internship-python-for-data-science-and-processing-intern-warszawa-plac-europejski-1,oferta,1003925097\n",
      "title: Internship - Python for Data Science and Processing Intern\n",
      "work_location: None\n",
      "validity: valid for a monthto 26 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time, part time\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['plac Europejski 1, Wola, WarszawaWarszawa, Masovian', 'contract of mandate', 'trainee']\n",
      "technologies: ['Python', 'Golang', 'JS', 'TS', 'vue.js']\n",
      "responsibilities: ['Developing scrapers and ML based scripts', 'Actively participate in Code Review process', 'Test your solutions well and assure software quality', 'Focus on product goals', 'Gain knowledge as fast as possible']\n",
      "requirements: ['(Essential) Very good Python knowledge proven with academic or professional experience', '(Essential) Last years or graduated from studies like computer science, electronics or similar (high grade expected)', 'Proven experience with ML/AI', 'Software development skills (version control, testing, debugging)', 'Analytical and logical mind set (algorithmic problem solving)', 'Communicative spoken and written English and Polish']\n",
      "application_link: https://www.pracuj.pl/aplikuj/internship-python-for-data-science-and-processing-intern-warszawa-plac-europejski-1,oferta,1003925097?apid=bc4eb961-b99b-4adc-8f2a-1b34f960b9cb&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 305 ---\n",
      "id: 358\n",
      "url: https://www.pracuj.pl/praca/junior-data-engineer-warszawa-jana-nowaka-jezioranskiego-53a,oferta,1003921561\n",
      "title: Junior Data Engineer\n",
      "work_location: Company locationJana Nowaka-Jeziorańskiego 53a, Praga-Południe, WarszawaWarszawa, Masovian\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: contract of mandate, B2B contract\n",
      "employment_type: full-time\n",
      "position: junior specialist (Junior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'Запрошуємо працівників з України']\n",
      "technologies: ['Microsoft Azure', 'Google Cloud Platform', 'AWS', 'Python', 'SQL', 'PostgreSQL', 'Microsoft SQL Server', 'Apache Spark', 'Databricks', 'Azure DevOps', 'Apache Airflow', 'in house', \"at the client's site\", 'you focus on a single project at a time', 'you develop several projects simultaneously', 'you can change the project', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you develop the code \"from scratch\"', 'you focus on product development', 'agile', 'scrum']\n",
      "responsibilities: ['Implementing, and optimizing modern cloud-based solutions.', 'Building and launching new data models and data pipelines.', 'Implementing best practices in data engineering including data integrity, quality, and documentation.', 'Optimization of existing analytical solutions.', 'Leading small size teams of engineers and being a role model.']\n",
      "requirements: ['1+ years of experience delivering data warehouse, data lake, or business intelligence solutions.', 'Experience with cloud services such as Azure, AWS, or GCP.', 'Proficient in Python for data-related tasks.', 'Strong understanding of SQL, including data analysis and working with relational databases (e.g., SQL Server, PostgreSQL).', 'Basic knowledge of public cloud architecture, security, and networking concepts (MS Azure experience preferred).', 'Familiarity with data warehouse modeling practices and ETL/ELT development.', 'Strong conceptual and analytical skills to solve data-related challenges.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/junior-data-engineer-warszawa-jana-nowaka-jezioranskiego-53a,oferta,1003921561?apid=5698035c-76bd-45bc-b446-8d03c6476c98&oca=None&acv=0\n",
      "\n",
      "--- Job Record 306 ---\n",
      "id: 359\n",
      "url: https://www.pracuj.pl/praca/clinical-informaticist-warszawa-dluga-29,oferta,1003903999\n",
      "title: Clinical Informaticist\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 17 dnido 15 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: None\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['część etatu']\n",
      "technologies: ['u klienta', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'scrum']\n",
      "responsibilities: ['Pełnienie funkcji ewaluatora, konsultanta i edukatora w celu usprawnienia obecnych systemów i przepływów pracy związanych z pozyskiwaniem danych ze świata rzeczywistego', 'Identyfikowanie możliwości usprawnień w ramach obecnych narzędzi/systemów i działanie jako agent zmian w celu ułatwienia usprawnień', 'Zrozumienie złożoności przypadków użycia RWD zarówno z perspektywy biznesowej, jak i IT/technicznej oraz opracowanie strategicznego podejścia w celu zrównoważenia krótkoterminowych korzyści i długoterminowych kosztów', 'Pełnienie funkcji ambasadora informatyki medycznej i podkreślanie znaczenia informatyki klinicznej dla interesariuszy biznesowych, domen biznesowych IT i kierownictwa wyższego szczebla', 'Działanie jako łącznik między działami a zespołem informatyki medycznej']\n",
      "requirements: ['Wykształcenie w zakresie medycyny lub farmacji; najlepiej w zakresie informatyki klinicznej (ze szczególnym uwzględnieniem systemów nauki o danych/AI/ML)', 'Co najmniej 5 lat doświadczenia w pracy na podobnych stanowiskach', 'Doświadczenie w zakresie integracji przepływu pracy klinicznej i wdrażania systemów opartych na danych/AI (np. CDSS)', 'Doświadczenie w zakresie standardów interoperacyjności danych opieki zdrowotnej (np. FHIR) i terminologii (np. SNOMED)', 'Doświadczenie i znajomość przepisów dotyczących prywatności pacjentów, takich jak HIPAA i/lub GDPR', 'Bardzo dobry angielski (min B2/C1)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/clinical-informaticist-warszawa-dluga-29,oferta,1003903999?apid=ec444017-3b46-409e-bea0-64e1fe354799&oca=None&acv=0\n",
      "\n",
      "--- Job Record 307 ---\n",
      "id: 360\n",
      "url: https://www.pracuj.pl/praca/enterprise-risk-analytics-stress-testing-specialist-warszawa,oferta,1003885886\n",
      "title: Enterprise Risk Analytics – Stress Testing Specialist\n",
      "work_location: Company locationWarszawaWarszawa, Masovian\n",
      "validity: valid for 9 daysto 07 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Microsoft Excel', 'SAS', 'R', 'Python', 'agile', 'waterfall']\n",
      "responsibilities: ['Conduct annual reviews of liquidity, market non-trading, and operational risk stress testing programs, including regulatory-driven and internal stress tests', 'Evaluate stress test results and actions, including risk appetite, limit setting, and risk identification.', 'Perform additional reviews based on scenario changes, methodological updates, or significant macroeconomic or market events.', 'Conduct data analysis, support pool owners in reviewing results, and provide recommendations addressing business needs.', 'Communicate findings to stakeholders and ensure action plans are implemented to remediate identified issues.', 'Assist in the development of analytical engines for business product lines and package findings into detailed technical documentation.', 'Identify modeling opportunities to enhance business results.', 'Supervise junior team members and provide mentorship.']\n",
      "requirements: ['Demonstrated industry experience in treasury & funding risk, operational risk, market risk, or counterparty credit risk (CCR).', 'Strong knowledge of relevant regulatory guidance, including Reg YY and SR12-7.', 'Familiarity with risk appetite frameworks, limit setting, and risk management practices for both banking and trading book products.', 'Advanced academic qualifications (Master’s, PhD) or certifications such as FRM or CFA (preferred).', 'Exceptional communication skills, with the ability to present complex findings clearly and concisely.', 'Self-motivated with strong attention to detail and the ability to manage multiple projects.', 'Proficiency in Microsoft Excel, SAS, R, Python, or other programming languages.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/enterprise-risk-analytics-stress-testing-specialist-warszawa,oferta,1003885886?apid=70f6403f-7fc7-4d3b-95f3-dab21bb8ae0b&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 308 ---\n",
      "id: 361\n",
      "url: https://www.pracuj.pl/praca/data-engineer-data-scientist-poznan-mikolaja-reja-2,oferta,1003924962\n",
      "title: Data Engineer - Data Scientist\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: umowa zlecenie, kontrakt B2B, umowa o staż / praktyki\n",
      "employment_type: pełny etat, część etatu, dodatkowa / tymczasowa\n",
      "position: specjalista (Mid / Regular), młodszy specjalista (Junior)\n",
      "work_arrangement: praca stacjonarna, praca zdalna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Mikołaja Reja 2, Jeżyce, PoznańPoznań, wielkopolskie']\n",
      "technologies: ['Microsoft Data Factory', 'Microsoft Excel', 'Microsoft SQL Server', 'Git', 'Microsoft Azure', 'Python', 'Snowflake Data Cloud', 'Databricks', 'SAP', 'PostgreSQL', 'SQLite', 'elastyczny czas pracy']\n",
      "responsibilities: ['Analiza techniczna i dobór technologii: Ocena potrzeb projektowych oraz wybór odpowiednich usług Microsoft Azure (m.in. Data Factory, Synapse, SQL Database).', 'Projektowanie architektury rozwiązania: Opracowanie kompleksowej architektury systemu obejmującej procesy ETL, integrację z modułami AI oraz interfejs użytkownika.', 'Konfiguracja środowisk chmurowych: Ustawienie i zarządzanie środowiskami w Azure, w tym konfiguracja baz danych, Data Factory, API oraz innych narzędzi.', 'Projektowanie i implementacja procesów ETL: Budowa, optymalizacja i wdrażanie procesów ekstrakcji, transformacji i ładowania danych z różnych źródeł (Excel, SQL Server) do centralnej hurtowni danych.', 'Migracja i transformacja danych: Przenoszenie danych oraz przygotowywanie ich do analizy przez systemy AI, w tym opracowanie scenariuszy migracji i testów weryfikacyjnych.', 'Integracja z agentami AI: Połączenie przetwarzania danych z modułami AI (np. chatbotami) w celu generowania raportów i analiz danych.']\n",
      "requirements: ['Znajomość platformy Microsoft Azure: doświadczenie z usługami takimi jak Azure Data Factory, Azure Synapse, SQL Database oraz konfiguracją środowisk w Azure.', 'Biegłość w procesach ETL: projektowanie, implementacja oraz optymalizacja procesów ekstrakcji, transformacji i ładowania danych.', 'Doświadczenie w migracji danych: przenoszenie danych z różnych źródeł (np. plików Excel, SQL Server) do centralnej hurtowni danych lub bazy danych. Znajomość baz danych: umiejętność pracy zarówno z relacyjnymi (SQL Server, Azure SQL Database), jak i - dodatkowym atutem - z wektorowymi bazami danych.', 'Integracja systemów i API: konfiguracja i integracja API w celu komunikacji między różnymi systemami, w tym integracja z rozwiązaniami AI.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-data-scientist-poznan-mikolaja-reja-2,oferta,1003924962?apid=487a6bd8-137e-48c3-b1c7-8441158d25ee&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 309 ---\n",
      "id: 362\n",
      "url: https://www.pracuj.pl/praca/junior-engineer-opportunity-warszawa-jana-nowaka-jezioranskiego-53a,oferta,1003921446\n",
      "title: Junior Engineer Opportunity\n",
      "work_location: Company locationJana Nowaka-Jeziorańskiego 53a, Praga-Południe, WarszawaWarszawa, Masovian\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: contract of employment, contract of mandate, B2B contract\n",
      "employment_type: full-time\n",
      "position: junior specialist (Junior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'Запрошуємо працівників з України']\n",
      "technologies: ['Python', 'SQL', 'Java', 'React.js', 'TypeScript', 'Node.js', 'Power BI', 'AI/ML', 'in house', \"at the client's site\", 'you focus on a single project at a time', 'you develop several projects simultaneously', 'you can change the project', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you develop the code \"from scratch\"', 'you focus on product development', 'agile', 'scrum']\n",
      "responsibilities: ['Mandatory: Currently enrolled in or a recent graduate from technical faculties such as Computer Science, Mathematics, Physics, etc.', 'Strong analytical and problem-solving skills.', 'Excellent communication skills.', 'English min. B2.']\n",
      "requirements: ['Activity in university scientific clubs.', 'Winnings in contests, hackathons, etc.', 'Special awards, scholarships.', 'Participation in optional / extra scientific projects.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/junior-engineer-opportunity-warszawa-jana-nowaka-jezioranskiego-53a,oferta,1003921446?apid=1ef9ff9f-8a65-4f61-a1f7-a1ef720b5b97&oca=None&acv=0\n",
      "\n",
      "--- Job Record 310 ---\n",
      "id: 363\n",
      "url: https://www.pracuj.pl/praca/technical-project-manager-ai-powered-translation-system-krakow,oferta,1003921330\n",
      "title: Technical Project Manager – AI-Powered Translation System\n",
      "work_location: None\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time, additional / temporary\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KrakówKraków, Lesser Poland']\n",
      "technologies: ['NLP', 'AI']\n",
      "responsibilities: ['Experience in NLP or Machine Learning and Training', 'Experience working with third-party software vendors, particularly in AI/translation-related technologies and SLA’s.', '3+ years of experience in technical project management, particularly in AI, NLP, or software integration projects.', 'Proven track record of managing enterprise IT projects with multiple stakeholders and contractual constraints.', 'Strong understanding of software development lifecycle (SDLC), cloud-based deployment, and machine learning (ML) implementation.', 'Knowledge of data security and compliance requirements, preferably within Polish/European regulations.', 'Excellent communication, negotiation, and stakeholder management skills.', 'Ability to anticipate risks, troubleshoot issues, and drive structured problem-solving approaches.', 'Polish native speaker, writing business English.', 'Familiar with project management tools like MS Project.']\n",
      "requirements: ['Experience managing remote IT infrastructure deployments and system integrations is a plus.', 'Familiarity with automated translation technologies (MT, TM) or similar platforms is a strong advantage.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/technical-project-manager-ai-powered-translation-system-krakow,oferta,1003921330?apid=091852b5-68cc-42d3-91e7-772db7414258&oca=None&acv=0\n",
      "\n",
      "--- Job Record 311 ---\n",
      "id: 364\n",
      "url: https://www.pracuj.pl/praca/machine-learning-engineer-chorzow-opolska-19,oferta,1003924819\n",
      "title: Machine Learning Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior), ekspert\n",
      "work_arrangement: None\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Opolska 19, ChorzówChorzów, śląskie', 'praca stacjonarna']\n",
      "technologies: ['Python', '.NET', 'wewnątrz organizacji', 'koncentrujesz się na jednym projekcie', 'tworzysz kod \"od zera\"', 'scrum', 'backend developer', 'frontend developer', 'fullstack developer', 'project manager']\n",
      "responsibilities: ['research oraz testowanie nowych rozwiązań z zakresu uczenia maszynowego (ML) w kontekście potrzeb organizacji', 'przygotowanie modeli ML do wdrożenia w środowisku produkcyjnym', 'monitorowanie działania modeli ML w środowisku produkcyjnym i ich regularna aktualizacja w celu utrzymania wysokiej jakości predykcji', 'tworzenie interfejsów do modeli, takich jak REST API', 'nadzorowanie prac programistycznych, zapewniając najwyższą jakość rozwiązań poprzez testy automatyczne', 'tworzenie i dostosowywanie narzędzi testowych']\n",
      "requirements: ['minimum 3 lata różnorodnego, komercyjnego doświadczenia w pracy z frameworkami ML', 'bardzo dobra znajomość modeli uczenia maszynowego', 'dobra znajomość języka Python lub .NET', 'umiejętność tworzenia i optymalizacji narzędzi do weryfikacji działania modeli ML', 'doświadczenie w przeprowadzaniu badań technicznych w zakresie ML', 'wysokie zdolności komunikacyjne i chęć współpracy w zespole']\n",
      "application_link: https://www.pracuj.pl/aplikuj/machine-learning-engineer-chorzow-opolska-19,oferta,1003924819?apid=1cd44aa4-12ee-4de5-b622-678fb007aa76&oca=None&acv=0\n",
      "\n",
      "--- Job Record 312 ---\n",
      "id: 365\n",
      "url: https://www.pracuj.pl/praca/bi-developer-nifi-spark-pyspark-warszawa-encyklopedyczna-2a,oferta,1003903756\n",
      "title: BI Developer (NiFi/ Spark/ pySpark)\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 17 dnido 15 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: None\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['część etatu']\n",
      "technologies: ['Python', 'Kafka', 'Hive', 'Spark', 'PySpark', 'Nifi', 'Hadoop', 'Oracle', 'HDFS', 'u klienta', 'koncentrujesz się na jednym projekcie', 'masz wpływ na wybór narzędzi i technologii']\n",
      "responsibilities: ['Tworzenie rekomendacji dla projektów związanych z przetwarzaniem danych.', 'Implementacja rozwiązań z wykorzystaniem odpowiednich technologii.', 'Wdrażanie i optymalizacja rozwiązań BI oraz/lub Big Data (tj. integracje, przetwarzanie danych, raportowanie).', 'Dbanie o efektywne wykorzystanie zasobów przy implementacji logiki (z zachowaniem kompaktowania małych plików i niskiej defragmentacji flow files).', 'Przygotowanie testów.', 'Stosowanie prawidłowych standardów i dobrych praktyk kodowania – wdrażanie ich w codzienną pracę ułatwi ‘życie’ w Zespole oraz funkcjonalność produktu!']\n",
      "requirements: ['Stawiasz na pisanie wysokiej jakości kodu, który jest dobrze przetestowany, wydajny i łatwy w utrzymaniu.', 'Posiadasz minimum 3-letnie doświadczenie w podobnej roli.', 'Praktyczna wiedza w zakresie tworzenia rozwiązań BigData (w szczególności Spark/pySpark, Kafka, Hadoop, Hive, Nifi).', 'Praktyczna znajomość programowania w Python, SQL.', 'Wiedza w zakresie projektowania rozwiązań w obszarze BI.', 'Lubisz się uczyć oraz dzielić się wiedzą – gotowość do pomocy innym oraz doskonalenie własnych umiejętności to ważne dla nas wartości.', 'Możesz powiedzieć o sobie: pasja i ciekawość technologii zmotywowały mnie do podjęcia tej pracy!', 'Proaktywność – lubisz przejmować odpowiedzialność za swoje zadania i wiesz jak domykać tematy.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/bi-developer-nifi-spark-pyspark-warszawa-encyklopedyczna-2a,oferta,1003903756?apid=cdc2797d-0a46-4a14-839c-075542825369&oca=None&acv=0\n",
      "\n",
      "--- Job Record 313 ---\n",
      "id: 366\n",
      "url: https://www.pracuj.pl/praca/mlodszy-programista-php-lodz-obywatelska-128-152,oferta,1003924663\n",
      "title: Młodszy Programista PHP\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 25 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), młodszy specjalista (Junior)\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Obywatelska 128/152, Polesie, ŁódźŁódź, łódzkie', 'praca stacjonarna']\n",
      "technologies: ['PHP', 'MySQL', 'PostgreSQL', 'Git', 'SVN', 'wewnątrz organizacji', 'agile', 'scrum', 'code review']\n",
      "responsibilities: ['Aktywny udział w spotkaniach Daily Scrum;', 'Planowaniem pracy na podstawie backlogu;', 'Analiza, dekompozycja, szacowaniem zadań do wykonania;', 'Implementacja robotów i aplikacji wewnętrznych z wykorzystaniem m.in. PHP, PostgreSQL;', 'Reagowanie na bieżące potrzeby i dostosowywanie aplikacji do wymagań;', 'Code review kodu innych członków zespołu;', 'Opracowywanie optymalizacji w istniejących aplikacjach, zwiększanie ich efektywności;', 'Współpraca z pozostałymi programistami, testerami i analitykami;', 'Tworzenie i utrzymywanie dokumentacji technicznej;', 'Udział w cyklicznym usuwaniu długu technologicznego.']\n",
      "requirements: ['Wykształcenie wyższe lub w trakcie (Informatyka lub pokrewne)', 'Minimum 1 rok doświadczenia na podobnym stanowisku (preferowane min. 2 lata)', 'Znajomość języka programowania: PHP w wersji 7+,', 'Znajomość jednego z frameworków PHP zgodnych z MVC (mile widziana znajomość: Frameworka Codeigniter)', 'Praktyczna znajomość relacyjnych baz danych PostgreSQL / MySQL', 'Angielski min. B1,']\n",
      "application_link: https://www.pracuj.pl/aplikuj/mlodszy-programista-php-lodz-obywatelska-128-152,oferta,1003924663?apid=9c209ef7-1fd1-41d5-92ef-2dc65a0982e2&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 314 ---\n",
      "id: 367\n",
      "url: https://www.pracuj.pl/praca/data-scientist-mid-warszawa,oferta,1003864183\n",
      "title: Data Scientist (Mid)\n",
      "work_location: Company locationWarszawaWarszawa, Masovian\n",
      "validity: None\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: None\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'valid 5 hours']\n",
      "technologies: ['Python', 'SQL', 'Tableau', 'Power BI', 'Spark', 'Hadoop', 'Pandas', 'scikit-learn', \"at the client's site\", 'you focus on a single project at a time', 'you have influence on the product']\n",
      "responsibilities: ['Conducting numerical analyses and designing algorithms in the area of Data Science', 'Maintaining existing analytical tools and identifying areas for development', 'Diagnosing problems and implementing solutions within analytical projects', 'Cooperation with the team, including searching for information and answers in the organization']\n",
      "requirements: ['3+ years of experience as a Data Scientist', 'Practical knowledge of technologies and tools such as: Python (numpy, pandas, scikit-learn, matplotlib/seaborn)', 'SQL (databases, query optimization)', 'Visualization tools (Tableau, Power BI)', 'Big Data (e.g. Spark, Hadoop)', 'Knowledge of Machine Learning techniques and data modeling (regression, classification, clustering)', 'Ability to work in a team and independently search for solutions in large organizational structures', 'Openness to challenges and development in the area of data analysis']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-mid-warszawa,oferta,1003864183?apid=4f2b39c2-1eed-4677-b987-166ec3dda258&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 315 ---\n",
      "id: 368\n",
      "url: https://www.pracuj.pl/praca/programista-bazy-danych-oracle-senior-warszawa,oferta,1003867296\n",
      "title: Programista Bazy Danych Oracle Senior\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze dzieńdo 27 lutego 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna, praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Oracle', 'Microsoft SQL Server', 'Jira', 'Confluence', 'u klienta']\n",
      "responsibilities: ['Inicjowanie i modyfikacja baz danych,', 'Programowanie zmiany (kodowanie),', 'Przygotowanie i przeprowadzenie testów wewnętrznych (integracyjnych, jednostkowych),', 'Weryfikacja zgodności realizowanej zmiany z architekturą systemu i standardami (przegląd, kontrola jakości),', 'Przygotowanie zmiany do wdrożenia (repozytorium kodu, konfiguracje, dokumentacja wdrożeniowa etc.).']\n",
      "requirements: ['Doświadczenie w pracy w aplikacji Oracle APEX,', 'Doświadczenie w pracy na dużych wolumenach danych w aplikacjach bazodanowych w technologii Oracle,', 'Znajomość języka programowania MS SQL,', 'Systematyczność i zaangażowanie,', 'Umiejętność analitycznego myślenia,', 'Umiejętność współpracy w zespole,', 'Doświadczenie w pracy z JIRA i Confluence.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/programista-bazy-danych-oracle-senior-warszawa,oferta,1003867296?apid=0c42f2dd-3771-448a-a5be-d15215608693&oca=None&acv=0\n",
      "\n",
      "--- Job Record 316 ---\n",
      "id: 369\n",
      "url: https://www.pracuj.pl/praca/llm-developer-warszawa-aleja-jana-pawla-ii-27,oferta,1003924498\n",
      "title: LLM Developer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze miesiącdo 26 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat, część etatu\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Szukamy wielu kandydatówwakaty: 4']\n",
      "technologies: ['LLM', 'RAG', 'AI', 'Machine Learning', 'Python', 'PyTorch', 'TensorFlow', 'Mflow', 'Kubernetes', 'CI/CD', 'AWS', 'OpenAI', 'Snowflake', 'OpenShift', 'wewnątrz organizacji', 'koncentrujesz się na jednym projekcie', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na produkt', 'tworzysz kod \"od zera\"', 'agile', 'scrum', 'backend developer', 'frontend developer', 'mobile developer', 'lider techniczny', 'architekt', 'devOps', 'data scientist', 'programista testów automatycznych', 'tester manualny', 'product owner', 'project manager', 'UI designer', 'UX designer']\n",
      "responsibilities: ['Współpraca przy tworzeniu algorytmów AI', 'Współpraca przy tworzeniu rozwiązań opartych o LLM', 'Współpraca przy tworzeniu rozwiązań opartych o RAG', 'Implementacja modeli AI']\n",
      "requirements: ['Minimum 3 letnie ogólne doświadczenie developerskie (Python)', 'Minimum roczne komercyjne doświadczenie związane z LLM (wdrażanie/utrzymanie)', 'Znajomość bibliotek i narzędzi Machine Learning', 'Umiejętność pracy z danymi oraz analizy dużych zbiorów danych', 'Znajomość narzędzi do konteneryzacji', 'Znajomość języka angielskiego werbalnie na poziomie B2/C1']\n",
      "application_link: https://www.pracuj.pl/aplikuj/llm-developer-warszawa-aleja-jana-pawla-ii-27,oferta,1003924498?apid=c38ac2d6-808e-4b64-a898-a921ea5a5530&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 317 ---\n",
      "id: 370\n",
      "url: https://www.pracuj.pl/praca/power-automate-developer-rybnik-obwiednia-poludniowa-22,oferta,1003885446\n",
      "title: Power Automate Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 9 dnido 07 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Obwiednia Południowa 22, Śródmieście, RybnikRybnik, śląskie']\n",
      "technologies: ['Microsoft 365', 'Power Platform', 'Power Automate', 'SQL', 'Microsoft Power Apps', 'Microsoft Azure', 'BPMN', 'wewnątrz organizacji', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt']\n",
      "responsibilities: ['Microsoft 365 ecosystem, Power Platform, especially Power Automate', 'business process automation & workflow design', 'relational databases and SQL', 'privacy by design & privacy by default concepts in practice', 'problem solving skills with a attention to details', 'good collaboration and communication and skills', 'very good knowledge of English', 'Optional skills & experience', 'Microsoft Power Apps', 'Microsoft Azure', 'BPMN']\n",
      "requirements: []\n",
      "application_link: https://www.pracuj.pl/aplikuj/power-automate-developer-rybnik-obwiednia-poludniowa-22,oferta,1003885446?apid=373d5090-b144-4fde-9446-bad77309ee82&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 318 ---\n",
      "id: 371\n",
      "url: https://www.pracuj.pl/praca/data-engineer-warszawa-pulawska-39,oferta,1003885440\n",
      "title: Data Engineer\n",
      "work_location: Company locationPuławska 39, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for 9 daysto 07 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Python', 'Scala', 'Spark', 'Hadoop', 'Kubernetes', 'Git']\n",
      "responsibilities: ['Development and maintenance of ETL and data platforms (Python, Scala, Spark, HDFS, Hive)', 'Development and maintenance of access applications for business users and user support (Airflow, Jupyterhub, Trino, Superset, MLFlow) in the context of Kubernetes, Docker, ArgoCD', 'Automation and CICD (Gitlab-CI)', 'Monitoring (Prometheus)', \"R&D, maintenance, and monitoring of the platform's components\", \"Implementing and executing policies aligned with the company's strategic plans concerning used technologies, work organization, etc.\"]\n",
      "requirements: ['Proficiency in a programming language like Python and Scala', 'Working with Spark messaging systems', 'Experience with Hadoop', 'Hands-on experience with Kubernetes', 'Strong programming skills with a solid understanding of software engineering principles, best practices, and solutions', 'Experience with Version Control System, preferably GIT', 'Ability to actively participate/lead discussions with clients to identify and assess concrete and ambitious avenues for improvement']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-warszawa-pulawska-39,oferta,1003885440?apid=397d2958-68c5-4a14-9793-1da4aec3446e&oca=None&acv=0\n",
      "\n",
      "--- Job Record 319 ---\n",
      "id: 372\n",
      "url: https://www.pracuj.pl/praca/senior-analytics-engineer-gcp-dbt-warszawa-pulawska-39,oferta,1003885441\n",
      "title: Senior Analytics Engineer (GCP / DBT)\n",
      "work_location: Company locationPuławska 39, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for 9 daysto 07 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Google Cloud Platform', 'SQL', 'Python', 'Microsoft Power BI', 'Looker']\n",
      "responsibilities: ['Gathering business requirements for data solutions (reports, analysis)', 'Providing guidance on metrics for monitoring business performance', 'Overseeing, developing data pipelines, and executing data transformations (primarily in SQL)', 'Designing and implementing data models within a BI tool', 'Creating reports and dashboards', 'Validating and testing data', 'Presenting results to the business and supporting decision-making based on insights', 'Sharing knowledge on data literacy', 'Keeping current with the latest trends and best practices in analytics engineering']\n",
      "requirements: ['Strong proficiency in GCP, with expertise in BigQuery,', 'Solid experience with dbt (Data Build Tool), Dataflow, and Data Vault', 'Ability to transform and model data using dbt, including creating logical data models and optimizing them for business analytics', 'Advanced proficiency in SQL and Python', 'Ability to integrate data from various sources, also in streaming mode', 'Knowledge of data quality management (Data Quality) and tracking metadata and lineage in data platforms', 'Ability to work with business teams to define and implement analytics use cases', \"Ability to communicate and adapt technology solutions to SFG's business needs\"]\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-analytics-engineer-gcp-dbt-warszawa-pulawska-39,oferta,1003885441?apid=ccead1a1-dc48-4ab8-974e-7bb19f875c95&oca=None&acv=0\n",
      "\n",
      "--- Job Record 320 ---\n",
      "id: 373\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-net-azure-warszawa-aleje-jerozolimskie-158,oferta,1003924267\n",
      "title: Senior Data Engineer (.NET, Azure)\n",
      "work_location: None\n",
      "validity: valid for a monthto 25 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Aleje Jerozolimskie 158, Ochota, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['.NET', 'Azure', 'C#', 'ASP.NETCore', '.NET Core', 'Postgres', 'Microsoft SQL Server', 'MySQL', 'Cosmos DB', 'Microsoft Fabric', 'Power BI', 'Visual Studio Code', 'Visual Studio', 'Azure DevOps', 'in house']\n",
      "responsibilities: ['Take responsibility for implementation of our platform data strategy. Therefore, develop data management concepts especially the sharing of data from 3rd party vendors and their modules and data of external IoT devices for farming. Drive data governance to ensure the data standardization on the platform to ensure the platform further develops to a data hub.', 'Develop and implement a platform analytics concept to provide analytical services and tools to the different platform stakeholders e.g. farmers, vendors and owner.', 'Provide insights on top of the developed toolbox e.g. BI reports.', 'Ensure the implementation of legal data requirements e.g. the EU data act.', 'Implement and enhance our general software architecture and services.', 'Work with our product managers.', 'Join and coordinate software development activities together with our internal teams and external partners.']\n",
      "requirements: ['Education in computer science or similar. Minimum Bachelor grade,', 'Full stack development, data engineering/science, data management experience - minimum 5 years,', 'Agile software development methodologies,', 'Very good knowledge of backend development with the .NET Framework (C#, ASP.NETCore, .NET Core),', 'Very good knowledge of database technologies e.g. Postgres, Microsoft SQL Server, MySQL, Cosmos DB,', 'Good knowledge in data harmonization and normalization,', 'Analytics frameworks e.g. Microsoft Fabric, Power BI,', 'Work with modern development environments e.g. Visual Studio Code, Visual Studio,', 'Azure DevOps,', 'Cloud computing, preferred Azure.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-net-azure-warszawa-aleje-jerozolimskie-158,oferta,1003924267?apid=3af34539-7f57-4971-bb38-512f96a8cbc9&oca=None&acv=0\n",
      "\n",
      "--- Job Record 321 ---\n",
      "id: 374\n",
      "url: https://www.pracuj.pl/praca/senior-ai-engineer-krakow-lubicz-23a,oferta,1003885153\n",
      "title: Senior AI Engineer\n",
      "work_location: None\n",
      "validity: valid for 8 daysto 06 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Lubicz 23a, Grzegórzki, KrakówKraków, Lesser Poland', 'contract of employment']\n",
      "technologies: ['Python', 'JavaScript', 'Google Cloud Platform']\n",
      "responsibilities: ['Independent Execution:', 'Demonstrate the ability to execute tasks and projects independently, driving them from inception to completion with minimal supervision.', 'Exhibit self-reliance and resourcefulness in problem-solving, leveraging technical expertise to overcome challenges effectively.', 'Take ownership of assigned responsibilities and deliverables, ensuring they are completed accurately and on schedule.', 'Design and whiteboard solutions, collaborating with the team to develop and implement innovative approaches.', 'Generative AI Exploration:', \"Investigate and apply Generative AI methodologies to meet the company's specific needs and challenges.\", 'Collaborate with cross-functional teams to determine the best applications of Generative AI for the business.', 'Cloud Solutions Development:', 'Take an active role in the design and implementation of Cloud solutions, particularly on Google Cloud Platform (GCP) and Vertex AI, for assigned projects or components within the team.', 'Guide the full lifecycle of development, from initial design to production deployment, ensuring quality and efficiency.', 'Design and whiteboard solutions, collaborating with the team to develop and implement innovative approaches.', 'Generative AI Exploration:', \"Investigate and apply Generative AI methodologies to meet the company's specific needs and challenges.\", 'Collaborate with cross-functional teams to determine the best applications of Generative AI for the business.', 'Continuous Learning and Improvement:', \"Stay abreast of emerging technologies and industry trends to evolve the team's capabilities continually.\", 'Foster a culture of curiosity and learning, encouraging experimentation and growth.']\n",
      "requirements: ['Bachelor’s degree in Computer Science, Engineering, or related field.', 'Minimum of 4-5 years of experience in software development, with a strong focus on Python, Cloud solutions and development-to-production lifecycle.', 'Passion for learning and applying new technologies, with the ability to inspire and drive innovation within the team.', 'Excellent communication, collaboration, and problem-solving skills.', 'Experience with Generative AI is a major plus.', 'Experience with deploying and supporting Machine Learning/Large Language Models a major plus.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-ai-engineer-krakow-lubicz-23a,oferta,1003885153?apid=4050b508-b8c4-4060-859a-2ca8659400d7&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 322 ---\n",
      "id: 375\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-krakow-lubicz-23a,oferta,1003894267\n",
      "title: Senior Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 12 daysto 10 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Lubicz 23a, Grzegórzki, KrakówKraków, Lesser Poland', 'contract of employment']\n",
      "technologies: ['SQL', 'Python', 'Google Cloud Platform']\n",
      "responsibilities: ['Contribute to the development and design of the data platform, including the design and execution of data extraction, transformation, and optimal data pipeline architectures and solutions at an enterprise scale to enable business intelligence, analytics, and data science use cases', 'Design and implement processes around data-loading, data transformation, and feature engineering as well as access control, data security, and data privacy', 'Develop complex code-based ETL/ELT data pipelines with performance optimized data modeling', 'Develop source of truth data models to power business analytics solutions using data warehouse modeling techniques, data quality checks, reconciliation processes, and thorough testing practices', 'Establish and implement best practices in coding, monitoring, and alerting, using CI/CD and other relevant techniques', 'Ensure code is managed, validated, and released using best practices (DevOps) making effective use of automation for these processes', 'Maintain a repeatable and automated approach to data management including a continuous deployment pipeline, centralized configuration management, automated testing patterns, provisioning of environments, and controlled release to production', 'Ensure all features and new releases are tested, upgraded, and changed as necessary from a data perspective']\n",
      "requirements: ['Minimum 4 years of experience in a similar role', 'Proficiency in designing efficient and robust ETL/ELT workflows, data modeling, and data pipelines – batch and/or event-driven', 'Proficiency in designing data solutions and creating solution architecture', 'Experience working in a complex enterprise data warehouse environment and developing and maintaining pipeline processes to move and transform data', 'Experience with data warehousing principles, including orchestration framework, schema structure, and data architecture principles', 'Experience with GCP cloud (3+ preferred).', 'Experience with Informatica ETL and Governance', 'Ability to write robust code in SQL, and Python', 'Experience implementing and managing CI/CD process', 'Experience with stakeholder management and engaging with business teams to facilitate data connectivity and exchange']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-krakow-lubicz-23a,oferta,1003894267?apid=1c789f6f-2c96-4a68-8707-4b72402c94e0&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 323 ---\n",
      "id: 376\n",
      "url: https://www.pracuj.pl/praca/senior-ml-engineer-krakow-zablocie-43b,oferta,1003903468\n",
      "title: Senior ML Engineer\n",
      "work_location: None\n",
      "validity: valid for 16 daysto 14 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Zabłocie 43B, Podgórze, KrakówKraków, Lesser Poland']\n",
      "technologies: ['Java', 'Scala', 'Python', 'SQL', 'Rust', 'Docker', 'Kubernetes', 'Kafka', 'Scylla', 'Clickhouse', 'in house', 'you focus on a single project at a time', 'you develop several projects simultaneously', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'you focus on code maintenance', 'data scientist']\n",
      "responsibilities: ['Implement new functionalities for a software that will be used in ML training and inference pipelines.', 'Develop software with a core focus on reliability and performance.', 'Translate complex functional and technical requirements into detailed design.', 'Collaborate with data scientist.', 'Creating tools for automating manual processes.']\n",
      "requirements: ['Strong fundamentals in software engineering: Data Structures, Algorithms.', 'Experience in Java or Scala (at least willing to learn Scala).', 'Programming skills in Python (advanced).', 'Experience with SQL databases.', 'Excellent troubleshooting and debugging skills.', 'High care of user experience.', 'Being open to learn and understand ML/deep learning concepts.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-ml-engineer-krakow-zablocie-43b,oferta,1003903468?apid=e2c5d182-6d1b-4ab6-b81c-c1ac9547d02b&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 324 ---\n",
      "id: 377\n",
      "url: https://www.pracuj.pl/praca/analityk-ds-zarzadzania-danymi-it-warszawa-dolna-3,oferta,1003903102\n",
      "title: Analityk ds.Zarządzania Danymi IT\n",
      "work_location: None\n",
      "validity: ważna jeszcze 16 dnido 14 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Dolna 3, Mokotów, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SAP Business Objects', 'Qlik Sense', 'SQL', 'Teradata', 'wewnątrz organizacji', 'agile']\n",
      "responsibilities: ['Projektowanie, wdrażanie i utrzymywanie przepływów danych (wsadowych/w czasie rzeczywistym) w celu spełnienia wymagań biznesowych i umożliwienia konsumpcji danych.', 'Tworzenie i zarządzanie modelami danych na wszystkich poziomach: fizycznym (magazyn danych), logicznym (stos Business Intelligence), mieszanym (narzędzia samoobsługowe).', 'Udzielanie wsparcia użytkownikom końcowym w odpowiedzi na zgłoszone incydenty i wewnątrz lokalnego centrum kompetencji w zakresie produktów danych.', 'Współpraca z globalnym zespołem ds. technologii danych w projektach międzynarodowych.']\n",
      "requirements: ['Co najmniej 2 lata doświadczeniem w projektach Data Warehousing i ETL.', 'Silne umiejętności programowania w SQL.', 'Znajomość systemów zarządzania bazami danych, np. Teradata.', 'Dobrze rozwinięte umiejętności analityczne i zdolności do rozwiązywania problemów.', 'Umiejętność pracy pod presją i duża samodzielność.', 'Dobra znajomość języka angielskiego, umożliwiającą komunikację z użytkownikami końcowymi i kierownictwem.', 'Doświadczenie w pracy z narzędziami do raportowania, takimi jak SAP Business Objects, Qlik Sense lub podobne, będzie dodatkowym atutem.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/analityk-ds-zarzadzania-danymi-it-warszawa-dolna-3,oferta,1003903102?apid=8d6da86e-a894-43b4-b003-a6e2f8574ada&oca=None&acv=0\n",
      "\n",
      "--- Job Record 325 ---\n",
      "id: 378\n",
      "url: https://www.pracuj.pl/praca/chief-product-owner-formacji-sztucznej-inteligencji-warszawa-pulawska-15,oferta,1003903086\n",
      "title: Chief Product Owner Formacji Sztucznej Inteligencji\n",
      "work_location: None\n",
      "validity: ważna jeszcze 16 dnido 14 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: None\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Puławska 15, Mokotów, WarszawaWarszawa, mazowieckie', 'kierownik / koordynator']\n",
      "technologies: ['Jira', 'Confluence']\n",
      "responsibilities: ['definiowanie wizji oraz backlogu Formacji Sztucznej Inteligencji,', 'tworzenie wizji produktu prac Formacji Sztucznej Inteligencji,', 'przygotowywanie roadmapy prac Formacji Sztucznej Inteligencji,', 'jasne i precyzyjne komunikowanie oczekiwań w formie spriorytetyzowanego backlogu,', 'wyznaczanie terminów realizacji poszczególnych zadań,', 'zlecanie badań potrzeb i doświadczeń klienta celem jak najdokładniejszego zdefiniowania potrzeb biznesowych,', 'ustalanie celów (OKR) Formacji Sztucznej Inteligencji,', 'przekazywanie Formacji informacji biznesowych z reszty organizacji, niezbędnych w procesie wytwarzania rozwiązania oraz podejmowania decyzji implementacyjnych,', 'priorytetyzowanie backlogu Formacji zgodnie ze spodziewaną wartością biznesową,', 'planowanie pracy Formacji w oparciu o wyceny i pojemność zespołów oraz identyfikowane współzależności technologiczne,', 'zbieranie informacji zwrotnych od interesariuszy w ramach procesu wytwórczego danego zadania,', 'zapewnianie synchronizacji pomiędzy zespołami w Formacji celem identyfikacji i zarządzania potencjalnymi zależnościami oraz ryzykami,', 'identyfikowanie, komunikowanie i proponowanie rozwiązań w ramach zależności rozwiązań z innymi procesami lub produktami w banku,', \"odbieranie prac Formacji zgodnie z 'Definition of Done' oraz z punktu widzenia klienta końcowego i interesariuszy biznesowych,\", 'raportowanie postępu prac do interesariuszy (w tym członków zarządu) indywidualnie oraz podczas cyklicznych przeglądów sprintów,', 'odpowiadanie za bezpieczeństwo powstałych rozwiązań poprzez weryfikację ich pod kątem obowiązujących w banku regulacji i wytycznych z zakresu cyberbezpieczeństwa.']\n",
      "requirements: ['doświadczenie: biznesowe (powyżej 5 lat) w ramach pracy w złożonych instytucjach, managerskie (zarządzanie kilkunastoosobowymi zespołami), w prowadzeniu projektów o dużym wpływie na organizację, w podejmowaniu decyzji w oparciu o różne źródła informacji, w realizacji projektów informatycznych, z dostarczaniem rozwiązań informatycznych (niekoniecznie jako programista, może być również jako np. menedżer projektu), w roli Product Ownera/Właściciela biznesowego, w pracy z JIRA/Confluence,', 'cechujesz się dobrą znajomością biznesu i technologii oraz głębokim, taktycznym sposobem myślenia,', 'posiadasz umiejętność wspierania autonomii zespołów przy jednoczesnej dbałości o zapewnienie klientom wysokiej wartości dostarczanych rozwiązań', 'bierzesz odpowiedzialność za podejmowanie decyzji oraz ich konsekwencje,', 'posiadasz praktyczną znajomość narzędzi coachingowych w zarządzaniu zespołem,', 'jesteś zainteresowany metodyką Scrum bądź innymi praktykami zwinnymi (min. w aspekcie teoretycznym),', 'posiadasz dobra znajomość technologii powiązanych z wytwarzanym produktem oraz znajomość i rozumienie procesu wytwarzania oprogramowania,', 'wdrażałeś innowacyjne rozwiązania zwiększające wartość biznesową,', 'trafnie identyfikujesz potrzeby i wymagania klientów zewnętrznych oraz wewnętrznych,', 'posiadasz umiejętność zarządzanie Zespołem pracującym w Scrumie oraz posiadasz dobrą znajomość i rozumienie metodyk zwinnych (w tym Agile),', 'przekładasz kierunki strategiczne na cele dla własnego obszaru odpowiedzialności,', 'posiadasz umiejętność delegowania zadania zespołowi w oparciu o zdefiniowane reguły współpracy i postępowania,', 'na bieżąco weryfikujesz stawiane założenia oraz adekwatnie reaguje na nieprzewidziane sytuacje,', 'stale rozwijasz swoje umiejętności w zakresie nowych metodyk pracy oraz rozwiązań technologicznych.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/chief-product-owner-formacji-sztucznej-inteligencji-warszawa-pulawska-15,oferta,1003903086?apid=29bcaa68-ebde-4069-95cb-162d68a5999e&oca=None&acv=0\n",
      "\n",
      "--- Job Record 326 ---\n",
      "id: 379\n",
      "url: https://www.pracuj.pl/praca/oracle-database-administrator-gdansk,oferta,1003905871\n",
      "title: Oracle Database Administrator\n",
      "work_location: None\n",
      "validity: valid for 17 daysto 15 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['GdańskGdańsk, Pomeranian', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Azure', 'AWS', 'Oracle', 'project manager']\n",
      "responsibilities: ['Perform Oracle Grid, ASM, Database installations/configurations of infrastructure components', 'Perform database backup/restore/refresh using various methods like RMAN, Datapump export/import', 'Perform configuration changes as required by the project team', 'Work with application/project teams to provide technical direction', 'Candidate is Certified Oracle Cloud Infrastructure Architect  professional and oracle certified professional', 'Working with Postgre SQL, Cosmo and MS SQL']\n",
      "requirements: ['5-8 years of relevant experience', 'Knowledge on OCI, Azure & AWS Cloud platforms and the database administration related to cloud environments', 'Extensive experience on oracle 12cR2/19c/23ai', 'Experience on Oracle Multitenant Architecture, working with Container/Pluggable databases', 'Experience on Database Upgrade/migration from different versions(12c to 19c will be a plus)', 'Thorough understanding/knowledge on Oracle architecture', 'Experience of working on Real Application Cluster, Active Dataguard is a must.', 'Installation and configuration of components such as Grid, ASM, database', 'Experience on different database migration methods using RMAN, datapump EXP/IMP', 'Setting up replication using Golden Gate is desirable.', 'English on at least B2 Level', 'Higer Diploma Engineering or Computer Science preferred']\n",
      "application_link: https://www.pracuj.pl/aplikuj/oracle-database-administrator-gdansk,oferta,1003905871?apid=f10d3303-6d97-45c5-96be-285f3a1bddcd&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 327 ---\n",
      "id: 380\n",
      "url: https://www.pracuj.pl/praca/analityk-product-owner-warszawa,oferta,1003903014\n",
      "title: Analityk - Product Owner\n",
      "work_location: None\n",
      "validity: ważna jeszcze 16 dnido 14 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: None\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca stacjonarna, praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Python', 'Java', 'SQL', 'UML', 'BPMN', 'Enterprise Architect', 'u klienta', 'koncentrujesz się na jednym projekcie', 'agile']\n",
      "responsibilities: ['Zarządzanie wybranymi backlogami systemowymi', 'Prowadzenie analizy systemowej dla rozwiązań informatycznych', 'Wsparcie analityczne podczas wytwarzania oprogramowania, testów i wdrożenia', 'Współpraca merytoryczna z programistami, testerami, użytkownikami biznesowymi, administratorami', 'Przygotowywanie dokumentacji związanej z analizą systemową oraz analizą biznesową', 'Modelowanie procesów, danych, raportów, interfejsów oraz przepływów i relacji']\n",
      "requirements: ['Wykształcenie wyższe – informatyczne, telekomunikacyjne lub pokrewne', 'Minimum trzyletnie doświadczenie na tożsamym stanowisku', 'Minimum 5 letnie doświadczenie programistyczne i analityczne', 'Umiejętność modelowania w notacji UML i BPMN', 'Znajomość narzędzi typu CASE (np. Enterprise Architect)', 'Wiedza praktyczna i umiejętności programowania z zakresu wielu technologii w szczególności Python, Java, SQL, relacyjne bazy danych oraz bazy NoSQL', 'Praktyczna wiedza związana z zagadnieniami: walki radioelektronicznej, drony, komunikacja bezprzewodowa, przetwarzanie obrazów', 'Praktyczna wiedza z zakresu analizy danych, modelowania procesów biznesowych, integracji systemów oraz Machine Learning i sztucznej inteligencji', 'Umiejętność przygotowywania analiz systemowych (w tym umiejętność tworzenia między innymi przypadków użycia, user story, mockupów, modeli danych)', 'Znajomość języka angielskiego, umożliwiająca co najmniej efektywne zapoznawanie się z dokumentacją z zakresu IT', 'Praktyczna wiedza z zakresu zarządzania backlogiem zmian systemowych oraz wiedza w zakresie prowadzenia projektów informatycznych oraz narzędzi raportowych', 'Praktyczna znajomość metodyki Agile']\n",
      "application_link: https://www.pracuj.pl/aplikuj/analityk-product-owner-warszawa,oferta,1003903014?apid=cb1b3085-bb17-4603-b6d1-0520aa8ac284&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 328 ---\n",
      "id: 381\n",
      "url: https://www.pracuj.pl/praca/ekspert-ds-algorytmow-sztucznej-inteligencji-warszawa,oferta,1003902990\n",
      "title: Ekspert ds. algorytmów sztucznej inteligencji\n",
      "work_location: None\n",
      "validity: ważna jeszcze 16 dnido 14 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: None\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'część etatu']\n",
      "technologies: ['Python', 'TensorFlow', 'Keras', 'scikit-learn', 'NumPy', 'Pandas', 'Matplotlib', 'Kubernetes', 'Jenkins', 'Docker', 'GitLab', 'u klienta', 'koncentrujesz się na jednym projekcie', 'agile']\n",
      "responsibilities: ['Realizacja prac badawczo-rozwojowych i eksperymentalnych z zakresu sztucznej inteligencji i uczenia maszynowego', 'Projektowanie, implementowanie, testowanie, wdrażanie i utrzymywanie produktów opartych na modelach generatywnych, predykcyjnych, oraz innych technikach uczenia maszynowego', 'Wykorzystanie odpowiednich bibliotek Python lub Java do budowy algorytmów uczenia maszynowego oraz wdrażania wydajnych i skalowalnych przepływów pracy symulacji', 'Śledzenie najnowszych trendów branżowych w obszarze Machine Learning/ Artificial Intelligence', 'Komunikacja koncepcji technicznych zarówno odbiorcom technicznym, jak i nietechnicznym']\n",
      "requirements: ['Minimum 4 letnie doświadczenie w doświadczenia w projektach komercyjnych i/lub naukowo-badawczych związanych z praktycznym wykorzystaniem sztucznej inteligencji', 'Wykształcenie wyższe techniczne (kierunek Informatyka lub podobny)', 'Praktyczna wiedza z zakresu tworzenia modeli uczenia maszynowego oraz ich ewaluacji', 'Doświadczenie z dużymi modelami językowymi (LLM), embedingami oraz modelami GenAI', 'Dobra znajomość programowania w Pythonie i znajomość narzędzi do sztucznej inteligencji i analizy danych (np. Tensorflow ,Keras, scikit-learn, NumPy, Pandas, Matplotlib)', 'Praktyczna wiedza związana z zagadnieniami: analizy danych, raportowania, analizy statycznej, uczenia maszynowego oraz sztucznej inteligencji, itp.', 'Znajomość narzędzi i technologii wspierających CI/CD: Kubernetes, Jenkins, Docker, Gitlab/GitHab', 'Dobra znajomość tematyki Deep Learning, Natural Language Processing, Generative AI, Preference learning, koncepcji takich jak: Explainable AI, Responsible AI, Human-Centered AI', 'Znajomość języka angielskiego, umożliwiająca co najmniej efektywne zapoznawanie się z dokumentacją z zakresu IT', 'Praktycznej znajomości metodyki Agile']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ekspert-ds-algorytmow-sztucznej-inteligencji-warszawa,oferta,1003902990?apid=1e97d24c-f03e-49fd-b89b-e4bc1582a84c&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 329 ---\n",
      "id: 382\n",
      "url: https://www.pracuj.pl/praca/data-analyst-warszawa-pulawska-182,oferta,1003905809\n",
      "title: Data Analyst\n",
      "work_location: None\n",
      "validity: valid for 17 daysto 15 March 2025\n",
      "contract_type: contract of mandate, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Puławska 182, Mokotów, WarszawaWarszawa, Masovian']\n",
      "technologies: ['SQL', 'Tableau', 'Looker', 'Git', 'Python']\n",
      "responsibilities: ['Utilize SQL and/or Python to perform in-depth analysis using databases like Google Big Query', 'Assist local teams in achieving outstanding outcomes by offering insights and recommendations', 'Develop and maintain customized reporting and dashboards using tools like Tableau, Looker Studio, or similar platforms', 'Collaborate closely with Sales and Platform Managers to deliver exceptional Analytics service to clients', 'Conduct meta-analysis across campaigns to extract insights regarding campaign performance and user behavior', 'Propose and implement processes to enhance the efficiency of internal workflows']\n",
      "requirements: ['1-3 years of relevant experience, preferably in roles involving data analytics, focused on deriving insights and generating reports from real-world data', 'Proficient in SQL with experience in writing complex queries to extract, manipulate, and analyze large datasets', 'Experience with data visualization tools such as Tableau, Looker, PowerBI, or Looker Studio', 'Familiarity with GCP services, especially BigQuery, as well as Git and Python, is a strong plus', 'Ability to frame unstructured problems using available data, coupled with skills in visualizing and communicating results', 'Proficiency in traditional office applications like Excel and PowerPoint (or their Google Suite equivalents) is required', 'Understanding of basic statistics and hypothesis testing, including A/B testing and statistical significance', 'Familiarity with online media and/or e-commerce, encompassing expertise in web analytics tools (e.g., Google Analytics, Adobe), digital media sales models (CPM, CPC, CPA), and analysis of on-site user behavior will be an advantage', 'Fluent in English with the ability to present findings in a pedagogue manner', 'Bachelor Degree in a quantitative field (or non-quantitative field with significant quantitative experience). Master Degree or certifications in analytics, data science, or digital media/marketing are advantageous but not mandatory', 'Demonstrated flexibility and adaptability in dynamic, fast-paced environments', 'Strong willingness and ability to learn new tools, technologies, and processes quickly', 'Location- Warsaw or nearby (hybrid working model)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-analyst-warszawa-pulawska-182,oferta,1003905809?apid=c1776d73-8621-44c1-b63f-d80808293b68&oca=None&acv=0\n",
      "\n",
      "--- Job Record 330 ---\n",
      "id: 383\n",
      "url: https://www.pracuj.pl/praca/architekt-analityk-it-warszawa,oferta,1003902966\n",
      "title: Architekt - Analityk IT\n",
      "work_location: None\n",
      "validity: ważna jeszcze 16 dnido 14 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: None\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'część etatu']\n",
      "technologies: ['Java', 'Python', 'SQL', 'ELK Stack', 'MongoDB', 'Cassandra', 'Enterprise Architect', 'Archimate', 'BPMN', 'UML', 'Agile', 'u klienta', 'koncentrujesz się na jednym projekcie', 'koncentrujesz się na rozwoju produktu', 'agile']\n",
      "responsibilities: ['Budowanie architektury systemów IT w ramach realizowanego projektu B+R', 'Projektowanie rozwiązań, nadzór merytoryczny i wsparcie merytoryczne w realizacji projektu', 'Tworzenie technologicznej dokumentacji architektonicznej i analitycznej', 'Konsultacje z zespołami analitycznymi i developerskimi', 'Zarządzanie repozytorium architektonicznym oraz standardami IT', 'Tworzenie i konsultowanie pryncypiów architektonicznych', 'Wyznaczanie kierunków jakościowych w projekcie', 'Wsparcie analityczne podczas wytwarzania oprogramowania, współpraca z programistami, testerami, użytkownikami biznesowymi, administratorami', 'Wsparcie technologiczne w przygotowywaniu dokumentacji związanej z analizą systemową oraz analizą biznesową']\n",
      "requirements: ['Minimum trzyletnie doświadczenie na tożsamym stanowisku oraz min. 5 letnie doświadczenie programistyczne i analityczne', 'Wykształcenie wyższe techniczne (kierunek Informatyka lub podobny)', 'Praktyczna znajomość zagadnień związanych z zarządzaniem Architekturą IT, w szczególności w zakresie Architektury Aplikacji i Danych', 'Wiedza praktyczna i umiejętności programowania z zakresu wielu technologii w tym między innymi: Java, Python, SQL (np. Oracle, PostgreSQL) i NoSQL(np. Mongo DB, Cassandra, Driud), ELK Stack.', 'Praktyczna wiedza związana z zagadnieniami: walka radioelektroniczna, systemy radarowe, drony (obsługa, urządzenia peryferyjne, oprogramowanie), komunikacja bezprzewodowa, analiza danych, przetwarzanie obrazów, sztuczna inteligencja, itp.', 'Znajomość narzędzia Enterprise Architect. Umiejętność świadomego korzystania z notacji ArchiMate (2.0/3.0), BPMN (2.0) i UML (tworzenie dokumentacji)', 'Znajomość zagadnień z obszaru integracji systemów zarówno „wysokopoziomowych” jak i rozwiązań „embedded”', 'Znajomość języka angielskiego, umożliwiająca co najmniej efektywne zapoznawanie się z dokumentacją z zakresu IT', 'Praktyczna znajomość metodyki Agile']\n",
      "application_link: https://www.pracuj.pl/aplikuj/architekt-analityk-it-warszawa,oferta,1003902966?apid=227e62bc-df50-41d1-9c4f-36aef6f94828&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 331 ---\n",
      "id: 384\n",
      "url: https://www.pracuj.pl/praca/data-scientist-branza-produkcyjna-krakow-cementowa-2,oferta,1003885998\n",
      "title: Data Scientist (branża produkcyjna)\n",
      "work_location: None\n",
      "validity: ważna jeszcze 4 dnido 02 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Cementowa 2, Nowa Huta, KrakówKraków, małopolskie']\n",
      "technologies: ['Python', 'R', 'SQL', 'Qlik Sense', 'Looker', 'Salesforce', 'SAP', 'wewnątrz organizacji', 'rozwijasz kilka projektów jednocześnie', 'agile', 'scrum']\n",
      "responsibilities: ['Analiza sprzedaży: tworzenie modeli analitycznych mających na celu zrozumienie i przewidywanie trendów sprzedażowych, identyfikowanie szans i ryzyk.', 'Rozwój strategii sprzedażowych: wsparcie zespołów sprzedażowych i marketingowych w tworzeniu strategii ukierunkowanych na wzrost sprzedaży i ekspansję rynkową.', 'Analiza segmentów rynku: opracowywanie zaawansowanych raportów i analiz dla segmentów: Cement, Kruszywa, Betony oraz Suche Mieszanki.', 'Wsparcie w optymalizacji procesów sprzedażowych i produkcyjnych.', 'Modelowanie predykcyjne: tworzenie modeli statystycznych i predykcyjnych mających na celu przewidywanie popytu, zachowań klientów oraz potencjalnych barier rynkowych.', 'Raportowanie: regularne przygotowywanie raportów dla Zarządu oraz zespołów sprzedażowych, uwzględniających rekomendacje oraz wnioski.', 'Współpraca z innymi działami firmy.']\n",
      "requirements: ['Wykształcenie wyższe w dziedzinie analizy danych, statystyki, matematyki, informatyki lub pokrewnej.', 'Minimum 3 lata doświadczenia na podobnym stanowisku w branży produkcyjnej lub budowlanej.', 'Znajomość języka angielskiego na poziomie B2/ C1 oraz j. polskiego na poziomie min. C1.', 'Znajomość narzędzi analitycznych, takich jak Python, R, SQL oraz narzędzi do wizualizacji danych (Qlik Sense, Looker, etc.).', 'Doświadczenie w implementacji algorytmów sztucznej inteligencji (AI) w analizie danych w obszarach sprzedaży, segmentacji klientów i predykcji popytu.', 'Praktyczna znajomość technik uczenia maszynowego (machine learning) oraz głębokiego uczenia (deep learning).', 'Doświadczenie w pracy z danymi sprzedażowymi i rynkowymi.', 'Jesteśmy otwarci na pracowników również z innych lokalizacji w Polsce.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-branza-produkcyjna-krakow-cementowa-2,oferta,1003885998?apid=48bb82c3-82e0-4753-8f5c-d2c20bf530c8&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 332 ---\n",
      "id: 385\n",
      "url: https://www.pracuj.pl/praca/ekspert-ds-analizy-danych-warszawa,oferta,1003902944\n",
      "title: Ekspert ds. Analizy Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 16 dnido 14 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: None\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'część etatu']\n",
      "technologies: ['Python', 'Java', 'SQL', 'Oracle', 'PostgreSQL', 'MongoDB', 'Cassandra', 'Pandas', 'u klienta', 'koncentrujesz się na jednym projekcie', 'koncentrujesz się na rozwoju produktu', 'agile']\n",
      "responsibilities: ['Projektowanie i wdrażanie algorytmów dotyczących gromadzenia, analizowania i interpretowania dużych zbiorów danych', 'Opracowanie efektywnych metod syntezy i priorytetyzowania informacji pozyskiwanych z sensorów na podstawie uzyskanej fuzji danych', 'Stosowanie zaawansowanych technik symulacji, aby przyspieszyć symulacje i poprawić dokładność opracowywanych danych i raportów', 'Wykorzystywanie odpowiednich bibliotek Python lub Java do wdrażania wydajnych i skalowalnych przepływów pracy symulacji', 'Komunikacja koncepcji technicznych zarówno odbiorcom technicznym, jak i nietechnicznym', 'Optymalizacja procesów: Wdrażanie i usprawnianie narzędzi oraz metod analitycznych w ramach realizowanego projektu']\n",
      "requirements: ['Minimum 5 letnie doświadczenie w doświadczenia w projektach komercyjnych i/lub naukowo-badawczych związanych z programowaniem oraz analizą danych', 'Wykształcenie wyższe techniczne (kierunek Informatyka lub podobny)', 'Wiedza praktyczna i umiejętności programowania z zakresu wielu technologii w tym między innymi: Java, Python oraz bazami danych i znajomością SQL (np. Oracle, PostgreSQL) i NoSQL(np. Mongo DB, Cassandra, Driud)', 'Dobra znajomość programowania w Pythonie i narzędzi do walidacji i analizy danych (np. Pydantic, NumPy, Pandas, Matplotlib)', 'Praktyczna wiedza związaną z zagadnieniami: analiza danych, raportowanie, analizy statycznej, uczenia maszynowego oraz sztucznej inteligencji, itp.', 'Umiejętność prowadzenia analiz metod detekcji ruchu i śledzenia obiektów ruchomych w obrazie cyfrowym, klasyfikacji i identyfikacji obiektów na podstawie cech dystynktywnych obrazów akwizowanych z sensorów użytych w projekcie', 'Umiejętność opisywania danych strumieniowych oraz integracji z czujnikami wideo-radarowymi', 'Znajomość języka angielskiego, umożliwiająca co najmniej efektywne zapoznawanie się z dokumentacją z zakresu IT', 'Praktyczna znajomość metodyki Agile']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ekspert-ds-analizy-danych-warszawa,oferta,1003902944?apid=c5125be2-362c-4b9b-a2f6-a10ed36f0669&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 333 ---\n",
      "id: 386\n",
      "url: https://www.pracuj.pl/praca/data-engineer-krakow,oferta,1003920211\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['KrakówKraków, Lesser Poland', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Python', 'Java', 'Scala', 'Spark', 'Hadoop', 'Kafka', 'AWS', 'Google Cloud Platform', 'Terraform', \"at the client's site\", 'agile']\n",
      "responsibilities: ['Design, develop, and maintain robust data platform solutions and features from concept to production;', 'Build scalable systems for processing and organizing massive volumes of data;', 'Collaborate with cross-functional teams, including Product Managers, Data Engineers/Analysts, and Software Engineers, to deliver innovative data solutions;', 'Evaluate and integrate new technologies to improve platform performance and reliability continuously;', 'Ensure high-quality deliverables through rigorous unit and integration testing;', 'Participate in system monitoring, on-call rotations, and troubleshooting to maintain platform stability;', 'To be a part of an international team that participates in various customer projects delivered all around the world.']\n",
      "requirements: ['A passion for designing and building scalable, high-performance data platforms;', '3+ years of experience in backend or data engineering roles;', 'Proficiency in programming languages such as Python, Java, or Scala;', 'Solid experience with big data technologies like Spark, Hadoop, or Kafka;', 'Strong problem-solving skills, a positive attitude, and a willingness to learn and take on challenges;', 'Ability to independently drive feature development from concept to delivery;', 'Strong English skills (C1).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-krakow,oferta,1003920211?apid=5cb3fb80-3316-4d9b-afd1-af60904c2167&oca=None&acv=0\n",
      "\n",
      "--- Job Record 334 ---\n",
      "id: 387\n",
      "url: https://www.pracuj.pl/praca/master-data-specialist-wroclaw-legnicka-48,oferta,1003914796\n",
      "title: Master Data Specialist\n",
      "work_location: None\n",
      "validity: valid for 22 daysto 20 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: full office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Legnicka 48, Fabryczna, WrocławWrocław, Lower Silesia', 'contract of employment']\n",
      "technologies: ['SAP', 'Tableau', 'SQL']\n",
      "responsibilities: ['First time good quality material master data maintenance, analyses, and clean-up activities in ERP systems,', 'Tickets resolution within agreed SLA,', 'Keeping documentation for realizing processes updated,', 'Responding to queries from range of Stakeholders using various means of communication,', 'Performing Internal Controls,', 'Supporting development and optimization of Master Data procedures, policies, standards and tools/systems,', 'Develop understanding of all master data areas and assist other department and or project,', 'Performing other ad-hoc tasks assigned by Team Leader and/or Manager,', 'Challenging old patterns, taking a part in projects related to Master Data.']\n",
      "requirements: ['Previous, minimum 1 year of experience in Master Data,', 'High communication skills - very good English,', 'Good knowledge of SAP ECC/S4 HANA (preferable in the Master Data environment, OTC, FICO knowledge is a plus),', 'Good Excel skills - reporting, macros, connecting excel with legacy system,', 'Quick learner,', 'Team player,', 'Details oriented – accuracy, data consistency,', 'Adaptability to changes,', 'Ability to work independently and under time pressure,', 'Desire to develop in the Master Data field for a long-term perspective,', 'Experience in PowerBi, Tableau, SQL will be an asset,', 'Experience in SSC.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/master-data-specialist-wroclaw-legnicka-48,oferta,1003914796?apid=24f1b4a0-6d5f-4478-8191-a0c639e699d7&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 335 ---\n",
      "id: 388\n",
      "url: https://www.pracuj.pl/praca/ai-engineer-automation-innovation-department-warszawa-marynarska-12,oferta,1003871874\n",
      "title: AI Engineer - Automation & Innovation Department\n",
      "work_location: None\n",
      "validity: ważna jeszcze 2 dnido 28 lutego 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Marynarska 12, Mokotów, WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatów']\n",
      "technologies: ['Python', 'SQL', 'AWS', 'Azure', 'Jenkins', 'Docker', 'Kubernetes', 'Pandas', 'Celery', 'Django', 'MongoDB', 'PostgreSQL', 'Oracle', 'Whisper', 'scrum']\n",
      "responsibilities: ['Tworzenie architektury, projektowanie i wdrażanie produktów i usług AI', 'Rozwój i utrzymanie rozwiązań w dziedzinie sztucznej inteligencji oraz ich optymalizacja w oparciu o najnowsze trendy i badania', 'Zapewnienie zgodności rozwiązań z regulacjami dotyczącymi bezpieczeństwa i ochrony danych', 'Zbieranie wymagań od interesariuszy i ich realizacja']\n",
      "requirements: ['Min. 2-3 lata doświadczenia w pracy jako AI Engineer w projektach związanych z implementacją rozwiązań AI, w szczególności pracy z dużymi modelami językowymi (LLM), GenAI', 'Doświadczenie w budowaniu chatbotów', 'Biegła znajomość Python i SQL', 'Rozumienie koncepcji RAG, prompt engineeringu, frameworków takich jak LangChain', 'Doświadczenie w pracy z technologiami chmurowymi: AWS, Azure', 'Znajomość architektury mikroserwisowej oraz CI/CD (Jenkins, Docker, Kubernetes)', 'Znajomość frameworków, takich jak:  Pandas, Celery, Django', 'Znajomość relacyjnych baz danych (MongoDB, PostgreSQL, Oracle), oraz umijętność przygotowania danych do budowy modeli AI', 'Umiejętność rozwiązywania problemów i poszukiwania nowych rozwiązań', 'Nastawienie na stosowanie podejścia Customer Centric w codziennej pracy', 'Umiejętność pracy zespołowej oraz komunikacji', 'Proaktywność w nauce i inspirowanie innych', 'Bardzo dobra znajomość języka angielskiego (min. B2)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-engineer-automation-innovation-department-warszawa-marynarska-12,oferta,1003871874?apid=e369214c-186f-431c-ba28-298d1f35a495&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 336 ---\n",
      "id: 389\n",
      "url: https://www.pracuj.pl/praca/ai-mlops-engineer-gdansk-aleja-grunwaldzka-415,oferta,1003881722\n",
      "title: AI & MLOps Engineer\n",
      "work_location: None\n",
      "validity: valid for 7 daysto 05 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: full office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Aleja Grunwaldzka 415, Oliwa, GdańskGdańsk, Pomeranian', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['Azure Machine Learning', 'Kubernetes', 'Git', 'in house', 'you focus on a single project at a time', 'you develop the code \"from scratch\"', 'agile', 'devOps', 'data scientist']\n",
      "responsibilities: ['Build and maintain MLOps- and LLMOps frameworks utilizing the Microsoft Azure hyperscaler (using Azure AI/ML services like Machine Learning Studio, Prompt Flow, Azure OpenAI)', 'Deploy- and manage cloud-based AI- and ML infrastructure using Infrastructure-as-Code', 'Ensure security- and compliance measures and industry best practices are implemented in our AI- and ML based applications and continuously improve these', 'Collaborate in an agile Scrum team to create and deploy efficient, scalable AI/ML-based applications', 'Build and manage workflows and deployments on Kubernetes ensuring smooth integration with our processes and guidelines', 'Improve our existing machine learning and data processing pipelines for NOTAM data management', 'Collaborate with other teams across the company in building reuseable LLMOps- and MLOps frameworks, processes and tools']\n",
      "requirements: ['Academic background in software development, computer science, or related field', 'Proven experience in building, deploying and operating cloud-based AI- and ML solutions', 'Familiarity with cloud services and AI/ML platforms like Azure ML Studio and Azure OpenAI', 'Proficiency in managing cloud infrastructure using Infrastructure-as-Code, preferably using Terraform', 'Proficiency in deploying applications using Kubernetes, Helm and containers', 'Knowledge of microservice architectures', 'Familiarity with Git, CI/CD and Linux', 'Strong English communication skills and teamwork orientation', 'Experience with Agile methodologies and data-focused Scrum', 'Readiness to work from the office in Gdańsk min. 2 days a week', 'Availability to travel.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-mlops-engineer-gdansk-aleja-grunwaldzka-415,oferta,1003881722?apid=ae7f3c80-78ae-4342-9719-9b84acd636dc&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 337 ---\n",
      "id: 390\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-warszawa-dluga-29,oferta,1003884634\n",
      "title: Senior Data Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 8 dnido 06 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Długa 29, Śródmieście, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['DataBricks', 'Delta Lake', 'Pyspark', 'Lakeflow', 'Delta', 'Microsoft Power BI', 'u klienta', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu']\n",
      "responsibilities: ['Pozyskiwanie i przekształcanie danych (wsadowych i strumieniowych) na platformie Databricks Lakehouse', 'Praca z Databricks Workflows w celu uzyskania dostępu do wykresów przepływu danych, pulpitów nawigacyjnych śledzących kondycję i wydajność zadań produkcyjnych oraz potoków Delta Live Tables', 'Monitorowanie i wizualizacja wydajności, jakości danych i wskaźników niezawodności za pomocą tabel Delta Lake', 'Orkiestracja różnorodnych obciążeń w pełnym cyklu życia, w tym Delta Live Tables, PySpark i innych']\n",
      "requirements: ['Minimum 5 lat doświadczenia w pracy na podobnych stanowiskach', 'Doświadczenie z: DataBricks, Delta Lake, Pyspark, Lakeflow, Delta live tables', 'Umiejętność korzystania z databricks jako rozwiązania lakehouse', 'Doświadczenie w modelowaniu danych', 'Język angielski umożliwiający swobodną komunikację (min B2/C1)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-warszawa-dluga-29,oferta,1003884634?apid=34879a67-672c-41da-9086-3926b6449a5b&oca=None&acv=0\n",
      "\n",
      "--- Job Record 338 ---\n",
      "id: 391\n",
      "url: https://www.pracuj.pl/praca/intern-master-data-krakow-aleja-generala-tadeusza-bora-komorowskiego-25c,oferta,1003914691\n",
      "title: Intern, Master Data\n",
      "work_location: None\n",
      "validity: valid for 22 daysto 20 March 2025\n",
      "contract_type: None\n",
      "employment_type: None\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Aleja Generała Tadeusza Bora-Komorowskiego 25c, Prądnik Czerwony, KrakówKraków, Lesser Poland', 'internship / apprenticeship contract', 'part time', 'trainee', 'Запрошуємо працівників з України']\n",
      "technologies: []\n",
      "responsibilities: ['Currently studying', 'Strong motivation to gain experience in Finance (Master Data)', 'Strong engagement and focus on achieving goals', 'Caring for details', 'Proactive and creative in terms of improving ways of working', 'Good knowledge of Microsoft Excel', 'Very good English (B2)', 'Availability: minimum 30 hours per week']\n",
      "requirements: []\n",
      "application_link: https://www.pracuj.pl/aplikuj/intern-master-data-krakow-aleja-generala-tadeusza-bora-komorowskiego-25c,oferta,1003914691?apid=b75035f4-bb20-4590-99d1-905494a67a64&oca=None&acv=0\n",
      "\n",
      "--- Job Record 339 ---\n",
      "id: 392\n",
      "url: https://www.pracuj.pl/praca/senior-data-quality-specialist-krakow-aleja-jana-pawla-ii-43a,oferta,1003920109\n",
      "title: Senior Data Quality Specialist\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['aleja Jana Pawła II 43a, Czyżyny, KrakówKraków, Lesser Poland', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['SAP']\n",
      "responsibilities: ['supporting defining DQ strategy & roll out plan for the DQ Factory', 'collection of DQ demand based on OpCos, business & functional owner requirements to address the most critical business issues', 'translating the business needs into actionable plans on how to improve DQ', 'prioritizing projects vs resources & propose DQ agenda & planning', 'providing ongoing, regular feedback to team members & empower their professional growth', 'liasing with business, functional owners & SMEs to be a point of contact for all DQ related topics', 'proposing DQ KPIs underpinning HEINEKEN ambition to become the best connected Brewer', 'providing regular DQ trend analysis & insights to DM LT', 'operationalizing our data quality services by developing, continuously improving and extending processes and procedures (local, regional and global) for data quality monitoring by embedding use of new DQ tooling and PowerBI dashboards for detection and monitoring of data quality issues', 'establishing and running operational delivery of KPIs for data quality that are accurate, efficient, fact based, and in line with defined timelines', 'custodianing of OpCo engagement materials for data quality that will be used during rollout of change and during demo days. Ensuring materials are maintained so that they are up to date and accurate and are available when required', 'custodianing of data quality communication plan and schedule. Ensuring this is maintain and available as required', 'co-ordinating change pipeline and delivery of actions arising in accordance with agreed SLAs', 'providing expertise to support design and roll out of data quality error prevention Thereafter, operationally run service', 'providing expertise to support design and roll out of data quality rule change management Thereafter, operationally run service in collaboration colleagues supporting change management for related assets', 'facilitate data quality forums with colleagues from OpCos to inform improvement and maturating of data services', 'delivering analysis for data quality as required', 'other tasks assigned by Line Manager']\n",
      "requirements: ['Data engineering or Computer science degree or equivalent', \"5+ years' experience in a data quality analyst role in a large enterprise with a diverse and complex set of business applications and data (or consultancy experience).\", 'proven record of Master data knowledge', 'proven record of SAP experience', 'proven record of leading big projects', 'proven record of leading teams', 'proven experience in data analysis role; specifically, for data quality assessment and lifecycle management in a business driven outcome environment', 'knowledge of data quality processes and workflow, and proven experience of using tools for data profiling, discovery and assessment', 'the ability to see the big picture, prioritise and define / deliver realistic plans', 'operational experience of designing and creating DQ dashboards for different stakeholder groups', 'ideally experience of working within the framework of an Enterprise Data Model', 'a continuous and innovation improvement mindset']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-quality-specialist-krakow-aleja-jana-pawla-ii-43a,oferta,1003920109?apid=431f968e-68ca-4684-b7d3-96f4a8c64be9&oca=None&acv=0\n",
      "\n",
      "--- Job Record 340 ---\n",
      "id: 393\n",
      "url: https://www.pracuj.pl/praca/junior-data-analyst-contact-centre-katowice-wroclawska-54,oferta,1003919811\n",
      "title: Junior Data Analyst (Contact Centre)\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: junior specialist (Junior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Wrocławska 54, KatowiceKatowice, Silesian']\n",
      "technologies: ['Microsoft Office', 'Microsoft Excel', 'Microsoft Power BI', 'SQL', 'in house', 'agile', 'scrum']\n",
      "responsibilities: ['College diploma or university degree in Business, Data Analytics, or a related field (preferred)', '1-2 years of experience in a contact centre, data analysis, or analytical support role', 'English C1 required', 'Experience working with Microsoft O365 (Excel and Power BI)', 'Strong attention to detail to ensure data accuracy', 'Ability to manage multiple tasks, prioritize, and meet deadlines', 'Good communication skills, especially when handling system access and reporting updates', 'Technical aptitude to troubleshoot reporting and system access issues']\n",
      "requirements: ['Familiarity with Genesys Engage or similar contact centre reporting platforms', 'Basic knowledge of SQL or data querying']\n",
      "application_link: https://www.pracuj.pl/aplikuj/junior-data-analyst-contact-centre-katowice-wroclawska-54,oferta,1003919811?apid=d88e9ddd-a8e0-4b8e-aea0-ee2fa0d0ece6&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 341 ---\n",
      "id: 394\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-sztucznej-inteligencji-programista-ai-bialystok-handlowa-6g,oferta,1003905216\n",
      "title: Specjalista ds. sztucznej inteligencji (Programista AI)\n",
      "work_location: None\n",
      "validity: ważna jeszcze 17 dnido 15 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Handlowa 6G, Nowe Miasto, BiałystokBiałystok, podlaskie', 'praca stacjonarna']\n",
      "technologies: ['TensorFlow', 'PyTorch', 'scikit-learn', 'OpenCV', 'Pillow', 'Python', 'wewnątrz organizacji', 'koncentrujesz się na jednym projekcie', 'masz wpływ na wybór narzędzi i technologii', 'koncentrujesz się na rozwoju produktu', 'koncentrujesz się na utrzymaniu kodu']\n",
      "responsibilities: ['tworzenie i optymalizacja algorytmów przetwarzania obrazów,', 'wprowadzanie wybranych rozwiązań AI,', 'monitorowanie działania wdrożonych rozwiązań i wprowadzanie usprawnień.']\n",
      "requirements: ['umiejętność analitycznego myślenia i rozwiązywania problemów,', 'znajomość klasycznych metod ML,', 'znajomość narzędzi AI (TensorFlow, PyTorch, scikit-learn),', 'znajomość nowoczesnych technik przetwarzania obrazów, w tym segmentacji,', 'znajomość języka angielskiego (praca z dokumentacją techniczną, narzędziami AI),', 'znajomość programowania w języku Python.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-sztucznej-inteligencji-programista-ai-bialystok-handlowa-6g,oferta,1003905216?apid=a1115996-3d9a-4c7b-92f0-ecbf82791e9f&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 342 ---\n",
      "id: 395\n",
      "url: https://www.pracuj.pl/praca/senior-machine-learning-engineer-warszawa,oferta,1003865747\n",
      "title: Senior Machine Learning Engineer\n",
      "work_location: Company locationWarszawaWarszawa, Masovian\n",
      "validity: None\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'valid 5 hours', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Python', 'Visual Studio Code', 'Jupyter Notebooks', 'Hadoop', 'Spark', 'Kafka', 'Google Cloud Platform', 'GitHub Actions', 'Harness', 'Jenkins', 'Docker', 'Kubernetes', 'Java', 'C++', 'Scala', \"at the client's site\", 'you focus on a single project at a time', 'you have influence on the choice of tools and technologies', 'you have influence on the product', 'you develop the code \"from scratch\"', 'you focus on product development', 'scrum']\n",
      "responsibilities: ['Implement end-to-end solutions for batch and real-time algorithms along with requisite tooling around monitoring, logging, automated testing, model retraining, model deployment and metadata tracking', 'Identify new opportunities to improve business processes and improve consumer experiences, and prototype solutions to demonstrate value with a crawl, walk, run mindset', 'Work with data scientists and analysts to create and deploy new product features on the e-commerce website, in-store portals, and mobile app', 'Establish scalable, efficient, automated processes for data analyses, model development, validation and implementation', 'Write efficient and scalable software to ship products in an iterative, continual-release environment', 'Contribute to and promote good software engineering practices across the team and build cloud-native software for ML pipelines', 'Contribute to and re-use community best practices', 'Embody the values and passions with empathy to engage with colleagues from multiple backgrounds']\n",
      "requirements: ['University or advanced degree in Engineering, Computer Science, Mathematics, or related field', '7+ years in software development, with strong experience in developing and deploying ML systems in large data science environments', 'Proficient in Python, with tools like VSCode and Jupyter Notebooks; familiarity with Java, C++, or Scala is a plus', 'Experienced with big data tools (Hadoop, Spark, Kafka) and cloud platforms, preferably GCP with serverless architecture', 'Expertise in CI/CD processes using GitHub Actions, Harness, Jenkins', 'Skilled in data warehousing (Google BigQuery) and independently managing Kubeflow pipelines', 'Knowledgeable in feature engineering, feature stores, and audit processes', 'Strong foundation in software engineering practices, including testing, CI, code reviews, and documentation', 'Experience with ML orchestration systems (Kubeflow, Step Functions, ML Flow, Airflow, TFX) and containerization (Docker, Kubernetes)', 'Strong reasoning to balance solution vs risk taking']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-machine-learning-engineer-warszawa,oferta,1003865747?apid=751a5d74-5870-418a-b778-28dee8f62b32&oca=None&acv=0\n",
      "\n",
      "--- Job Record 343 ---\n",
      "id: 396\n",
      "url: https://www.pracuj.pl/praca/administrator-baz-danych-warszawa-moldawska-9,oferta,1003881189\n",
      "title: Administrator baz danych\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 7 dnido 05 marca 2025\n",
      "contract_type: umowa o dzieło, umowa zlecenie, kontrakt B2B\n",
      "employment_type: None\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['część etatu']\n",
      "technologies: ['AI']\n",
      "responsibilities: ['Tworzenie i organizacja bazy wiedzy – gromadzenie, selekcjonowanie i kategoryzowanie wiedzy medycznej, dietetycznej, psychologicznej i coachingowej na potrzeby AI.', 'Aktualizowanie i walidacja treści – śledzenie badań naukowych, trendów i najlepszych praktyk, aby baza była zawsze rzetelna i aktualna.', 'Współpraca z ekspertami – nawiązywanie relacji z lekarzami, dietetykami, psychoterapeutami i innymi specjalistami, by weryfikować dane i zapewniać najwyższą jakość merytoryczną.', 'Praca nad personalizacją wiedzy w AI – współpraca z zespołem tech, aby model AI dostarczał spersonalizowane, trafne rekomendacje użytkowniczkom.', 'Dbanie o strukturę i intuicyjność bazy danych – tak, aby treści były logiczne, łatwo przeszukiwalne i zgodne z celem aplikacji.']\n",
      "requirements: ['Masz wiedzę i doświadczenie w jednym (lub kilku) z tych obszarów:', 'Warunek konieczny: Zdrowie kobiet (ginekologia, endokrynologia, dietetyka, medycyna funkcjonalna, psychologia zdrowia lub inne life sciences)', 'Zarządzanie danymi, baza wiedzy, analityka danych zdrowotnych', 'AI & NLP – jeśli masz doświadczenie w pracy z danymi do AI, będzie to ogromnym plusem', 'Tworzenie treści eksperckich – jeśli potrafisz pisać w sposób zrozumiały, czytelny i rzetelny', 'To praca dla Ciebie, jeśli:', 'Interesujesz się zdrowiem kobiet i chcesz realnie wpływać na ich dobrostan', 'Masz umiejętność organizowania dużej ilości wiedzy w przystępną i logiczną strukturę', 'Czujesz, że technologia może wspierać zdrowie, ale musi być mądra i rzetelna', 'Chcesz pracować w środowisku pełnym pasji, otwartości i możliwości eksperymentowania', 'Lubisz autonomię i możliwość realnego wpływu na produkt, zamiast tylko wykonywać zlecone zadania']\n",
      "application_link: https://www.pracuj.pl/aplikuj/administrator-baz-danych-warszawa-moldawska-9,oferta,1003881189?apid=5300d87c-974c-45be-a669-f597eeb0a35c&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 344 ---\n",
      "id: 397\n",
      "url: https://www.pracuj.pl/praca/senior-data-architect-warszawa-postepu-15,oferta,1003884045\n",
      "title: Senior Data Architect\n",
      "work_location: Company locationPostępu 15, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for 8 daysto 06 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior), expert\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['Microsoft Azure', 'Microsoft Power BI', 'SQL', 'R', 'Python', 'in house', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'agile']\n",
      "responsibilities: ['Identify Core Data Elements & Entities.', 'Create and design every aspect of a database, including conceptual and logical data models and data flow diagrams using a diagram drawing tool.', 'Understand, document, and maintain the data lineage for Finance Data Products in scope.', 'Ensure that data is Findable, Accessible, Interoperable, and Reusable.', 'Identify areas for improvement in current Data Products.', 'Find commonalities in existing data products, and create one foundational, universal, F.A.I.R. model to satisfy the needs of many Products with one Dataset.', 'Create documentation for existing Data Products in the Finance domain. Retrofit existing Data Products to be compliant with Company standards documentation-wise.', 'Identify data duplication between existing data platforms and design new data models to deduplicate common datasets', 'Make sure that the data products are properly cataloged and follow the definition of done principles.', 'Collaborate with other Data Product Managers to work on cross-domain data sharing, identify the needs of products, and make sure data contracts are in place.', 'Keep the confidential data secure by identifying the security rules & requirements and working with the Solution Architecture Team on implementing them.', 'Collaborate with senior Stakeholders to understand the Business needs and improve existing data models.', 'Collaborate with Data Stewards to understand the business requirements for Data Quality improvements and work with Data Engineers to implement these into metadata on the Platform.', 'Be part of a Data Governance chapter and collaborate with the larger team within the Data and Analytics department.']\n",
      "requirements: ['10+ years of experience with 6-8 years working with Data Business analysis or Data & Analytics products.', 'Data management skills.', 'Experience with Microsoft Azure tech stack (Databricks, ADO, ADF, Unity Catalog, Databricks).', 'Understanding of Data Mesh concept and F.A.I.R. principle.', 'Knowledge and understanding of Data Layers (Medallion Architecture) and Kimball Dimensional Modelling.', 'Data modeling skills (conceptual & logical data models, defining keys and dimensional vs transactional tables relations, normalized vs denormalized data).', 'Understanding of PowerBI, and basic SQL/R/Python skills.', 'Superior analytical skills and attention to detail.', 'Creative problem-solving abilities and unconstrained thinking approach.', 'Ability to work in a high-paced, dynamic environment, change management skills.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-architect-warszawa-postepu-15,oferta,1003884045?apid=d109b50c-9c13-43d6-89a2-d5ee062d4644&oca=None&acv=0\n",
      "\n",
      "--- Job Record 345 ---\n",
      "id: 398\n",
      "url: https://www.pracuj.pl/praca/data-engineer-oracle-odi-katowice,oferta,1003884021\n",
      "title: Data Engineer - Oracle ODI\n",
      "work_location: Company locationKatowiceKatowice, Silesian\n",
      "validity: valid for 8 daysto 06 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['Oracle', 'Azure DevOps', 'Maven', 'Grafana', 'Elasticsearch', 'Kibana', 'ServiceNow', 'Confluence', 'in house', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'agile']\n",
      "responsibilities: ['Developing, maintaining, and supporting data pipelines.', 'Working within the framework of IT risk management.']\n",
      "requirements: ['A proven track record of being a team leader and mentor, readily assisting colleagues with challenges.', 'Familiarity with CI/CD tools like Azure DevOps and Maven.', 'Experience with data visualization tools such as Grafana, Elastic, and Kibana.', 'Experience working within complex, corporate environments, particularly in Lending or Financial systems.', 'Proficiency with issue tracking systems (e.g., ServiceNow) and collaboration tools (e.g., Confluence).', 'Proactive in identifying and implementing improvements and innovations.', 'Adept at finding alternative solutions.', 'Open-minded, eager to learn, and committed to developing strong working relationships.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-oracle-odi-katowice,oferta,1003884021?apid=4403c2ee-8867-413a-8428-9fd55dde3745&oca=None&acv=0\n",
      "\n",
      "--- Job Record 346 ---\n",
      "id: 399\n",
      "url: https://www.pracuj.pl/praca/data-engineer-gdansk-aleja-grunwaldzka-415,oferta,1003904976\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 17 daysto 15 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: full office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Aleja Grunwaldzka 415, Oliwa, GdańskGdańsk, Pomeranian', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['in house', 'you focus on a single project at a time', 'you develop the code \"from scratch\"', 'agile', 'data scientist']\n",
      "responsibilities: ['Develop and optimize Machine-Learning algorithms for aeronautical data processing', 'Design and deploy and test prompts (Prompt-Flow) for aeronautical data parsing using LLMs', 'Design and implement data-driven models and pipelines using Python programming language', 'Collaborate in an agile Scrum team to create and deploy efficient, scalable data and application solutions', 'Build and manage workflows and deployments on Kubernetes and Azure ML Studio (or similar platforms) to support our Lido NOTAM-AI project', 'Improve our existing machine learning and data processing pipelines for NOTAM data management', 'Collaborate on software architecture and design solutions to enhance our workflows and processes']\n",
      "requirements: ['Academic background in data science, computer science, or related field', 'Ability to use and implement state-of-the-art generative AI solutions; especially when introducing such to big-corporate and assessing associated risks', 'Knowledge of natural language processing, tools and libraries', 'Ability to implement MLOps Pipelines in cloud environments such as Azure (ML-Studio) and GCP', 'Familiarity with SQL', 'Strong English communication skills and teamwork orientation', 'Experience with Agile methodologies and data-focused Scrum', 'Readiness to work from the office in Gdańsk min. 2 days a week', 'Availability to travel']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-gdansk-aleja-grunwaldzka-415,oferta,1003904976?apid=3d6109bb-eafd-4279-ac3a-b5d563ed77bd&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 347 ---\n",
      "id: 400\n",
      "url: https://www.pracuj.pl/praca/starszy-programista-sql-lodz,oferta,1003881065\n",
      "title: Starszy Programista SQL\n",
      "work_location: None\n",
      "validity: ważna jeszcze 7 dnido 05 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['ŁódźŁódź, łódzkie']\n",
      "technologies: ['T-SQL', 'MS SQL Server', 'SQL Server Integration Services', 'Crystal Raports', 'ERP', 'WebServices', 'JSON', 'C#', 'Java', 'Python', 'Visual Basic', 'MySQL', 'PostgreSQL', 'wewnątrz organizacji', 'rozwijasz kilka projektów jednocześnie']\n",
      "responsibilities: ['Rozwój kluczowych aplikacji biznesowych - projektowanie i weryfikację rozwiązań', 'Rozwój systemu ERP i wymiana danych z zewnętrznymi systemami (API)', 'Utrzymanie ciągłości pracy systemu ERP i aplikacji biznesowych.', 'Optymalizacja aktualnych rozwiązań', 'Tworzenie aplikacji dla użytkownika wewnętrznego', 'Tworzenie i modyfikacja raportów (ERP i Power BI)', 'Uczestniczenie w innowacyjnych i ciekawych projektach']\n",
      "requirements: ['Wykształcenie wyższe i/lub kliku lat pracy na podobnym stanowisku', 'Bardzo dobrej znajomości T-SQL', 'Znajomości bazy danych MS SQL Server', 'Tworzenia rozwiązań opartych na SQL Server Integration Services', 'Umiejętności analitycznego myślenia', 'Dobra znajomości j. angielskiego (rozumienie dokumentacji technicznej)', 'Odpowiedzialności, bardzo dobrej organizacji pracy i zaangażowania', 'Samodzielności w wykonywaniu powierzonych zadań']\n",
      "application_link: https://www.pracuj.pl/aplikuj/starszy-programista-sql-lodz,oferta,1003881065?apid=fd24b5f1-977c-4fe0-9291-91ef6009ba74&oca=None&acv=0\n",
      "\n",
      "--- Job Record 348 ---\n",
      "id: 401\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-analiz-i-wdrozen-poznan-plac-kolegiacki-17,oferta,1003904931\n",
      "title: Specjalista ds. analiz i wdrożeń\n",
      "work_location: None\n",
      "validity: ważna jeszcze 16 dnido 14 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['plac Kolegiacki 17, Stare Miasto, PoznańPoznań, wielkopolskie', 'praca stacjonarna']\n",
      "technologies: ['SQL', 'BPMN']\n",
      "responsibilities: ['ustalanie potrzeb użytkowników w zakresie rozwiązań informatycznych w jednostkach organizacyjnych Urzędu i Miasta', 'współpraca z wykonawcami systemów informatycznych w ramach analizy funkcjonalnej dla jednostek organizacyjnych Urzędu i Miasta', 'modelowanie i optymalizacja istniejących procesów informacyjnych', 'weryfikacja prowadzonej przez wykonawców dokumentacji realizowanych prac', 'monitorowanie harmonogramu prac wdrożeniowych', 'udział w testowaniu prototypu wdrażanego oprogramowania i opracowywaniu listy jego modyfikacji', 'samodzielna realizacja projektów informatycznych we współpracy z wewnętrznym działem programistów', 'nadzór nad wdrożonymi w Urzędzie i miejskich jednostkach organizacyjnych systemami informatycznymi wspomagającymi zarządzanie Miastem']\n",
      "requirements: ['wykształcenie wyższe', 'doświadczenie w zakresie wdrażania i/lub projektowania systemów informatycznych', 'znajomość metodyk projektowania systemów informatycznych', 'umiejętność prowadzenia analizy funkcjonalnej', 'znajomość języka SQL', 'umiejętność pracy w zespole, komunikatywność', 'umiejętność bardzo dobrej organizacji pracy', 'samodzielność']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-analiz-i-wdrozen-poznan-plac-kolegiacki-17,oferta,1003904931?apid=4ba2dd23-f361-4b22-8bad-752a9c0d6a46&oca=None&acv=0\n",
      "\n",
      "--- Job Record 349 ---\n",
      "id: 402\n",
      "url: https://www.pracuj.pl/praca/senior-cloud-data-engineer-warszawa,oferta,1003883871\n",
      "title: Senior Cloud Data Engineer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 8 dnido 06 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: None\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['SQL', 'Git', 'Microsoft Azure', 'scrum']\n",
      "responsibilities: ['monitorowanie oraz diagnostyka problemów w chmurze', 'monitorowanie problemów w chmurze', 'planowanie infrastruktury, obliczanie jej kosztu', 'migracje rozwiązań on-premise do chmury', 'bezpośrednia współpraca z klientem']\n",
      "requirements: ['min. 5 lat doświadczenia w IT, w tym min. 3,5 roku w pracy z danymi w chmurze (potwierdzone projektami komercyjnymi wdrożonymi na produkcje)', 'korzystasz z SQL na poziomie zaawansowanym i wykorzystujesz go na rozwiązaniach technologicznych MS i nie tylko', 'znasz metodyki i stosujesz biegle Git oraz CI/CD', 'tworzysz i optymalizujesz rozwiązania przetwarzające dane (ETL, ELT, itp.) poprzedzone projektem technicznym oraz alternatywami rozwiązań', 'koncepcje Delta Lake i Data Lakehouse są Ci znane', 'znasz architekturę SMP oraz MPP wraz z przykładami rozwiązań opartych o te architektury', 'masz wiedzę na temat migracji rozwiązań on-premise do chmury oraz znasz podstawowe typy migracji', 'masz wiedzę na temat stosowania mechanizmów związanych z bezpiecznym przechowywaniem i przetwarzaniem danych w chmurze', 'bardzo dobrze znasz usługi związanie z przechowywaniem i przetwarzaniem danych, oferowanych przez dostawcę chmury Azure', 'masz doświadczenie w bezpośredniej współpracy z klientem', 'posługujesz się j. angielskim na poziomie średniozaawansowanym (min. B2)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-cloud-data-engineer-warszawa,oferta,1003883871?apid=da53d04e-ca30-4f0e-8040-bc8c98d6b97f&oca=None&acv=0\n",
      "\n",
      "--- Job Record 350 ---\n",
      "id: 403\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-ms-sql-bydgoszcz-jagiellonska-17,oferta,1003902119\n",
      "title: Specjalista ds. MS SQL\n",
      "work_location: None\n",
      "validity: ważna jeszcze 16 dnido 14 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Jagiellońska 17, BydgoszczBydgoszcz, kujawsko-pomorskie']\n",
      "technologies: ['SQL', 'AWS', 'Azure SQL']\n",
      "responsibilities: ['Administracja bazami danych MS SQL oraz serwerami bazodanowymi Windows', 'Monitoring, analiza wydajności, aktualizacja statystyk, reorganizacja indeksów', 'Zarządzanie uprawnieniami, konfiguracja zewnętrznych źródeł danych (linked server)', 'Prace rozwojowe i utrzymanie dokumentacji w ramach zarządzanego środowiska']\n",
      "requirements: ['Dobra znajomość rozwiązań Microsoft oraz rozwiązań bazodanowych MS SQL', 'Zdolności organizacyjne i analityczne', 'Komunikatywność i umiejętność współpracy w zespole', 'Odporność na stres oraz umiejętność pracy pod presją czasu', 'Umiejętność szybkiego uczenia się i poznawania nowych technologii teleinformatycznych', 'Znajomość języka angielskiego na poziomie umożliwiającym posługiwanie się dokumentacją']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-ms-sql-bydgoszcz-jagiellonska-17,oferta,1003902119?apid=ff0efe34-1b10-4053-85e7-12b4ba52aa13&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 351 ---\n",
      "id: 404\n",
      "url: https://www.pracuj.pl/praca/azure-logic-apps-developer-warszawa,oferta,1003876807\n",
      "title: Azure Logic Apps Developer\n",
      "work_location: Company locationWarszawaWarszawa, Masovian\n",
      "validity: valid for 4 daysto 02 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['C#']\n",
      "responsibilities: ['Design and develop integration solutions using Azure Integration Services (AIS).', 'Implement standard Logic Apps for automating workflows and business processes.', 'Perform JSON to JSON transformations to ensure seamless data integration across systems.', 'Utilize the MS Project REST API for integrating with MS Project Online entities like Tasks and Custom Fields.', 'Implement OAuth 2.0 protocols for secure authorization of connections between services.', 'Develop and maintain Azure Functions to extend functionality and handle complex integration logic.']\n",
      "requirements: ['Proficiency in developing and managing Standard Logic Apps.', 'Experience with programming languages C#, Visual Basic.', 'Expertise in JSON to JSON data transformations.', 'Experience with the MS Project REST API and familiarity with MS Project Online entities.', 'Strong understanding of OAuth 2.0 authentication standards.', 'Demonstrable experience with Azure Functions.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/azure-logic-apps-developer-warszawa,oferta,1003876807?apid=befe3f36-8ed0-475c-859b-e8fc0f801fec&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 352 ---\n",
      "id: 405\n",
      "url: https://www.pracuj.pl/praca/data-engineer-energy-sector-gdansk,oferta,1003865457\n",
      "title: Data Engineer (Energy Sector)\n",
      "work_location: Company locationGdańskGdańsk, Pomeranian\n",
      "validity: None\n",
      "contract_type: B2B contract\n",
      "employment_type: None\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'valid 5 hours', 'part time, additional / temporary', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Oracle', 'PostgreSQL', 'DB2', 'AWS', 'Microsoft Azure', \"at the client's site\", 'you focus on a single project at a time', 'you have influence on the choice of tools and technologies', 'you have influence on the product', 'you develop the code \"from scratch\"', 'you focus on product development', 'scrum']\n",
      "responsibilities: ['Develop a comprehensive understanding of the existing data landscape, including data sources, sinks, and relevant systems', 'Design and implement a data model based on business requirements, system landscape, and available data, ensuring compliance with EU data protection and privacy standards (e.g., GDPR)', 'Analyze the structure of data provided by peripheral systems, ensuring compatibility with the new system and alignment with EU data quality standards', 'Work with peripheral systems to define data formats, ensuring they meet project requirements, and follow up on implementation', 'Software Selection: Evaluate and select appropriate software products for database management (Relational, NoSQL, Big Data), reporting systems, and analytics platforms', 'Collaborate with the IT Architect to design a robust IT architecture and select suitable infrastructure to support data flows, ensuring alignment with best practices and EU technical standards', 'Define and manage system interfaces, ensuring consistency with business processes and adherence to integration requirements', 'Quality Assurance', 'Contribute that appropriate data security measures are implemented, particularly in relation to automated test data processes, in line with EU cybersecurity frameworks', 'Ensure that all data processes, models, and tools comply with relevant regulations, including GDPR, and adhere to internal data protection and security policies']\n",
      "requirements: ['At least 5 years of professional experience as a Data Scientist or Data Engineer, with expertise in designing and implementing data models and analytics for complex systems', 'Strong knowledge of common database management systems (e.g., Oracle, PostgreSQL, Db2) and experience with Big Data and Data Lake technologies', 'Experience working with cloud platforms (e.g., AWS, Azure) and hybrid infrastructures', 'Proven experience in designing and implementing data structures for large-scale, business-critical systems, adhering to modern IT data design and EU standards', 'Fluency in English is required (German language skill is an asset)', 'Experience with projects in the energy sector, particularly related to TSOs, redispatching, and countertrading processes, is advantageous', 'Certifications: Relevant certifications such as Certified Data Management Professional (CDMP), experience in working in SCRUM or SAFe framework , or equivalent are desirable', 'Familiarity with EU regulations and standards, particularly GDPR, EU Cybersecurity Act', 'Strong ability to work effectively with a wide range of stakeholders, including internal teams, external service providers, and business users']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-energy-sector-gdansk,oferta,1003865457?apid=812bd36b-f820-49cd-87cd-4b0658106ee8&oca=None&acv=0\n",
      "\n",
      "--- Job Record 353 ---\n",
      "id: 406\n",
      "url: https://www.pracuj.pl/praca/senior-bi-analyst-with-etl-gdansk,oferta,1003876678\n",
      "title: Senior BI Analyst with ETL\n",
      "work_location: None\n",
      "validity: ważna jeszcze 4 dnido 02 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['GdańskGdańsk, pomorskie']\n",
      "technologies: ['DB2', 'Snowflake Data Cloud', 'Alteryx', 'PowerBI', 'data modeling', 'u klienta', 'agile', 'scrum']\n",
      "responsibilities: ['Analyze and document legacy system data, processes, and workflows.', 'Prepare and develop a BI solution by migrating data from DB2 to modern systems (Snowflake, Power BI, Alteryx).', 'Lead the development of data models and transformations, ensuring seamless integration and data flow.', 'Perform data mapping and draft clear requirements for future BI solutions.', 'Collaborate with stakeholders, providing recommendations and insights for the design of future solutions.', 'Support and work closely with other teams to ensure alignment on project deliverables and timelines.', 'Ensure that architecture principles and best practices are applied in the transformation process.', 'Maintain thorough documentation of as-is processes and the proposed architecture for future solutions.', 'Take ownership of the project, proactively addressing any challenges and seeking out solutions.']\n",
      "requirements: ['Proven experience as a BI Analyst, with hands-on experience in data modeling and transformation.', 'Strong knowledge of DB2, Snowflake, and Alteryx or Power BI.', 'Solid understanding of architecture principles and BI best practices.', 'Ability to deep dive into legacy systems, understand their data, and analyze the underlying processes.', 'Experience in documenting as-is processes, creating detailed requirements for future solutions, and performing data mapping.', 'Ability to work independently and proactively as a self-starter.', 'Strong stakeholder management and collaboration skills, with the ability to communicate effectively with both technical and non-technical teams.', 'Detail-oriented with excellent problem-solving skills.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-bi-analyst-with-etl-gdansk,oferta,1003876678?apid=49bc1e9a-61cd-474b-9a97-244167066842&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 354 ---\n",
      "id: 407\n",
      "url: https://www.pracuj.pl/praca/ai-ml-specialist-krakow-zablocie-43a,oferta,1003876649\n",
      "title: AI / ML Specialist\n",
      "work_location: None\n",
      "validity: valid for 4 daysto 02 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Zabłocie 43A, Podgórze, KrakówKraków, Lesser Poland', 'expert']\n",
      "technologies: ['Python', 'SQL', 'pandas', 'scikit-learn', 'Jupyter Notebook', 'Git', 'AWS', 'in house', \"at the client's site\", 'you focus on a single project at a time', 'you develop several projects simultaneously', 'you have influence on the product', 'you focus on product development', 'you focus on code maintenance', 'agile', 'scrum']\n",
      "responsibilities: ['Tech consulting in the sales process: meeting with clients to gather requirements for AI projects, understanding their needs and objectives, and translating them into actionable plans', 'Project Ownership: taking full responsibility for the successful execution of AI projects, from conceptualization to delivery, ensuring they align with the company’s AI strategy', 'Design, development and integration of Machine Learning solutions for mobile and web applications, including computer vision models, credit scoring, recommendation systems and predictive analytics', 'Developing state-of-the-art, LLM-Based (OpenAI, Llama, Bard) solutions for our clients, including knowledge hubs, conversational shopping, customer service, and document analysis', 'Building ETL pipelines for Machine Learning and Business Intelligence solutions', 'Leverage your experience with software engineering and cloud services to architect and deploy scalable AI/ML solutions in the cloud', 'AI Strategy Development: working closely with C-level executives to define and execute the company’s AI strategy, identifying opportunities for AI-driven innovation and growth']\n",
      "requirements: ['Proficiency in Python (including pandas, scikit-learn, and preferably other Machine Learning libraries)', 'Very good knowledge and understanding of Machine Learning algorithms and model validation techniques', 'Experience in building Machine Learning solutions, especially involving natural language processing', 'Good understanding of LLM (Large Language Models) architectures and training', 'Experience in prompting and fine-tuning LLMs, including GPT', 'Very good knowledge of SQL', 'Experience working with large datasets', 'Experience in one or more of the following areas: entity/relation extraction, information extraction, summarization, semantics, document classification, ontology, question answering, knowledge graph', 'Knowledge of the typical data science development environment (Jupyter Notebook, Git)', 'Creativity in data preparation and model building', 'Business approach and ability to present solutions to both technical and non-technical audiences (you will take part in the sales process)', 'Very good command of written and spoken English & Polish (C1 level)', 'Ability to cooperate with many different stakeholders and manage changing priorities']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ai-ml-specialist-krakow-zablocie-43a,oferta,1003876649?apid=c77a0276-0093-4fcc-92d5-679228c8cabc&oca=None&acv=0\n",
      "\n",
      "--- Job Record 355 ---\n",
      "id: 408\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-inzynierii-analitycznej-inzynier-danych-poznan-kasztelanska-37,oferta,1003883699\n",
      "title: Specjalista ds. inżynierii analitycznej | Inżynier danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 8 dnido 06 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Kasztelańska 37, Grunwald, PoznańPoznań, wielkopolskie']\n",
      "technologies: ['SQL', 'BigQuery', 'Snowflake', 'Redshift', 'GitLab', 'Python', 'wewnątrz organizacji', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'scrum', 'code review']\n",
      "responsibilities: ['Precyzyjne i wydajne modelowanie danych, wspierające potrzeby w zakresie analityki, Business Intelligence i machine learning.', 'Projektowanie, wdrażanie i utrzymanie architektury danych, w tym hurtowni danych.', 'Monitorowanie wydajności systemów danych i identyfikacja obszarów do poprawy.', 'Konsultowanie założeń biznesowych i technicznych z innymi zespołami w organizacji.', 'Automatyzacja procesów i zadań związanych z przetwarzaniem danych.', 'Poszukiwanie i wdrażanie nowych technologii i narzędzi z zakresu inżynierii danych.', 'Optymalizacja kosztowa procesów zasilających.', 'Zapewnienie jakości danych, w tym ich walidacji, czyszczenia i transformacji.', 'Dokumentowanie rozwiązań i procesów.']\n",
      "requirements: ['Minimum 2 lata doświadczenia na stanowisku, które wymagało modelowania danych.', 'Praktyczna znajomość SQL (dowolny dialekt).', 'Znajomość koncepcji hurtowni danych (Data Warehousing).', 'Doświadczenie w pracy z systemem kontroli wersji Git.', 'Umiejętność logicznego i analitycznego myślenia.', 'Samodzielność, komunikatywność i dobrej organizacji czasu pracy.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-inzynierii-analitycznej-inzynier-danych-poznan-kasztelanska-37,oferta,1003883699?apid=072ff11f-b184-4bcd-b31c-87c9080c2fe0&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 356 ---\n",
      "id: 409\n",
      "url: https://www.pracuj.pl/praca/it-analyst-warszawa,oferta,1003919179\n",
      "title: IT Analyst\n",
      "work_location: None\n",
      "validity: ważna jeszcze 24 dnido 22 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatów']\n",
      "technologies: ['SQL', 'UML', 'BPMN', 'Jira', 'Confluence', 'Postman', 'Swagger', 'u klienta', 'koncentrujesz się na jednym projekcie', 'możesz zmienić projekt', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'agile', 'scrum', 'dokumentacja']\n",
      "responsibilities: ['Wsparcie analityczne dwóch zespołów scrumowych rozwijających Kredyt Hipoteczny i Kredyt Gotówkowy.', 'Analiza wymagań biznesowych i ich przekładanie na specyfikacje techniczne i funkcjonalne.', 'Współpraca z właścicielami produktów, zespołami deweloperskimi i operacyjnymi.', 'Tworzenie dokumentacji analitycznej, w tym mapowań i specyfikacji usług.', 'Wsparcie w definiowaniu architektury systemów oraz komunikacji między nimi (kolejki, API, integracje synchroniczne/asynchroniczne).', 'Optymalizacja i rozwój istniejących rozwiązań oraz udział w inicjatywach związanych z wdrażaniem nowych funkcjonalności.', 'Wsparcie w utrzymaniu środowisk produkcyjnych i rozwiązywaniu incydentów.', 'Tworzenie scenariuszy testowych oraz weryfikacja funkcjonalności przed wdrożeniem.']\n",
      "requirements: ['Minimum 2-3 lata doświadczenia na stanowisku Analityka IT w sektorze finansowym (mile widziana znajomość procesów kredytowych).', 'Doświadczenie w pracy z systemami bankowymi, znajomość procesów kredytowych lub open API będzie atutem.', 'Bardzo dobra znajomość SQL i relacyjnych baz danych.', 'Umiejętność modelowania procesów biznesowych w UML, BPMN.', 'Znajomość zagadnień związanych z komunikacją synchroniczną/asynchroniczną, integracjami API oraz mechanizmami kolejkowymi.', 'Podstawowa znajomość programowania i baz dokumentowych/relacyjnych.', 'Praktyczna znajomość narzędzi Jira, Confluence, SQL Developer, PG Admin, Postman, Swagger.', 'Znajomość regulacji PSD2 i doświadczenie we współpracy z podmiotami zewnętrznymi będzie dodatkowym atutem.', 'Umiejętność analitycznego myślenia, samodzielność, inicjatywa oraz łatwość nawiązywania kontaktów.', 'Doświadczenie w pracy w metodologii Agile/Scrum.', 'Znajomość języka angielskiego na poziomie komunikatywnym.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/it-analyst-warszawa,oferta,1003919179?apid=dcbfc04c-35a1-46dc-8e3c-9d409ddb6025&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 357 ---\n",
      "id: 410\n",
      "url: https://www.pracuj.pl/praca/ml-engineer-medical-industry-wroclaw-na-ostatnim-groszu-3,oferta,1003901908\n",
      "title: ML Engineer – medical industry\n",
      "work_location: None\n",
      "validity: valid for 16 daysto 14 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Na Ostatnim Groszu 3, Fabryczna, WrocławWrocław, Lower Silesia']\n",
      "technologies: ['Python', 'TensorFlow', 'PyTorch']\n",
      "responsibilities: ['Analyzing large-scale medical datasets by collecting, cleaning, and uncovering patterns to drive key insights', 'Developing predictive models and ML algorithms to solve complex problems related to molecular biology and biotechnology', 'Deploying scalable AI/ML solutions within a modern cloud environment to enhance data processing and analysis', 'Collaborating with cross-functional teams on-site in Wrocław, including Data Engineers, Data Scientists, and business stakeholders', 'Communicating findings effectively to stakeholders, gathering requirements, and shaping the future direction of the project']\n",
      "requirements: ['At least 2 years of commercial experience as an ML Engineer or in a similar role', 'Practical experience with modern cloud data solutions and delivering AI/ML models at scale', 'Proficiency in Python and frameworks such as TensorFlow, PyTorch, or scikit-learn', 'Strong understanding of molecular biology and biotechnology domains', 'Fluency in English and willingness to work hybrid from Wrocław (on-site presence required)', 'Residing in Poland required']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ml-engineer-medical-industry-wroclaw-na-ostatnim-groszu-3,oferta,1003901908?apid=9c4fc2ed-b990-43e2-8999-e00dae8158c3&oca=None&acv=0\n",
      "\n",
      "--- Job Record 358 ---\n",
      "id: 411\n",
      "url: https://www.pracuj.pl/praca/marketing-analyst-emea-and-americas-wroclaw-ks-piotra-skargi-1,oferta,1003919115\n",
      "title: Marketing Analyst EMEA and Americas\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior), expert\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['ks. Piotra Skargi 1, Stare Miasto, WrocławWrocław, Lower Silesia', 'contract of employment']\n",
      "technologies: ['Power BI', 'SQL', 'DAX']\n",
      "responsibilities: ['Coordinate data collecting from Account Teams for the Americas and EMEA for quarterly topline planning process (Strategic Path Analysis, SPA).', 'Participate in SPA meetings and implement data entries as agreed with Regional Marketing Managers. Lead selected SPA meetings as agreed with Regional Marketing Managers.', 'Investigate optimization options for the process for data collecting for EMEA and the Americas (scheduling and data input templates)', 'Establish analytical procedures to ensure data completeness, consistency and quality including data accuracy checks against SAP, supply chain databases, and 3rd party market data.', 'Ensure robust documentation of topline assumptions for the regions.', 'Investigate options to simplify and automate data collecting and processing.', 'Provide in-depth analysis of market trends and topline data from various available sources as agreed with Regional Marketing Managers, Segment Marketing teams as well as other relevant stakeholders.', 'Provide data analysis to supports strategic marketing initiatives, strategic scenario planning, asset planning activities, and asset related projects.', 'Act as support and backup as for interfacing to commercial and operational teams within  the regions.', 'Supports market intelligence related tasks and projects agreed with Regional Marketing Managers.', 'Global owner of continuous improvement of the relevant modules of the business steering database (mainly market and sales modules). Responsible to coordinate the scope definition for system changes and collaboration with external providers executing the changes.']\n",
      "requirements: ['Bachelor Degree required; MBA a plus', '5+ years of experience in data analysis and reporting, preferably in an international business field.', 'Experience in automotive market segments or related industry preferred', 'Proven knowledge of SQL and DAX.', 'Experience with data visualization tools, such as PowerBI, is preferred.', 'Demonstrated experience in analyzing large datasets and reporting conclusions.', 'Demonstrated strategic thinking skills.', 'Strong knowledge of basic office applications (MS Powerpoint, Excel, Word).', 'Fluency in English required. Good command of German will be an advantage.', 'Open mindset to learn and incorporate innovation.', 'Willingness to question the status quo & challenge current methodologies to achieve best-in-class practices.', 'Effective communication skills to various levels in the organization.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/marketing-analyst-emea-and-americas-wroclaw-ks-piotra-skargi-1,oferta,1003919115?apid=3bd834cd-f9c3-4f1d-bb90-175c13674c62&oca=None&acv=0\n",
      "\n",
      "--- Job Record 359 ---\n",
      "id: 412\n",
      "url: https://www.pracuj.pl/praca/godw-migration-technical-lead-warszawa-dluga-29,oferta,1003883479\n",
      "title: GODW Migration Technical Lead\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 8 dnido 06 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: None\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['kierownik / koordynator']\n",
      "technologies: ['Snowflake', 'DBT', 'Talend', 'Informatica PowerCenter', 'AutomateNow', 'u klienta', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu']\n",
      "responsibilities: ['Nadzorowanie procesu migracji GODW (Global Operational Data Warehouse)', 'Opracowywanie i zarządzanie strategiami migracji, harmonogramami i planami', 'Współpraca interesariuszami w celu informacji o postępach i problemach', 'Sprawowanie nadzoru nad realizacją planu migracji, zapewnienie, że wszystkie elementy techniczne są dostarczane na czas i zgodnie z zakresem', 'Zapewnienie integralności danych, bezpieczeństwa i wydajności w całym procesie', 'Rozwiązywanie napotkanych problemów pojawiających się podczas migracji']\n",
      "requirements: ['Co najmniej 4 lata doświadczenia w pracy z danymi', 'Znajomość na co najmniej podstawowym poziomie takich narzędzi: Snowflake, DBT, Talend lub Informatica PowerCenter', 'Biegłość w obszarze migracji do chmury', 'Doświadczenie w metodologii DevOps, CI/CD', 'Doświadczenie w modelowaniu wymiarowym i migracji/konwersji do Data Vault', 'Znajomość narzędzi do tworzenia harmonogramów dla przedsiębiorstw np. AutomateNow', 'Język angielski umożliwiający swobodną komunikację (min B2/C1)', 'Gotowość do podróży służbowej do Bazylei (Szwajcaria) - zespół odbywa średnio 2 warsztaty na kwartał']\n",
      "application_link: https://www.pracuj.pl/aplikuj/godw-migration-technical-lead-warszawa-dluga-29,oferta,1003883479?apid=056881d2-efed-4fd7-bda1-42dbc926a5fa&oca=None&acv=0\n",
      "\n",
      "--- Job Record 360 ---\n",
      "id: 413\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-warszawa,oferta,1003919046\n",
      "title: Senior Data Engineer\n",
      "work_location: Company locationWarszawaWarszawa, Masovian\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['SQL', 'Python', 'Microsoft SQL Server', 'PostgreSQL', 'MongoDB', 'DynamoDB', 'Spark', 'Databricks', 'Hadoop', 'Azure Data Factory', 'Kafka', 'Airflow', 'Microsoft Azure', 'AWS', 'Git', \"at the client's site\", 'you focus on a single project at a time', 'you have influence on the technological solutions applied', 'you have influence on the product', 'scrum']\n",
      "responsibilities: ['Design and enhance data platforms, driving both functional and architectural improvements', 'Develop and optimize ETL/ELT processes for large-scale data processing', 'Standardize and streamline technical processes, ensuring best practices in code, testing, and documentation', 'Conduct code reviews and maintain high-quality standards', 'Collaborate with Data Engineers & Analysts to enhance technology solutions', 'Mentor the team on solution design, code quality, and process optimization']\n",
      "requirements: ['5+ years in BI, ETL/ELT, Data Warehouse, and Big Data solutions', 'Strong SQL & Python (5+ years)', 'Experience with relational (SQL Server, PostgreSQL, etc.) & NoSQL (MongoDB, DynamoDB, etc.) databases', 'Hands-on with Spark/Databricks, Hadoop, Azure Data Factory, Kafka, Airflow, dbt, etc', 'Knowledge of Data Governance, Streaming, and Data Architectures (Data Mesh, Data Vault, Lambda/Kappa)', 'Familiarity with Azure & AWS data services', 'Proficiency with Git (Bitbucket, GitHub, GitLab)', 'English: B2 minimum (C1+ preferred)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-warszawa,oferta,1003919046?apid=f64e97ab-f859-43ce-80b7-6f21da9e82a3&oca=None&acv=0\n",
      "\n",
      "--- Job Record 361 ---\n",
      "id: 414\n",
      "url: https://www.pracuj.pl/praca/data-analyst-with-etl-warszawa,oferta,1003919037\n",
      "title: Data Analyst with ETL\n",
      "work_location: None\n",
      "validity: ważna jeszcze 24 dnido 22 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatów']\n",
      "technologies: ['SQL', 'Python', 'ETL', 'u klienta', 'koncentrujesz się na jednym projekcie', 'możesz zmienić projekt', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'koncentrujesz się na rozwoju produktu', 'dokumentacja']\n",
      "responsibilities: ['Analiza i modelowanie danych w SQL i Python', 'Dokumentowanie zmian zgodnie ze standardami projektowymi', 'Współpraca w kluczowych projektach migracyjnych', 'Tłumaczenie wymagań biznesowych na koncepcje techniczne', 'Testowanie rozwiązań i kontrola jakości danych', 'Optymalizacja procesów związanych z przepływem danych', 'Aktywne wsparcie zespołu w projektach związanych z migracją danych']\n",
      "requirements: ['4 lata doświadczenia jako Data Analyst z praktyką w ETL', 'Doświadczenie w pracy z hurtowniami danych (DWH) i Data Lake', 'Bardzo dobra znajomość SQL (tworzenie złożonych zapytań, funkcje analityczne)', 'Podstawowa znajomość Pythona (obróbka i łączenie danych, pętle – klasy mile widziane)', 'Doświadczenie w pracy z dużymi zbiorami danych i modelowaniem danych', 'Umiejętność pracy na styku biznesu i technologii – zdolność przekładania wymagań biznesowych na rozwiązania techniczne', 'Dobra organizacja pracy – umiejętność priorytetyzowania zadań', 'Umiejętność współpracy zespołowej i dzielenia się wiedzą', 'Język angielski na poziomie B2 (możliwe spotkania z zespołem z Hiszpanii)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-analyst-with-etl-warszawa,oferta,1003919037?apid=79d5a211-a36d-43d4-be44-c194c79b530a&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 362 ---\n",
      "id: 415\n",
      "url: https://www.pracuj.pl/praca/big-data-engineer-warszawa-lucka-9,oferta,1003913572\n",
      "title: Big Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 22 daysto 20 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Łucka 9, Wola, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['Spark', 'Scala', 'Hive', 'SQL', 'Python', 'Java', 'DevOps', 'Big Data', 'CI/CD tools', 'microservices', 'API', 'Airflow', 'Git', 'in house']\n",
      "responsibilities: ['Designing and developing solutions for collecting, transforming, and processing large datasets.', 'Analyzing data from multiple sources and translating findings into technical specifications.', 'Industrializing data processing for reliability, robustness, efficiency, and resilience.', 'Developing and implementing data collection, processing jobs, and data mapping using Spark/Scala.', 'Conducting code reviews and collaborating with team members for continuous improvement.', 'Contributing to agile feature teams and helping advance agile best practices.', 'Delivering IT solutions in Scrum cycles (2-3 weeks) with tangible business value.', 'Recommending and contributing to big data architecture strategies.', 'Staying up to date with emerging data technologies and implementing best practices.', 'Supporting applications and troubleshooting production issues as needed.', 'Ensuring quality and maintainability of code through best development practices (BDD, TDD, clean code, DevOps).', 'Producing technical documentation, test plans, and project deliverables.', 'Identifying potential project risks related to costs and deadlines, proposing mitigation strategies when necessary.']\n",
      "requirements: ['3-5 years of professional experience in big data engineering, preferably with a background in banking.', 'Fluency in English (Polish and French are considered an advantage).', 'Expertise in Spark, Scala, Hive, and SQL.', 'Additional knowledge of Python, Java, DevOps, Big Data, CI/CD tools, microservices, APIs, Airflow, and GIT.', 'Strong analytical and problem-solving skills.', 'A proactive attitude and strong teamwork ability.', 'Effective communication skills.', 'The ability to work in a global team across different time zones.', 'Strong planning and prioritization skills']\n",
      "application_link: https://www.pracuj.pl/aplikuj/big-data-engineer-warszawa-lucka-9,oferta,1003913572?apid=d67d2d20-4562-440c-9a10-008a9ccfc8c2&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 363 ---\n",
      "id: 416\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-warszawa,oferta,1003876058\n",
      "title: Senior Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 4 daysto 02 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: full office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'More than one vacancy']\n",
      "technologies: ['SQL', 'PySpark', 'Python', 'Apache Airflow', 'AWS', 'Kafka', 'Redshift', 'Lambda', 'Terraform', 'Ansible', 'Microsoft Azure', \"at the client's site\", 'you develop several projects simultaneously', 'you can change the project', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'you focus on code maintenance', 'agile']\n",
      "responsibilities: ['Lead the design, development, and maintenance of data pipelines and ETL/ELT processes to handle large-scale, diverse datasets.', 'Optimize data ingestion, transformation, and delivery using SQL, PySpark, and Python.', 'Leverage frameworks like Apache Airflow, AWS Glue, Kafka, and Redshift to ensure efficient data orchestration, batch/stream processing, and high-performance analytics.', 'Drive best practices in version control (Git), infrastructure as code (Terraform, Ansible), and CI/CD pipelines to ensure robust, repeatable, and scalable deployments.', 'Collaborate closely with cross-functional teams—Data Scientists, Analysts, Product Managers—to design data models and architectures that meet business objectives.', 'Monitor, debug, and optimize ETL pipelines, ensuring high reliability, low latency, and cost efficiency.', 'Mentor mid-level and junior engineers, fostering a culture of knowledge sharing, continuous improvement, and innovation.']\n",
      "requirements: ['Strong proficiency in SQL, PySpark, and Python for data transformations and scalable pipeline development (6+ years of commercial experience).', 'Hands-on experience with Apache Airflow, AWS Glue, Kafka, and Redshift. Familiarity with handling large volumes of structured and semi-structured data. Experience with DBT is a bonus.', 'Proficiency with Git for version control. Airflow is crucial for orchestration.', 'Solid experience working with AWS (Lambda, S3, CloudWatch, SNS/SQS, Kinesis) and exposure to serverless architectures.', 'Experience with Terraform and Ansible to automate and manage infrastructure.', 'Strong skills in monitoring ETL pipelines, troubleshooting performance bottlenecks, and maintaining high operational reliability.', 'Familiarity with CI/CD processes to automate testing, deployment, and versioning of data pipelines.', 'Ability to design distributed systems that scale horizontally for large data volumes. Knowledge of real-time (Lambda) and batch (Kappa) processing architectures is a plus.', 'Experience building APIs (REST, GraphQL, OpenAPI, FastAPI) for data exchange.', 'Exposure to Data Mesh principles and self-service data tools is highly desirable. Previous experience in building scalable data platforms and transforming large datasets is a strong plus.', 'Higher education with a profile in Computer Science or related', 'English level min. B2.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-warszawa,oferta,1003876058?apid=58f8376a-910d-4c45-9826-eda628672d1a&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 364 ---\n",
      "id: 417\n",
      "url: https://www.pracuj.pl/praca/medical-data-processing-analyst-wroclaw-legnicka-48g,oferta,1003879892\n",
      "title: Medical Data Processing Analyst\n",
      "work_location: None\n",
      "validity: ważna jeszcze 7 dnido 05 marca 2025\n",
      "contract_type: umowa o pracę, umowa zlecenie, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca zdalna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Legnicka 48g, Fabryczna, WrocławWrocław, dolnośląskie', 'Szukamy wielu kandydatów']\n",
      "technologies: ['3D Slicer', 'Cannon Vitrea', 'Medis QAngio XA', 'RadiAnt', 'Medis QAngio CT', 'Siemens syngo.via', 'wewnątrz organizacji', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt']\n",
      "responsibilities: ['Przetwarzanie, ewaluacja i korygowanie danych medycznych', 'Ścisła współpraca z zespołami Product Design, AI oraz CFD oraz Web Development', 'Praca z oprogramowaniem do analizy danych medycznych 3D', 'Opracowywanie procesów i narzędzi optymalizacji pracy Zespołu', 'Samodzielne procesowanie badań w obrębie systemów produkowanych przez Organizację oraz za pomocą narzędzi zewnętrznych', 'Zbieranie informacji zwrotnej od klientów i przekazywanie Organizacji', 'Wykonywanie zadań wynikających z QMS (System Zarządzania Jakością)']\n",
      "requirements: ['Chęć do zdobywania wiedzy - nie musisz mieć wyjątkowego doświadczenia, nauczymy Cię analizować dane medyczne', 'Zamiłowanie do analizy danych oraz umiejętność pracy z danymi', 'Wykształcenie wyższe inżyniera biomedyczna lub pokrewne (medycyna, fizjoterapia, etc.)', 'Wiedza medyczna z obszaru elektroradiologii / radiologii / kardiologii', 'Znajomość zagadnień związanych z obrazowaniem naczyń wieńcowych np. koronarografia, FFR, IVUS, OCT, tomografia komputerowa serca', 'Systematyczność i konsekwencja w dążeniu do wyznaczonych celów', 'Skrupulatność i zaangażowanie w wykonywane obowiązki', 'Znajomość języka angielskiego na poziomie umożliwiającym czytanie ze zrozumieniem opracowań branżowych (publikacje, instrukcje) i uczestniczenie w webinarach prowadzonych w tym języku']\n",
      "application_link: https://www.pracuj.pl/aplikuj/medical-data-processing-analyst-wroclaw-legnicka-48g,oferta,1003879892?apid=3a21768e-9905-4ca9-9bec-d3b217888657&oca=None&acv=0\n",
      "\n",
      "--- Job Record 365 ---\n",
      "id: 418\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-warszawa-pulawska-2,oferta,1003875419\n",
      "title: Senior Data Engineer\n",
      "work_location: None\n",
      "validity: valid for 4 daysto 02 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Puławska 2, Mokotów, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['Python', 'PySpark', 'Spark', 'Hadoop', 'Hive', 'SQL', 'ETL', 'Jira', 'Git', 'AWS', 'Google Cloud Platform', 'Microsoft Azure', 'Microsoft Power BI', 'Tableau', 'Qlickview', 'scrum']\n",
      "responsibilities: ['Creating a Data Lake for business, with Machine Learning, Gen.AI and Data Engineering elements.', 'Assist in scoping and designing analytic data assets, implementing modelled attributes and contributing to brainstorming sessions.', 'Build and maintain a robust data engineering process to develop and implement self-serve data and tools']\n",
      "requirements: ['Minimum of 6+ years’ experience in building data engineering pipelines using Python, PySpark, Hadoop, Spark and SQL.', 'Knowledge of OOP concepts like inheritance, polymorphism and implementing Design Patterns in programming.', 'Experience with any ETL tool like Informatica, SSIS, Pentaho or Azure Data Factory.', 'Strong understanding of cloud architecture and service offerings including compute, storage, databases, networking, AI, and ML.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-warszawa-pulawska-2,oferta,1003875419?apid=ecd246b4-f02f-486e-8a4f-c3b5c153499a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 366 ---\n",
      "id: 419\n",
      "url: https://www.pracuj.pl/praca/data-transformation-consultant-she-he-they-ich-europe-warszawa,oferta,1003869893\n",
      "title: Data Transformation Consultant (She/He/They) - ICH Europe\n",
      "work_location: None\n",
      "validity: valid for a dayto 27 February 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'contract of employment', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['SQL', 'Python', 'Jira', 'Confluence', 'Git', 'Docker', 'Kubernetes', 'AWS']\n",
      "responsibilities: ['Analyse client requirements for their data journeys', 'Apply cloud and data engineering skills to solve problems and design approaches', 'Propose, build and implement big data engineering solutions for clients']\n",
      "requirements: ['Minimum Bachelor’s degree (preferred in Computer Science or Electronic Engineering)', '1-5 years of relevant professional experience', 'Experience in parallel data processing using Apache Spark or Databricks or Palantir Foundry', 'Ability to articulate complex problems and solutions in a simple and logical manner', 'Strong technical skills: SQL, python, pySpark', 'Familiarity with Jira, Confluence and git', 'Familiarity with Docker, Kubernetes and public cloud vendors (AWS or Azure or GCP)', 'Excellent written and verbal communication skills in English & Polish.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-transformation-consultant-she-he-they-ich-europe-warszawa,oferta,1003869893?apid=7c974eba-ad96-4a97-ba4d-eddb2963f599&oca=None&acv=0\n",
      "\n",
      "--- Job Record 367 ---\n",
      "id: 420\n",
      "url: https://www.pracuj.pl/praca/quality-assurance-engineer-ai-product-krakow-stanislawa-zolkiewskiego-17b,oferta,1003882566\n",
      "title: Quality Assurance Engineer (AI Product)\n",
      "work_location: None\n",
      "validity: valid for 8 daysto 06 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Stanisława Żółkiewskiego 17b, Grzegórzki, KrakówKraków, Lesser Poland']\n",
      "technologies: ['C#', 'Python', 'Selenium', 'Azure DevOps', 'in house', 'you focus on a single project at a time', 'you can change the project', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'you have influence on the product', 'you focus on product development', 'you focus on code maintenance', 'agile']\n",
      "responsibilities: ['Work with the development team to participate in functional testing as part of our Agile process', 'Have a real impact on the product by taking an active part in functionality design with Product Managers and the development team', 'Identify and share best practices in tests code quality, testing, testability and maintainability', 'Create test plans to cover new functionality', 'Develop and extend our existing test automation framework', 'Maintain tests to ensure optimal coverage and effective deduplication', 'Be responsible for not only implementing manual and automated tests, but also innovate in this field – proposing new approaches to keep the product quality high']\n",
      "requirements: ['Experience in testing cloud applications, debugging tools and analysis of stack traces, code, log-files and other data', 'Knowledge of testing in a Machine Learning field – LLMs and classic ML.', 'Proficiency in English, both written and spoken']\n",
      "application_link: https://www.pracuj.pl/aplikuj/quality-assurance-engineer-ai-product-krakow-stanislawa-zolkiewskiego-17b,oferta,1003882566?apid=f7841c55-9df6-4c06-b71c-bd1a0673ef4b&oca=None&acv=0\n",
      "\n",
      "--- Job Record 368 ---\n",
      "id: 421\n",
      "url: https://www.pracuj.pl/praca/junior-ml-engineer-paid-internship-wroclaw-legnicka-55f,oferta,1003875279\n",
      "title: Junior ML Engineer (paid internship)\n",
      "work_location: None\n",
      "validity: valid for 4 daysto 02 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: None\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Legnicka 55F, Fabryczna, WrocławWrocław, Lower Silesia', 'contract of mandate, internship / apprenticeship contract', 'trainee', 'full office work']\n",
      "technologies: ['Python', 'TensorFlow', 'PyTorch', 'scikit-learn', 'Pandas', 'NumPy', 'SQL', 'in house']\n",
      "responsibilities: ['Implementing and testing machine learning models,', 'Data analysis and preprocessing,', 'Collaborating with the Data Science and Software Engineering teams,', 'Optimizing and deploying ML models,', 'Documenting results and preparing reports.']\n",
      "requirements: ['Student status (preferred fields: Computer Science, Mathematics, Data Science, or related),', 'Proficiency in Python and ML libraries (e.g., TensorFlow, PyTorch, Scikit-learn),', 'Basic knowledge of machine learning concepts and data analysis,', 'Strong analytical and problem-solving skills,', 'Good communication and teamwork abilities.', 'Fluent communication in English.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/junior-ml-engineer-paid-internship-wroclaw-legnicka-55f,oferta,1003875279?apid=7c45a167-b29b-426e-9175-f0328c8d4e17&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 369 ---\n",
      "id: 422\n",
      "url: https://www.pracuj.pl/praca/structured-data-security-sme-specialist-cloud-krakow-kapelanka-42a,oferta,1003869816\n",
      "title: Structured Data Security SME Specialist - Cloud\n",
      "work_location: None\n",
      "validity: valid for 2 daysto 28 February 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['Microsoft Azure', 'AWS']\n",
      "responsibilities: ['Supporting the delivery and operating Strategy.', 'Providing key representation for and source of expertise on all issues.', 'Support the delivery of tooling to implement controls ensuring compliance with HSBC Information Security policies and standards globally including any regulatory requirements.', 'Collaborate to drive the implementation of the enterprise wide and regional / business level IT Strategy.', 'Ensure information security requirements are adhered to globally by ensuring effective compliance and measures are in place.', 'Work closely with the team as the 1LOD function and understand strategy while maintaining visibility of their IT security risk profile, exposures and control effectiveness and to provide robust challenge to the same audience when information security risk appetites are breached.', 'Drive engagement with all relevant regional and global stakeholders (cyber security colleagues across Strategy and Architecture, Security Shared Services, Security Engineering and business and IT Functions).']\n",
      "requirements: ['Experience on Cloud Native solutions and Cloud based data structures on AWS, GCP, Azure and AliCloud.', 'Experience with Databricks Platform and Machine Learning modelling.', 'Experience with structured data, and managing DLP technology.', 'Experience of building, deploying and using the tooling to support Data Security and Data Integration strategies such as IBM Guardium and/or Data Security Fabric.', 'Proven experience of successful operational management, utilising relevant tools and techniques to ensure consistent delivery.', 'A minimum of 4/5 years Cyber experience would be beneficial.', 'Experience working in a highly regulated environment.', \"A track record of making strategic business decisions, considering relevant risks, long term implications, commercial realities and stakeholders'' needs.\"]\n",
      "application_link: https://www.pracuj.pl/aplikuj/structured-data-security-sme-specialist-cloud-krakow-kapelanka-42a,oferta,1003869816?apid=c75cc41c-ce2f-4ee3-8034-6204afecd2d1&oca=None&acv=0\n",
      "\n",
      "--- Job Record 370 ---\n",
      "id: 423\n",
      "url: https://www.pracuj.pl/praca/programista-sql-specjalista-ds-baz-danych-kuriany-pow-bialostocki-zabludowska-104,oferta,1003869797\n",
      "title: Programista SQL / Specjalista ds. Baz Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 2 dnido 28 lutego 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Zabłudowska 104, Dojlidy Górne, Kuriany (pow. białostocki)Kuriany (pow. białostocki), podlaskie', 'praca stacjonarna']\n",
      "technologies: ['SQL', 'Comarch ERP XL', 'wewnątrz organizacji']\n",
      "responsibilities: ['Tworzenie i optymalizacja zapytań SQL oraz procedur składowanych w bazie MS SQL', 'Tworzenie rekomendacji i wdrażanie najlepszych praktyk w zakresie przetwarzania danych', 'Rozwiązywanie problemów związanych z działaniem baz danych oraz eliminacja ich przyczyn', 'Monitorowanie wydajności systemów baz danych oraz wdrażanie usprawnień', 'Zapewnienie poprawności i spójności generowanych wyników przetwarzanych danych', 'Współtworzenie planów rozwoju oprogramowania w kierunku innowacyjnego przetwarzania danych', 'Stabilizacja i wsparcie w zakresie wdrożonych rozwiązań bazodanowych', 'Praca z systemem Comarch ERP XL, w tym współpraca z zespołem wdrożeniowym oraz rozwiązywanie zagadnień związanych z integracją i działaniem systemu']\n",
      "requirements: ['Praktyczne doświadczenie w pracy z bazami danych MS SQL (szeroka znajomość SQL)', 'Znajomość systemu Comarch ERP XL i doświadczenie w jego obsłudze lub modyfikacjach', 'Umiejętność pracy z dużymi zbiorami danych oraz ich optymalizacją', 'Wiedza z zakresu monitorowania i oceny wydajności systemów baz danych', 'Umiejętność rozwiązywania problemów technicznych związanych z relacyjnymi bazami danych', 'Dążenie do poszukiwania usprawnień oraz wdrażania najlepszych praktyk przetwarzania danych', 'Wysoki poziom samodzielności i odpowiedzialności za realizowane zadania']\n",
      "application_link: https://www.pracuj.pl/aplikuj/programista-sql-specjalista-ds-baz-danych-kuriany-pow-bialostocki-zabludowska-104,oferta,1003869797?apid=2b3f43a9-a265-420f-8ba0-1c792d24291b&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 371 ---\n",
      "id: 424\n",
      "url: https://www.pracuj.pl/praca/sql-developer-krakow-cystersow-13,oferta,1003864009\n",
      "title: SQL Developer\n",
      "work_location: None\n",
      "validity: None\n",
      "contract_type: contract of employment, contract of mandate, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Cystersów 13, Grzegórzki, KrakówKraków, Lesser Poland', 'valid 5 hours']\n",
      "technologies: ['SQL']\n",
      "responsibilities: ['Designing and creating databases for computer systems', \"Designing, building and optimizing of ETL processes and preparing a necessary technical specification of the project solution in terms of project's system requirements\", 'Gathering requirements from customers', 'Designing solutions for client needs', 'Testing of deployed solutions']\n",
      "requirements: ['SQL programming knowledge (advanced)', 'Proactivity in designing database solutions', 'Experience in designing ETL processes', 'Experience in software implementation and software implementation', 'Knowledge of BI tools', 'Excellent command of English (C1)', 'Willingness to work in a flexible hybrid model from an office located in Kraków. For candidates based in Kraków: 2-3 days per week in the office. For candidates outside Kraków: 2-3 days per month in the office + 1-2 weeks of onboarding onsite']\n",
      "application_link: https://www.pracuj.pl/aplikuj/sql-developer-krakow-cystersow-13,oferta,1003864009?apid=3cf5ec77-f745-49aa-9026-7c6f7e914058&oca=None&acv=0\n",
      "\n",
      "--- Job Record 372 ---\n",
      "id: 425\n",
      "url: https://www.pracuj.pl/praca/oracle-db-administrator-devops-warszawa-aleja-jana-pawla-ii-22,oferta,1003918113\n",
      "title: Oracle DB Administrator & DevOps\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 24 dnido 22 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Oracle', 'Linux', 'Ansible', 'Terraform', 'Docker', 'Kubernetes', 'u klienta', 'koncentrujesz się na jednym projekcie', 'możesz zmienić projekt', 'koncentrujesz się na rozwoju produktu', 'agile']\n",
      "responsibilities: ['Administracja bazami danych Oracle:', 'Instalacja i konfiguracja Oracle Enterprise Databases (szyfrowane listenery, Oracle Wallet, external tables)', 'Monitorowanie i tunning wydajności baz danych, w tym optymalizacja zapytań SQL we współpracy z zespołami developerskimi', 'Tworzenie kopii zapasowych z wykorzystaniem RMAN i Veeam Backup', 'Wdrażanie zabezpieczeń baz danych i implementacja szczegółowych uprawnień zgodnie z wymaganiami', 'Diagnostyka i troubleshooting', 'Obsługa narzędzi do monitorowania i zarządzania bazami danych', 'Wdrażanie aktualizacji bezpieczeństwa bazy danych Oracle (Patch Management)', 'Obsługa na poziomie administracyjnym platformy Oracle Apex', 'Wsparcie w analizie problemów z wewnętrznymi oraz zewnętrzymi systemami Oracle DB', 'Planowanie i zarządzanie migracją danych', 'Zapewnienie ciągłości działania i rozwoju systemów informatycznych', 'Monitorowanie systemów IT, analiza logów systemowych', 'Zarządzanie infrastrukturą IT: Konfiguracja, utrzymanie i optymalizacja', 'Automatyzacja zadań: Tworzenie i utrzymanie skryptów do automatyzacji codziennych operacji i zadań administracyjnych (np. użycie Bash, Python, Ansible)', 'Aktualizacje i patchowanie: Regularne aktualizowanie systemów operacyjnych, aplikacji oraz baz danych w celu zabezpieczenia przed zagrożeniami (Patch Management)', 'Zarządzanie incydentami: Reagowanie na incydenty i awarie systemowe, diagnoza problemów oraz szybkie przywracanie działania usług, prowadzenie analiz przyczyn źródłowych (Root Cause Analysis) w celu zapobiegania przyszłym incydentom', 'Optymalizacja wydajności: Analiza i tuning wydajności systemów i baz danych w celu minimalizacji opóźnień i poprawy czasu reakcji', 'Dokumentacja i szkolenia: Tworzenie i aktualizowanie dokumentacji systemów', 'Projekty rozwojowe: Identyfikowanie obszarów wymagających modernizacji lub wprowadzenia innowacyjnych rozwiązań technologicznych; współpraca z zespołami deweloperskimi i innymi działami w celu wdrażania nowych funkcjonalności i technologii']\n",
      "requirements: ['Min. 3 letnie doświadczenie na podobnym stanowisku', 'Solidne doświadczenie w administracji bazami danych Oracle', 'Doświadczenie w administracji systemami Linux', 'Udokumentowane sukcesy w obszarze zabezpieczeń systemów i baz danych', 'Doświadczenie w implementacji zabezpieczeń w bazach danych i systemach operacyjnych', 'Znajomość standardów bezpieczeństwa i najlepszych praktyk (np. CIS Benchmark, OWASP)', 'Prowadzenie projektów wdrożeniowych dotyczących bezpieczeństwa i infrastruktury IT', 'Silne zdolności analityczne i komunikacyjne (raportowanie, współpraca międzydziałowa)', 'Język angielski na poziomie min. B2']\n",
      "application_link: https://www.pracuj.pl/aplikuj/oracle-db-administrator-devops-warszawa-aleja-jana-pawla-ii-22,oferta,1003918113?apid=965d71cd-9f88-4d30-bca6-18d01bd01f79&oca=None&acv=0\n",
      "\n",
      "--- Job Record 373 ---\n",
      "id: 426\n",
      "url: https://www.pracuj.pl/praca/inzynier-danych-sady-pow-poznanski,oferta,1003923967\n",
      "title: Inżynier Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze miesiącdo 24 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Sady (pow. poznański)Sady (pow. poznański), wielkopolskie']\n",
      "technologies: ['SQL', 'Python', 'Google Cloud Platform']\n",
      "responsibilities: ['projektowanie, budowanie i utrzymywanie hurtowni danych i systemów przechowywania danych umożliwiających zapewnienie spójnego i zintegrowanego środowiska danych wspomagających raportowanie i analizy', 'projektowanie, tworzenie i optymalizacja potoków danych (w tym procesów ETL) w celu zapewnienia kompletności i aktualności danych', 'zapewnienie integralności, jakości i bezpieczeństwa danych, na których firma może polegać i dla których można budować modele ML i AI do dalszej analizy', 'monitorowanie i rozwiązywanie problemów związanych z infrastrukturą danych w celu zapewnienia wysokiej wydajności i niezawodności systemu', 'usprawnienie procesów w obszarze zarządzania danymi', 'poszukiwanie i wdrażanie nowych technologii i rozwiązań', 'współpraca z zespołami IT', 'przygotowywanie dokumentacji technicznej związanej z systemami gromadzenia i przetwarzania danych']\n",
      "requirements: ['wykształcenie wyższe, preferowane kierunki ścisłe (np. informatyka, matematyka, statystyka, inżynieria baz danych i pokrewne)', 'doświadczenie zawodowe w obszarze Data Engineeringu lub w pokrewnych obszarach odpowiadające poziomowi stanowiska', 'doświadczenie w budowaniu i zarządzaniu hurtowniami danych i potokami danych (mile widziane: Google Cloud Platform, SAP Datasphere)', 'biegła znajomość języka SQL (jako umiejętność pisania zapytań i procedur w SQL)', 'znajomość języków programowania (mile widziany Python)', 'znajomość języka angielskiego na poziomie minimum B2', 'znajomość niemieckiego będzie dodatkowym atutem', 'umiejętność rozwiązywania problemów i krytycznego myślenia', 'umiejętność pracy w zespole', 'umiejętność budowania i podtrzymywania pozytywnych relacji z innymi pracownikami firmy', 'dokładność i systematyczność w codziennej realizacji zadań']\n",
      "application_link: https://www.pracuj.pl/aplikuj/inzynier-danych-sady-pow-poznanski,oferta,1003923967?apid=a957a6fe-f1b8-4800-baa3-a207d963afc6&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 374 ---\n",
      "id: 427\n",
      "url: https://www.pracuj.pl/praca/programista-sql-krakow-jozefa-marcika-14c,oferta,1003879083\n",
      "title: Programista SQL\n",
      "work_location: None\n",
      "validity: ważna jeszcze 6 dnido 04 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Józefa Marcika 14c, Łagiewniki-Borek Fałęcki, KrakówKraków, małopolskie']\n",
      "technologies: ['T-SQL', 'WebAPI', 'C#', '.NET', 'Microsoft SQL Server']\n",
      "responsibilities: ['rozwój aplikacji i tworzenie mechanizmów (WebServices, SOAP, REST) do integracji z systemami m.in. ERP, platformami e-commerce, platformami kurierskim', 'tworzenie i rozbudowa oprogramowania wspomagającego pracę magazynów', 'projektowanie i prowadzenie rozwoju baz danych oprogramowania do wspomagania produkcji, transportu oraz logistyki magazynowania', 'przygotowywanie dokumentacji technicznej dla realizowanej funkcjonalności']\n",
      "requirements: ['minimum dwuletnie doświadczenie na podobnym stanowisku', 'dobra znajomość języka T-SQL oraz zasad funkcjonowania relacyjnych baz danych (MSSQL)', 'znajomość zasad korzystania z SQL Profiler', 'doświadczenie w tworzeniu interface komunikacyjnych, np. WebAPI', 'znajomość C # (.Net)', 'zdolności interpersonalne, samodzielność oraz kreatywność w zakresie powierzonych zadań']\n",
      "application_link: https://www.pracuj.pl/aplikuj/programista-sql-krakow-jozefa-marcika-14c,oferta,1003879083?apid=b8bc591f-cfbe-4c79-a29a-bc559d753944&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 375 ---\n",
      "id: 428\n",
      "url: https://www.pracuj.pl/praca/assistant-vice-president-data-analytics-krakow-kapelanka-42a,oferta,1003894169\n",
      "title: Assistant Vice President - Data Analytics\n",
      "work_location: None\n",
      "validity: valid for 11 daysto 09 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Jira', 'Confluence', 'SAS', 'Python', 'PySpark', 'SQL']\n",
      "responsibilities: ['The role in the Data Management and Analytics team supports GRADE (Global Risk Analytics Data Environment) platform analytical projects. Our data is pivotal for WCR (Wholesale Credit Risk) model development, validation, and monitoring at HSBC. We handle risk data through activities such as ingesting data from internal and external sources, measuring and monitoring data quality, and providing visualizations.', 'Data Analysis - collate, test and check independently sourced data and assess its robustness and fitness for purpose.', 'Learn and understand source systems data, models and data standards.', 'Understanding data requirements from model development & monitoring teams, working on DQ issues, liaising with project teams for new ingests or data assets, working with teams to ensure data governance and compliance.', 'Develop ETL pipelines to make data assets available on GRADE.', 'Contribute to designing roadmaps for data, application and business process architecture to support future state environments.', 'Prepare effective material for dissemination to key business stakeholders at all levels of seniority.', 'Effectively manage relationships across key functional teams.', 'Work with management to prioritize business and information needs.']\n",
      "requirements: ['Solid and relevant experience in a similar job role.', 'Bachelor’s degree in IT or a numerate subject; Master’s degree preferred.', 'Strong theoretical and practical knowledge of programming tools, including proficiency in at least one of the following tools: SAS, Python, PySpark, SQL, and Hadoop Big Data, experience with Prophecy is a plus.', 'Business Analysis – requirements gathering and specification.', 'Ability to comprehend complex data architecture.', 'Strong organizational, analytical, problem-solving, and project management skills.', 'Multitasking ability and the capacity to work under pressure within tight timelines are essential.', 'Working knowledge of JIRA, Confluence, and Monday.com.', 'Open personality with effective communication skills and the flexibility to work in an international team.', 'Previous experience in IT/Analytics is desirable.', 'Preferable knowledge of banking risk data, systems, and risk models.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/assistant-vice-president-data-analytics-krakow-kapelanka-42a,oferta,1003894169?apid=3ff240b2-e7f1-4b23-9392-ed4eea035816&oca=None&acv=0\n",
      "\n",
      "--- Job Record 376 ---\n",
      "id: 429\n",
      "url: https://www.pracuj.pl/praca/data-science-manager-warszawa-aleje-jerozolimskie-134,oferta,1003909073\n",
      "title: Data Science Manager\n",
      "work_location: None\n",
      "validity: ważna jeszcze 19 dnido 17 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: None\n",
      "work_arrangement: None\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Aleje Jerozolimskie 134, Ochota, WarszawaWarszawa, mazowieckie', 'kierownik / koordynator, menedżer', 'praca stacjonarna']\n",
      "technologies: ['SQL', 'Power BI', 'Excel', 'Python/R', 'VBA', 'Crone', 'SSAS', 'SSRS', 'Bash', 'Azure Data Pipeline', 'SSIS']\n",
      "responsibilities: ['Zarządzanie zespołem analityków danych', 'Utrzymanie bazy danych oraz narzędzi do raportowania', 'Digitalizacja i automatyzacja raportowania i analiz', 'Projektowanie i zarządzanie strukturą danych w Power BI', 'Odpowiedzialność i zarządzanie narzędziami w systemie Business Intelligence', 'Zapewnianie optymalnego wykorzystania dostępnych danych (Data enablement)', 'Dobra znajomość środowiska Snowflake']\n",
      "requirements: ['Minimum 3 lata doświadczenia w obszarze Analiz (BI)/Analityki Danych', 'Zdolności analityczne w obszarze analizy biznesowej (m. in. potrafisz weryfikować wymagania oraz tworzyć dokumentację techniczną) i systemowej (umiesz współpracować z Developerami, rozumiesz ich tok komunikacji)', 'Doświadczenie we wdrażaniu i rozwoju narzędzi Business Intelligence/Power BI E2E', 'Znajomość narzędzi takich jak Jira, SQL, Power BI, Excel, Snowflake, Python, R', 'Znajomość angielskiego w stopniu komunikatywnym']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-science-manager-warszawa-aleje-jerozolimskie-134,oferta,1003909073?apid=2e4465f3-9cf3-4f87-aeb3-e40e95741818&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 377 ---\n",
      "id: 430\n",
      "url: https://www.pracuj.pl/praca/specjalista-it-ds-bi-jastrzebie-zdroj,oferta,1003893740\n",
      "title: Specjalista IT ds. BI\n",
      "work_location: None\n",
      "validity: ważna jeszcze 11 dnido 09 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: None\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Jastrzębie-ZdrójJastrzębie-Zdrój, śląskie', 'praca stacjonarna']\n",
      "technologies: ['Tableau', 'SQL', 'Microsoft Power BI', 'Python', 'R']\n",
      "responsibilities: ['Tworzenie i rozwijanie raportów oraz pulpitów nawigacyjnych w systemach Business Intelligence', 'Analiza danych i wsparcie w podejmowaniu decyzji biznesowych', 'Optymalizacja procesów raportowania i automatyzacja zadań', 'Współpraca z użytkownikami końcowymi i zespołami IT']\n",
      "requirements: ['Wykształcenie wyższe informatyczne lub pokrewne', 'Doświadczenie w pracy z narzędziami BI (np. Power BI, Tableau)', 'Znajomość języków programowania (np. Python, R)', 'Umiejętności analityczne i znajomość SQL']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-it-ds-bi-jastrzebie-zdroj,oferta,1003893740?apid=d9d09caf-1c4a-4b1b-aa81-fc6886f358b4&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 378 ---\n",
      "id: 431\n",
      "url: https://www.pracuj.pl/praca/cloud-data-engineer-sektor-finansowy-lodz-piotrkowska-157,oferta,1003893621\n",
      "title: Cloud Data Engineer – sektor finansowy\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 11 dnido 09 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna, praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: None\n",
      "technologies: ['Microsoft Azure', 'AWS', 'Google Cloud Platform', 'Databrics', 'Spark/PySpark', 'snowFlake']\n",
      "responsibilities: ['Budowanie i projektowanie rozwiązań Data Lake i DWH', 'Automatyzowanie oraz optymalizowanie procesów przetwarzania danych', 'Łączenie danych z różnych systemów źródłowych (bazy danych, pliki, API) w jedno spójne środowisko analityczne', 'Rozwijanie procesów ekstrakcji, transformacji oraz ładowania danych (ETL) z różnych źródeł', 'Ścisła współpraca z interesariuszami biznesowymi', 'Rozwijanie i optymalizacja środowiska chmurowego pod kątem wydajności i niezawodności']\n",
      "requirements: ['Minimum 5 lat doświadczenia na podobnym stanowisku', 'Bardzo dobra znajomość języków SQL i Python oraz narzędzia Spark/PySpark', 'Doświadczenie w budowaniu skalowalnych, działających w czasie rzeczywistym i wysokowydajnych rozwiązań typu Data Lake', 'Umiejętność rozwijania procesu ETL w różnych narzędziach (SSIS, ADF, IPC)', 'Znajomość rozwiązań chmurowych (Azure, AWS, GCP)', 'Doświadczenie w projektach związanych z migracją danych z on-premise do chmury (bazy danych: Oracle, PostgreSQL, MS SQL)', 'Swobodna komunikacja w języku angielskim']\n",
      "application_link: https://www.pracuj.pl/aplikuj/cloud-data-engineer-sektor-finansowy-lodz-piotrkowska-157,oferta,1003893621?apid=89cf0c43-b539-4770-85ed-bacbc463ab46&oca=None&acv=0\n",
      "\n",
      "--- Job Record 379 ---\n",
      "id: 432\n",
      "url: https://www.pracuj.pl/praca/python-developer-krakow-kapelanka-42a,oferta,1003893605\n",
      "title: Python Developer\n",
      "work_location: None\n",
      "validity: valid for 11 daysto 09 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['Python', 'MATLAB', 'Java', 'C++', 'C#', 'Go', 'Jenkins', 'Kubernetes', 'Docker']\n",
      "responsibilities: [\"Drive development of an integrated platform for building and executing Risk models underlying majority of Bank's decisions.\", 'Provide a reference point for the exploration of advanced methods for other teams within risk area.', 'Foster innovation in risk measurement and prototype modern processes of model development.', 'Manage team of highly specialized analysts and developers.', 'Maintain good relationships with Stakeholders, ensuring mutual understanding of priorities and proper communication of results.']\n",
      "requirements: ['Solid experience in building, and testing applications in a professional environment in Python.', 'Some experience programming in a mathematical or engineering technical environment (such as MATLAB).', 'Knowledge of the more common design patterns.', 'Strong knowledge of testing principles.', 'Some experience developing in an Agile environment with preferably some experience with continuous integration practices.', 'Familiarity with algorithms for capturing and processing large datasets (i.e. Map/Reduce).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/python-developer-krakow-kapelanka-42a,oferta,1003893605?apid=bd604b37-7436-4bf8-a2f1-5148c4007f8a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 380 ---\n",
      "id: 433\n",
      "url: https://www.pracuj.pl/praca/bi-developer-warszawa-adama-branickiego-17,oferta,1003893372\n",
      "title: BI Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 11 dnido 09 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Adama Branickiego 17, Wilanów, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Microsoft Power BI', 'SQL', 'BigQuery', 'Python', 'wewnątrz organizacji']\n",
      "responsibilities: ['Projektowanie raportów i dashboardów, w oparciu o zasady efektywnej wizualizacji danych oraz specyficzne potrzeby biznesowe – Power BI', 'Duże skupienie na wizualizacji – storytelling danych', 'Przygotowywanie analiz (transformacja, modelowanie i wizualizacja informacji pochodzących z dużych zbiorów danych) na potrzeby działów biznesowych oraz kluczowych projektów, pozwalających na monitorowanie inicjatyw i efektywne komunikowanie postępów, KPI', 'Koordynacja projektów standaryzacji oraz automatyzacji procesów analitycznych – w szczególności w zakresie optymalizacji przygotowania danych, kalkulacji wskaźników i najlepszych praktyk prezentacji informacji zarządczej', 'Współpraca z interesariuszami w zakresie wyznaczenia i zastosowania krótko i długo-terminowych KPI spójnych z finansowymi celami firmy i strategią', 'Współpraca i wsparcie przy nadzorze nad przestrzenią Maczfit Official (Snowflake) - tworzenie koncepcji nowych widoków, utrzymywanie dokumentacji, słownika definicji, ustalanie standardów współpracy z BI DEV', 'Współpraca przy tworzeniu strategii rozwoju narzędzi analitycznych/BI w organizacji']\n",
      "requirements: ['Poszukujemy doświadczonego analityka, który lubi wgłębić się w dane, ustrukturyzować problemy i przeprowadzać analizy przekładające się na biznesowe rekomendacje (doświadczenie w programach lojalnościowych/ Retail/ eCommerce/ FMCG mile widziane)', 'Minimum rok doświadczenia na podobnym stanowisku', 'Wykształcenie wyższe (preferowane ekonomiczne, informatyczne lub pokrewne)', 'Biegła znajomość MS Excel (mile widziana znajomość PowerPivot, VBA, DAX itd.)', 'Obligatoryjnie doświadczenie w zakresie projektowania raportów i dashboardów przy użyciu nowoczesnych narzędzi BI (znajomość MS PowerBI)', 'Doświadczenie w zakresie transformacji dużych zbiorów danych oraz pracy z hurtowniami danych (dobra znajomość języka SQL)', 'Silna orientacja biznesowa i koncentracja na wyznaczonych celach', 'Umiejętności komunikacyjne, pozwalające na przełożenie potrzeb biznesowych na konkretne rozwiązania analityczne', 'Samodzielności i kreatywności w poszukaniu rozwiązań problemów biznesowych', 'Zaangażowanie i dobra organizacja pracy własnej; umiejętność budowy solidnych relacji z interesariuszami i członkami zespołu', 'Znajomość języka angielskiego na poziomie min. B2']\n",
      "application_link: https://www.pracuj.pl/aplikuj/bi-developer-warszawa-adama-branickiego-17,oferta,1003893372?apid=2d119606-81ce-4e13-b41c-a4a3b100700e&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 381 ---\n",
      "id: 434\n",
      "url: https://www.pracuj.pl/praca/data-science-engineer-katowice,oferta,1003893341\n",
      "title: Data Science Engineer\n",
      "work_location: None\n",
      "validity: valid for 11 daysto 09 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: None\n",
      "additional_info: ['KatowiceKatowice, Silesian']\n",
      "technologies: ['R', 'SQL', 'Python', 'TensorFlow', 'PySpark', 'Java', 'JavaScript', 'C++', 'Google Cloud Platform']\n",
      "responsibilities: ['Work with subject matter experts from airlines to identify opportunities for leveraging data to deliver insights and actionable prediction of customer behavior and operations performance.', 'Assess the effectiveness and accuracy of new data sources, data gathering and forecasting techniques.', 'Develop custom data models and algorithms to apply to data sets and run proof of concept studies.', 'Leverage existing Statistical and Machine Learning tools to enhance in-house algorithms.', 'Collaborate with software engineers to implement and test production quality code for AI/ML models.', 'Develop processes and tools to monitor and analyze data accuracy and models’ performance.', 'Demonstrate software to customers and perform value proving benchmarks. Calibrate software for customer needs and train customer for using and maintaining software.', 'Resolve customer complaints with software and respond to suggestions for enhancements.']\n",
      "requirements: ['Advanced Degree in Statistics, Operations Research, Computer Science, Mathematics, or Machine Learning.', 'Proven ability to apply modeling and analytical skills to real-world problems.', 'Knowledge of machine learning techniques (clustering, decision tree learning, artificial neural networks, etc.) and statistical concepts (regression, properties of distributions, statistical tests, etc.)', 'Solid programming skills 2-3 languages out of R, SQL, Python, TensorFlow, PySpark, Java, JavaScript or C++.', 'Absolutely must have: Graduate school level knowledge of Revenue Management models and algorithms.', 'Experience (minimum 4 out of 7) with deployment of machine learning and statistical models on a cloud:', 'MLOps within the enterprise CI/CD process for ML models – 2 years', 'Experience deploying ML APIs in production environments in GCP using GKE – 2 years', 'Experience in using GCP Vertex AI for ML and BigQuery – 1 year', 'Knowledge in Terraform and Containers technologies – 2 years', 'Experience writing data processing jobs using GCP Dataflow and Dataproc – 2 years', 'Experience setting up ML model monitoring and autoscaling for ML prediction jobs – 1 year', 'Understanding of machine learning concepts to scale ML across different services by leveraging Feature Store, Artifacts Registry and Analytics Hub – 1 year']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-science-engineer-katowice,oferta,1003893341?apid=813fe0b0-8706-4df1-8a13-14cdd75c439e&oca=None&acv=0\n",
      "\n",
      "--- Job Record 382 ---\n",
      "id: 435\n",
      "url: https://www.pracuj.pl/praca/junior-vendor-master-data-process-specialist-key-user-krakow-aleja-powstancow-slaskich-26,oferta,1003893335\n",
      "title: Junior Vendor Master Data Process Specialist / Key User\n",
      "work_location: None\n",
      "validity: valid for 11 daysto 09 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['aleja Powstańców Śląskich 26, Podgórze, KrakówKraków, Lesser Poland', 'contract of employment']\n",
      "technologies: ['SAP', 'in house', 'you focus on a single project at a time', 'you have influence on the product', 'you focus on product development']\n",
      "responsibilities: ['Vendor Data Management: Maintain and update vendor master data in systems like SAP, ensuring accuracy and policy compliance.', 'Data Validation: Review and cleanse vendor data to eliminate inaccuracies and duplicates.', 'Governance: Enforce standards for vendor data governance.', 'Process Improvement: Identify and implement efficiency enhancements and automation for vendor data processes.', 'Compliance & Security: Ensure vendor data adheres to regulations and company policies.', 'Reporting: Create reports on vendor data quality, process efficiency, and compliance metrics.', 'Vendor Collaboration: Work with internal teams to foster strong vendor relationships and smooth data exchange.', 'Training & Support: Train and support team members on vendor data processes and systems, addressing inquiries and resolving issues.']\n",
      "requirements: ['Proven experience in vendor master data management', 'Advanced knowledge of Excel', 'Fluent English (written and spoken)', 'Familiarity with SAP and/or other ERP systems', 'Strong analytical skills and attention to details', 'Ability to work independently and cooperate with internal stakeholders', 'Problem-solving mindset with a focus on process improvement and optimization']\n",
      "application_link: https://www.pracuj.pl/aplikuj/junior-vendor-master-data-process-specialist-key-user-krakow-aleja-powstancow-slaskich-26,oferta,1003893335?apid=6ed4a5e2-d4c9-4de2-8c6f-68464a4b8c3b&oca=None&acv=0\n",
      "\n",
      "--- Job Record 383 ---\n",
      "id: 436\n",
      "url: https://www.pracuj.pl/praca/data-scientist-w-obszarze-ryzyka-kredytowego-klientow-indywidualnych-warszawa-swietokrzyska-36,oferta,1003911536\n",
      "title: Data Scientist w obszarze ryzyka kredytowego klientów indywidualnych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 21 dnido 19 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Świętokrzyska 36, Śródmieście, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL', 'Python', 'AWS', 'Microsoft Azure', 'Google Cloud Platform']\n",
      "responsibilities: ['szukasz w danych odpowiedzi na ciekawe, biznesowe pytania,', 'przygotowujesz prezentacje z wynikami analiz oraz rekomendacjami,', 'rozwijasz procesy kredytowe z wykorzystaniem analityki danych,', 'budujesz i utrzymujesz modele statystyczne w obszarze ryzyka kredytowego,', 'badasz biznesowy potencjał nowych źródeł danych.']\n",
      "requirements: ['lubisz pracę z danymi i potrafisz je interpretować tak, by odpowiadały na pytania,', 'myślisz analitycznie, wyciągasz wnioski i kreatywnie szukasz rozwiązań,', 'chętnie sięgniesz po nowe technologie, które wdrażamy lub zaproponujesz własne narzędzia i metody,', 'posiadasz wiedzę z zakresu bankowości lub chciałbyś/chciałabyś poznać obszar ryzyka kredytowego,', 'dobrze znasz SQL (umiejętność tworzenia zapytań, praca z bazami danych),', 'biegle posługujesz się Pythonem (biblioteki takie jak Pandas, NumPy, Scikit-learn, itp.).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-w-obszarze-ryzyka-kredytowego-klientow-indywidualnych-warszawa-swietokrzyska-36,oferta,1003911536?apid=aae4b0fc-ce38-47f4-a54e-6d93c7496bf1&oca=None&acv=0\n",
      "\n",
      "--- Job Record 384 ---\n",
      "id: 437\n",
      "url: https://www.pracuj.pl/praca/data-engineer-warszawa-rondo-onz-1,oferta,1003868761\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze dzieńdo 27 lutego 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['rondo ONZ 1, Śródmieście, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL', 'Hadoop', 'SAP', 'SAP HANA', 'Oracle', 'wewnątrz organizacji']\n",
      "responsibilities: ['Collect, analyze, and understand the data requirements of our internal customers in all parts of the company\\u200b', 'Advise our internal customers on the design of efficient data-driven processes\\u200b', 'Create and maintain modular conceptual and logical data models based on these requirements\\u200b', 'Design, implement, test, and deploy efficient data integration/harmonization processes from a variety of internal and external data sources\\u200b', 'Design and negotiate interfaces and service level agreements with your process partners\\u200b', 'Advance the agile way of working at Data Foundation with your ideas for process improvements']\n",
      "requirements: ['Degree in Information Systems, Business Informatics, or a related field\\u200b', 'Approx. 3-5 years of work experience in implementing a modern data architecture as a requirements engineer, database developer, or software engineer, including multi-year hands-on experience with data modeling, ETL processes, and data integration/harmonization techniques (particularly SQL, Oracle, SAP BW/4HANA, Hadoop, and Informatica PowerCenter)\\u200b', '3-5 years of experience in software development (as a developer), including proven experience in effort estimation for complex software requirements\\u200b', 'Problem-solving skills and the ability to prioritize tasks effectively under pressure\\u200b', 'Practical experience in agile work environments (preferably based on SAFe) using tools such as Jira\\u200b', 'Excellent English language skills (C1 level)\\u200b', 'Very good Polish language skills (B2+ level)', 'Familiarity with IT Service Management frameworks like ITIL are beneficial\\u200b/nice to have', 'High level of customer focus, demonstrated on several occasions']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-warszawa-rondo-onz-1,oferta,1003868761?apid=92ebcff5-ddf2-4540-b33d-d5d83a4ed75f&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 385 ---\n",
      "id: 438\n",
      "url: https://www.pracuj.pl/praca/bi-architect-data-architect-data-team-lead-manufacturing-supply-chain-wroclaw-powstancow-slaskich-9,oferta,1003868462\n",
      "title: BI Architect - Data Architect - Data Team Lead (manufacturing, supply chain)\n",
      "work_location: None\n",
      "validity: valid for a dayto 27 February 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Powstańców Śląskich 9, Krzyki, WrocławWrocław, Lower Silesia', 'contract of employment', 'manager / supervisor, team manager']\n",
      "technologies: ['SQL', 'DataLake', 'ETL', 'Snowflake', 'Azure Data factory', 'ADLS', 'Azure DevOps', 'Atlan Data Catalog', 'Power BI', 'Tableau', 'BO Web Intelligence', 'Data Analytics', 'Data Warehouse', 'Certified Scrum Product Owner (CSPO)', 'Project Management Professional (PMP)', 'in house', 'you develop several projects simultaneously', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'agile', 'scrum', 'technical leader', 'architect', 'big data developer', 'data scientist', 'product owner', 'scrum master']\n",
      "responsibilities: ['Data Architecture: Design, manage, and maintain the EMEA data platform. Implement best practices for data engineering and technical standards.', 'Shape the data infrastructure, design, manage, implement, and maintain technical systems that store, process, and analyze data). Be an expert in data modelling, database and warehouse development', 'Maintain and prioritize the product backlog. Ensure alignment with business goals and stakeholders needs.', 'Team Management: Lead, mentor, and motivate a team of 8 IT professionals. Oversee daily operations, projects, and user requests.', 'Collaboration: Work with business leaders to understand and deliver new data models, processes, and reports.']\n",
      "requirements: ['Experience in data modeling, database development, and data warehousing preferably in manufacturing, supply chain, warehousing, logistics, sales & finance.', 'Proven experience in leading IT BI & data development teams using SCRUM methodology.', 'Strong background in BI and data analytics with hands on experience.', 'Proficient communication (English Polish),', 'Organizational, and time management skills.', 'Proficiency in SQL and tools like Power BI, Tableau, Snowflake, and Azure.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/bi-architect-data-architect-data-team-lead-manufacturing-supply-chain-wroclaw-powstancow-slaskich-9,oferta,1003868462?apid=74845dea-75c1-4659-9ad2-33941859d8bd&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 386 ---\n",
      "id: 439\n",
      "url: https://www.pracuj.pl/praca/analityk-systemowo-biznesowy-it-krakow-gromadzka-71,oferta,1003911118\n",
      "title: Analityk systemowo - biznesowy IT\n",
      "work_location: None\n",
      "validity: ważna jeszcze 21 dnido 19 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Gromadzka 71, Podgórze, KrakówKraków, małopolskie', 'praca stacjonarna']\n",
      "technologies: ['Confluence', 'Jira', 'XML', 'UML', 'Figma', 'wewnątrz organizacji', 'rozwijasz kilka projektów jednocześnie', 'możesz zmienić projekt', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'tworzysz kod \"od zera\"', 'koncentrujesz się na rozwoju produktu', 'agile', 'scrum', 'backend developer', 'frontend developer', 'fullstack developer', 'lider techniczny', 'architekt', 'devOps', 'embedded developer', 'programista testów automatycznych', 'tester manualny', 'support', 'product owner', 'project manager', 'scrum master', 'UI designer', 'UX designer', 'analityk systemowy']\n",
      "responsibilities: ['Opracowanie wymagań systemowych na podstawie wymagań biznesowych,', 'Współpraca z Kierownikami zespołów w zakresie rozwoju oprogramowania,', 'Rozwijanie innowacyjnych modułów w ramach licznych projektów,', 'Przygotowywanie oraz uczestniczenie w spotkaniach dotyczących wytwarzanych narzędzi.']\n",
      "requirements: ['Wykształcenie wyższe, preferowane: informatyczne, ekonomiczne lub ścisłe,', 'Posiadanie przynajmniej 2-letniego doświadczenia związanego z analizą systemów IT,', 'Umiejętność zarządzania dokumentacją przy użyciu notacji UML oraz BPMN,', 'Doświadczenie w tworzeniu i utrzymaniu dokumentacji systemowej,', 'Doświadczenie w analizie wymagań z wykorzystaniem przypadków użycia,', 'Znajomość cyklu życia oprogramowania,', 'Umiejętność modelowania zachowań oprogramowania.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/analityk-systemowo-biznesowy-it-krakow-gromadzka-71,oferta,1003911118?apid=c0ff61a5-537a-48fa-9117-998a81f8ba1b&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 387 ---\n",
      "id: 440\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-raportow-i-analiz-radom,oferta,1003902094\n",
      "title: Specjalista ds. Raportów i Analiz\n",
      "work_location: None\n",
      "validity: ważna jeszcze 16 dnido 14 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['RadomRadom, mazowieckie', 'praca stacjonarna']\n",
      "technologies: ['SQL', 'MS Excel', 'PowerBI', 'Tableau']\n",
      "responsibilities: ['Analiza danych z obszaru Łańcucha Dostaw;', 'Przygotowywanie raportów i dashboardów operacyjnych w Excelu / systemie BI;', 'Automatyzacja i robotyzacja przetwarzania danych;', 'Praca z dużymi zbiorami danych;', 'Kontrola i monitorowanie aktualności danych, identyfikowanie przyczyn rozbieżności i wprowadzanie działań korygująco-zapobiegawczych.']\n",
      "requirements: ['Biegła znajomość programu MS Excel (tabele przestawne, VBA);', 'Umiejętność tworzenia zapytań SQL;', 'Podstawy znajomości przetwarzania danych w języku Python (biblioteka „pandas”).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-raportow-i-analiz-radom,oferta,1003902094?apid=d87d6180-3895-4998-b371-cec192895fe1&oca=None&acv=0\n",
      "\n",
      "--- Job Record 388 ---\n",
      "id: 441\n",
      "url: https://www.pracuj.pl/praca/analyst-model-risk-management-retail-credit-krakow-kapelanka-42a,oferta,1003892412\n",
      "title: Analyst, Model Risk Management (Retail Credit)\n",
      "work_location: None\n",
      "validity: valid for 11 daysto 09 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: None\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Kapelanka 42a, Dębniki, KrakówKraków, Lesser Poland', 'contract of employment', 'assistant', 'Запрошуємо працівників з України']\n",
      "technologies: ['SAS', 'Python', 'R', 'Matlab', 'C++', 'VBA']\n",
      "responsibilities: ['Undertake model validation and testing activities as dictated by the Global Model Risk Policy including the assessment of; model inputs, calculations, reporting outputs, conceptual soundness of the underlying theory and the suitability of the use for its intended purpose, relevance and completeness of data, qualitative information and judgements, documentation, and implementation of the model.', 'Provide written reports detailing the results of validations highlighting issues identified during the validation.', 'Communicate technical model related information and results to Model Owners and Model Users through the course of a validation.', 'Contribute to management, regulatory, and external confidence in all models used across the group.']\n",
      "requirements: ['Master’s or PhD degree in a quantitative discipline like Financial Mathematics, Statistics, Econometrics, Quantitative Finance, Economics or Engineering.', 'Experience with some statistical modelling software / programming language e.g. SAS, Python, R, Matlab, C++, VBA.', 'Experience of developing and reviewing models throughout the customer lifecycle.', 'Experience of conducting independent model reviews is beneficial.', 'Some knowledge in Retail Credit, Stress Testing and/or Front Office Modelling and an awareness of IRB and/or IFRS9 regulatory requirements.', 'Knowledge of statistical model and scorecard development techniques.', 'Knowledge of Risk models, performance metrics and risks and associated issues.', 'Some knowledge of internal procedures and local regulations and those of other country regulators would be an advantage.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/analyst-model-risk-management-retail-credit-krakow-kapelanka-42a,oferta,1003892412?apid=6480d560-f519-480f-ba94-baa5a1b129a1&oca=None&acv=0\n",
      "\n",
      "--- Job Record 389 ---\n",
      "id: 442\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-bi-warszawa-wybrzeze-kosciuszkowskie-2,oferta,1003867951\n",
      "title: Specjalista ds. BI\n",
      "work_location: None\n",
      "validity: ważna jeszcze dzieńdo 27 lutego 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), młodszy specjalista (Junior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Wybrzeże Kościuszkowskie 2, Śródmieście, WarszawaWarszawa, mazowieckie', 'Запрошуємо працівників з України']\n",
      "technologies: ['Snowflake Data Cloud', 'wewnątrz organizacji', 'rozwijasz kilka projektów jednocześnie', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'agile', 'waterfall', 'big data developer', 'tester manualny']\n",
      "responsibilities: ['Współpraca z użytkownikami w celu zrozumienia ich potrzeb biznesowych i technologicznych w zakresie danych.', 'Projektowanie architektury danych oraz rozwiązań analitycznych opartych na Snowflake.', 'Wdrażanie i konfiguracja środowisk Snowflake, w tym integracja z narzędziami ETL/ELT oraz BI.', 'Optymalizacja wydajności baz danych i zapytań w Snowflake.', 'Szkolenie i wsparcie dla zespołów klientów w zakresie najlepszych praktyk używania Snowflake.', 'Monitorowanie i diagnostyka problemów z danymi oraz zaproponowanie odpowiednich rozwiązań.', 'Współpraca z Dostawcami w zakresie wdrożenia rozwiązania Snowflake.']\n",
      "requirements: ['Doświadczenie w pracy z platformą Snowflake lub podobnymi rozwiązaniami chmurowymi.', 'Umiejętność projektowania architektury danych oraz procesów ETL/ELT.', 'Doskonałe umiejętności analityczne i rozwiązywania problemów.', 'Umiejętność pracy w zespole.', 'Dobre umiejętności organizacyjne oraz zdolność do pracy wielozadaniowej.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-bi-warszawa-wybrzeze-kosciuszkowskie-2,oferta,1003867951?apid=aa0068ce-1af9-4daa-9f54-097da7d66bd6&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 390 ---\n",
      "id: 443\n",
      "url: https://www.pracuj.pl/praca/data-and-analytics-senior-strategist-for-business-development-warszawa,oferta,1003909946\n",
      "title: Data and Analytics Senior Strategist for Business Development\n",
      "work_location: Company locationWarszawaWarszawa, Masovian\n",
      "validity: valid for 21 daysto 19 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: None\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['Google Analytics', 'Tableau', 'Firebase', 'Snowflake Data Cloud']\n",
      "responsibilities: ['Build and implement Data and Analytics strategy that will support client business;', 'Identify the data maturity level of the organization and guide them to the next stage;', 'Drive new business offerings for scandiweb existing clients and leads;', 'Plan Data and Analytics tech ecosystem improvements for leads (CDP, data warehouses, data visualization, etc);', 'Develop long-term Analytics roadmaps and present them to C-suite executives;', 'Engage in key client meetings to present data findings and discuss strategic recommendations.']\n",
      "requirements: ['At least 2-5 years of experience in a senior role focusing on data analytics and business development;', 'Strong background in using analytics to drive sales strategies in eCommerce or similar industries;', 'Proficiency in major analytics and business intelligence platforms (Google Analytics, Tableau, GTM, CDPs);', 'Easily navigate across all available platforms, tools, and tactics in Data Analytics - from Adobe Analytics to Firebase, from BigQuery to Snowflake and everything around it;', 'Exceptional analytical and problem-solving skills with the ability to convert complex data into actionable business strategies;', 'Excellent presentation skills, confident spoken and written English;', 'Exceeding communication and client relationship skills to effectively manage expectations and deliver insights.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-and-analytics-senior-strategist-for-business-development-warszawa,oferta,1003909946?apid=ccd38703-aa13-4906-a065-929b73dd9697&oca=None&acv=0\n",
      "\n",
      "--- Job Record 391 ---\n",
      "id: 444\n",
      "url: https://www.pracuj.pl/praca/data-engineer-cop-gdansk-aleja-grunwaldzka-415,oferta,1003909932\n",
      "title: Data Engineer (CoP)\n",
      "work_location: None\n",
      "validity: valid for 21 daysto 19 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['aleja Grunwaldzka 415, Oliwa, GdańskGdańsk, Pomeranian', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['Python', 'SQL', 'Azure Data Factory', 'Azure Databricks']\n",
      "responsibilities: ['Actively engage within our Community of Practice together with other digital experts across Europe,', 'Mentor and support team members, encouraging best practices and knowledge sharing,', 'Drive engagement and identify growth opportunities within our data engineering network,', 'Plan, implement and maintain data products on our Azure Data & Analytics Platform,', 'Establish and execute processes to monitor data availability, integrity, and quality,', 'Explore and analyze data and communicate relevant insights to stakeholders.']\n",
      "requirements: ['3+ years of experience working with data as a Data Engineer,', 'Working experience with ADLS (Azure Data Lake Storage), Azure Data Factory, Databricks, Azure SQL Databases,', 'Ability to work independently and quickly learn new technologies,', 'Experience in coordination and organization of community or team activities,', 'Working Experience in architecture and design of modern cloud-based data platforms,', 'Experience in working in cross-functional teams in an agile environment,', 'Experience in data engineering with a focus on supporting and mentoring others,', 'Excellent communication and interpersonal skills, with a passion for community building,', 'Strong analytical thinking,', 'Willingness to come into the office in Gdańsk at least twice a week or  to relocate to Trójmiasto,', 'Willingness to expand your comfort zone by sometimes leaving it,', 'Excellent communication and interpersonal skills, with a passion for community building,', 'English fluent, spoken and written.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-cop-gdansk-aleja-grunwaldzka-415,oferta,1003909932?apid=74889275-46ed-43a1-b6f5-2c25d1ccefc5&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 392 ---\n",
      "id: 445\n",
      "url: https://www.pracuj.pl/praca/data-scientist-poznan-wierzbiecice-1b,oferta,1003891415\n",
      "title: Data Scientist\n",
      "work_location: None\n",
      "validity: valid for 11 daysto 09 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: None\n",
      "additional_info: ['Wierzbięcice 1b, Wilda, PoznańPoznań, Greater Poland', 'contract of employment']\n",
      "technologies: ['GenAI', 'Python', 'SQL', 'Kubernetes', 'Docker', 'yaml', 'Kubeflow', 'MLflow', 'scrum']\n",
      "responsibilities: ['You will develop machine learning and AI models in various areas ranging from manufacturing and commercial data. Use cases can range from image recognition and computer vision to email classification and optimization algorithms.', 'You will work on the entire AI lifecycle, from ideation to production. The role requires you to not only develop a model but also be able to wrap it into a product which can be used by people both internally (ROCKWOOL employees) and externally (ROCKWOOL customers).', 'You will utilize technologies such as Neural networks, XGBoost, time series analysis, YOLO, Azure OpenAI, LLMs, vector databases, Langchain, and other services to create and maintain solutions across the AI landscape.', 'You will utilize web technologies such as React to develop web applications as interfaces for our AI models and databases such as Postgres, MongoDB, and other database management systems to manage the underlying data for the AI products.']\n",
      "requirements: [\"A master's degree in computer science, Data Science, Mathematics, Statistics, or related field\", '3+ years of experience in Data Science', 'Experience with both GenAI and classical machine learning techniques.', 'Experience of working with LLMs, transformer models, vector databases, RAG frameworks, Langchain, etc.', 'Excellent skills within Python, SQL, and cloud computing technologies', 'Hands on experience in deploying models and algorithms into production systems with tools such as Kubernetes, docker, yaml, etc.', 'Experience with ML Ops tools and frameworks such as Kubeflow, MLflow, e.g. is desirable', 'Professional in English language', 'ROCKWOOL are keen to nurture employees develop their talents and move forward in their careers. You can expect to be supported in your professional ambitions through our programs, to help you reach your professional goals.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-poznan-wierzbiecice-1b,oferta,1003891415?apid=37dffeae-24de-4bdb-901c-2401d65380e7&oca=None&acv=0\n",
      "\n",
      "--- Job Record 393 ---\n",
      "id: 446\n",
      "url: https://www.pracuj.pl/praca/data-automation-senior-specialist-warszawa-plac-konesera-11,oferta,1003909616\n",
      "title: Data Automation Senior Specialist\n",
      "work_location: None\n",
      "validity: ważna jeszcze 21 dnido 19 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Plac Konesera 11, Praga-Północ, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Google Cloud Platform', 'SQL', 'PowerQuery', 'DAX', 'BigQuery', 'Power BI']\n",
      "responsibilities: ['pobieranie i przygotowywanie danych z różnych systemów reklamowych (Google Ads, Facebook Ads, itp.),', 'monitorowanie i zapewnianie wysokiej jakości danych,', 'przygotowywanie zestawów danych wspierających zespoły analityczne i data science,', 'tworzenie dashboardy - głownie w PowerBI,', 'projektowanie frameworków raportowych,', 'rekomendowanie kompleksowych zmian w raportowaniu,', 'udzielanie wsparcia merytorycznego współpracownikom,', 'nadzorowanie konfiguracji oraz zarządzanie automatycznymi raportami, w tym tworzenie i dystrybuowanie raportów ad-hoc.']\n",
      "requirements: ['praktyczna znajomość technologii chmurowych, w tym Google Cloud Platgform/BigQuery,', 'doświadczenie w budowie i utrzymaniu procesów ETL/ELT w BigQuery, obejmujących oczyszczanie, transformację i agregację dużych zbiorów danych,', 'umiejętność wykorzystywania SQL, PowerQuery/M oraz DAX,', 'umiejętność tworzenia zaawansowanych dashboardów w Power BI,', 'swobodna komunikacja w języku angielskim (min.B2),', 'świetnie odnajdziesz się w naszym zespole jeśli cechujesz się samodzielnością, skrupulatnością oraz dobrą komunikacją biznesową.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-automation-senior-specialist-warszawa-plac-konesera-11,oferta,1003909616?apid=86bceb58-d5cc-4a35-acd6-c8522be0d16e&oca=None&acv=0\n",
      "\n",
      "--- Job Record 394 ---\n",
      "id: 447\n",
      "url: https://www.pracuj.pl/praca/senior-data-warehouse-developer-data-management-area-warszawa,oferta,1003866622\n",
      "title: Senior Data Warehouse Developer – Data Management Area\n",
      "work_location: None\n",
      "validity: None\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'ważna 5 godzin']\n",
      "technologies: ['Oracle', 'IBM Infosphere Data Stage', 'Linux', 'PL/SQL', 'ETL', 'DataWarehouse', 'Java', 'Bash', 'SAS', 'Python', 'Kafka', 'GCP', 'wewnątrz organizacji', 'agile']\n",
      "responsibilities: ['modelujesz struktury Hurtowni Danych', 'projektujesz i programujesz zadań ETL', 'pracujesz z bazą danych Oracle', 'integrujesz wytwarzane rozwiązania z innymi wewnętrznymi systemami Banku i Grupy ING', 'przygotowujesz zmiany programowe zgodnie z praktykami CI/CD (konfiguracja, skrypty)', 'automatyzujesz testy']\n",
      "requirements: ['znasz problematykę Hurtowni Danych i tworzenia procesów ETL', 'posiadasz bardzo dobrą znajomość PL\\\\SQL oraz baz danych Oracle', 'masz doświadczenie w dowolnej technologii ETL (preferowany IBM Infosphere Data Stage)', 'posiadasz znajomość Linux oraz umiejętność pisania skryptów powłoki', 'znasz język angielski na poziomie min. B2', 'potrafisz pracować w samoorganizującym się zespole z wykorzystaniem metodologii Agile (Scrum)', 'masz doświadczenie programistyczne w jednym z obiektowych języków programowania']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-warehouse-developer-data-management-area-warszawa,oferta,1003866622?apid=ed088024-6a4e-4f61-b273-f9cbe307928a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 395 ---\n",
      "id: 448\n",
      "url: https://www.pracuj.pl/praca/data-business-analyst-warszawa-domaniewska-44a,oferta,1003890358\n",
      "title: Data Business Analyst\n",
      "work_location: Company locationDomaniewska 44A, Mokotów, WarszawaWarszawa, Masovian\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['SQL', 'Microsoft Power BI', 'in house']\n",
      "responsibilities: ['Scope definition as classing Business Analysis & BRS', 'Scope definition via mockups', 'Possibility to organize workshops with key business stakeholders', 'Monitoring BI project progress & supporting scope control', 'Being proxy-Product Owner in name of customer']\n",
      "requirements: ['1-3 years+ experience in similar role on commercial projects strictly on DATA projects', '1 years+ experience with use of PowerBI on commercial projects', 'Experience with organizing and conducting workshops with business stakeholders from customer', 'Strong ability to establish two-ways communication channels between business users and technical BI frontend and backend developers', 'Basic knowledge of data modeling principles (for example designing and optimizing star schema and snowflake schema, different kinds of relationships etc)', 'Understanding PowerBI security models', 'Applying best practices in UX/UI for dashboards', 'Understanding business processes and KPIs', 'Making data-driven recommendations', 'Experience with SQL Server incl. SSIS and SSAS, DataBricks, Snowflake is a plus', 'Possibility to work with customers face to face in Warsaw (place of residence close to Warsaw is a strong plus)', 'Excellent written and spoken English skills at the B2+ level']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-business-analyst-warszawa-domaniewska-44a,oferta,1003890358?apid=2eda4fb6-e003-4beb-a32f-9818366effd1&oca=None&acv=0\n",
      "\n",
      "--- Job Record 396 ---\n",
      "id: 449\n",
      "url: https://www.pracuj.pl/praca/bi-developer-data-engineer-poznan-wierzbiecice-1b,oferta,1003923242\n",
      "title: BI Developer/Data Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 25 dnido 23 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Wierzbięcice 1B, Wilda, PoznańPoznań, wielkopolskie']\n",
      "technologies: ['PowerBI', 'SQL', 'DAX', 'Python', 'Azure', 'SAP', 'wewnątrz organizacji', 'rozwijasz kilka projektów jednocześnie', 'możesz zmienić projekt', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt', 'agile', 'scrum', 'kanban']\n",
      "responsibilities: ['Projektowanie, implementacja i optymalizacja architektury hurtowni danych.', 'Tworzenie i optymalizacja efektywnych kosztowo zapytań SQL oraz DAX.', 'Tworzenie i modyfikacja modeli danych w Power BI, aby wspierać analizy i raportowanie.', 'Tworzenie i zarządzanie przepływami danych z różnych źródeł oraz automatyzacja procesów przetwarzania.', 'Zbieranie i dokumentowanie wymagań biznesowych oraz współpraca z interesariuszami w celu dostarczenia odpowiednich rozwiązań analitycznych.', 'Udzielanie wsparcia technicznego użytkownikom końcowym oraz organizowanie szkoleń w zakresie narzędzi BI.']\n",
      "requirements: ['Co najmniej 2-letnie doświadczenie na stanowisku BI Developer, Data Engineer lub podobnym,', 'Bardzo dobra znajomość języka SQL oraz DAX,', 'Dobra znajomość języka programowania Python,', 'Umiejętność projektowania modeli semantycznych Power BI,', 'Znajomość komponentów Azure związanych z danymi (Databrics, Delta Lake, etc)', 'Znajomość DevOps', 'Znajomość języka angielskiego na poziomie umożliwiającym swobodną komunikację (min. B2).', 'Otwartość na współpracę w międzynarodowym zespole,', 'Otwartość, proaktywność i silna motywacja do nauki,', 'Nastawienie na poszukiwanie rozwiązań,', 'Dociekliwość, umiejętność analitycznego myślenia i budowania logicznych ścieżek zdarzeń,', 'Wysoki poziom samoorganizacji pracy oraz duża samodzielność w wykonywaniu.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/bi-developer-data-engineer-poznan-wierzbiecice-1b,oferta,1003923242?apid=2af94556-974b-42a4-a868-9409e14a8565&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 397 ---\n",
      "id: 450\n",
      "url: https://www.pracuj.pl/praca/data-architect-wroclaw-plac-powstancow-slaskich-1,oferta,1003922852\n",
      "title: Data Architect\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: ważna jeszcze 25 dnido 23 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca zdalna\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: None\n",
      "technologies: ['SQL']\n",
      "responsibilities: ['Projektowanie architektury danych dla funkcjonalności biznesowych organizacji,', 'Modelowanie danych pod wykorzystanie w narzędziach hurtownianych (np. budowanie modelu danych dla wybranego wycinka rzeczywistości biznesowej firmy tj. model dla datamart marketingu)', 'Zrozumienie obecnego modelu funkcjonowania firmy tj. danych dotyczących procesów operacyjnych', 'Projektowanie źródłowych baz danych pod dalsze wykorzystanie w innych systemach', 'Współpraca z odbiorcami danych wewnątrz i zewnętrznych firmy.']\n",
      "requirements: ['Co najmniej 2 lata doświadczenia w pracy z systemami hurtowni danych', 'Znajomość dowolnego narzędzia służącego do przechowywania informacji o danych tzn. ich znaczenia oraz relacji między nimi', 'Znajomość narzędzia do modelowanie procesów biznesowych', 'Umiejętność logicznego, analitycznego myślenia', 'Dbałość o szczegóły i uporządkowanie efektów swojej pracy', 'Samodzielność i odpowiedzialność']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-architect-wroclaw-plac-powstancow-slaskich-1,oferta,1003922852?apid=91610ab9-b1c8-4124-87f2-d728d18158a8&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 398 ---\n",
      "id: 451\n",
      "url: https://www.pracuj.pl/praca/data-analyst-warszawa-lopuszanska-32,oferta,1003865661\n",
      "title: Data Analyst\n",
      "work_location: None\n",
      "validity: None\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Łopuszańska 32, Włochy, WarszawaWarszawa, mazowieckie', 'ważna 5 godzin']\n",
      "technologies: ['Python', 'SQL', 'Microsoft Power BI']\n",
      "responsibilities: ['Work closely with business, corporate data professionals, and other stakeholders to understand data requirements and objectives', 'Utilize analytical techniques to extract valuable insights (e.g., patterns, trends, and correlations) in data to support decision-making processes;', 'Communicate findings and insights to non-technical stakeholders in a clear and understandable manner;', 'Monitor and assess the quality of data, identifying and rectifying inconsistencies or errors;', 'Develop and implement data quality standards and procedures.', 'Collaborate with data stewards and other stakeholders to establish and enforce data quality policies;', 'Control, cleanse, and transform data into a usable format for analysis and reporting purposes;', 'Ensure compliance with data privacy regulations and organizational data governance standards;', 'Identify opportunities for process improvement in data management and analysis;', 'Stay informed about advancements in data analytics and quality assurance methodologies;', 'Contribute to data-related communities of practices at ZEISS.']\n",
      "requirements: ['An excellent university degree in a relevant field such as Data Science, Statistics, Computer Science, Information Management, or a related discipline;', 'Strong analytical and problem-solving skills to interpret complex data sets and extract meaningful insights;', 'Proficiency in statistical analysis and data modeling;', 'Familiarity with data analytics tools and languages such as Python, SQL, or other statistical software;', 'Experience working with data visualization tools (Microsoft Power BI is a plus) to communicate findings effectively;', 'Experience working with data cataloging and metadata management tools (e.g. Informatica, Collibra);', '3+ years of progressively responsible experience performing duties related to data and analytics (e.g., data quality, data governance, data architecture, data lineage, data modelling, metadata, business intelligence, machine learning, artificial intelligence, …);', 'Knowledge of data quality management principles and best practices;', 'Understanding of database systems and data warehousing concepts;', 'Understanding of regulatory requirements related to data privacy and compliance;', 'Effective communication skills to convey complex technical information to non-technical stakeholders;', 'A proactive approach to identifying and solving data-related challenges;', 'Fluent English language skills;', 'Stay updated on industry best practices and emerging technologies related to data analytics and quality;']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-analyst-warszawa-lopuszanska-32,oferta,1003865661?apid=53a9b762-462a-45aa-8540-e1e721b3daa5&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 399 ---\n",
      "id: 452\n",
      "url: https://www.pracuj.pl/praca/senior-product-data-analyst-warszawa-prosta-67,oferta,1003922631\n",
      "title: Senior Product Data Analyst\n",
      "work_location: Company locationProsta 67, Włochy, WarszawaWarszawa, Masovian\n",
      "validity: valid for 25 daysto 23 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['SQL', 'BigQuery', 'PowerBI', 'in house', 'you develop several projects simultaneously', 'you focus on product development']\n",
      "responsibilities: ['Drive insights: Understand funnels, ecosystems, user behaviors, and long-term trends in the adoption of our products to identify opportunities for step-changes of specific customer segments.', 'Metric Definition: Define and analyze key metrics that reflect the success of products and allow us to monitor the health of product adoption in the markets we serve.', 'Growth Opportunities: Identify growth opportunities, remove barriers for product adoption, retention, engagement, and/or monetization by quantitative analysis, data mining, and the presentation of data to see beyond the numbers and understand how our merchants and consumers interact with Booksy products.', 'Reporting: Prepare, monitor and adjust reports and reporting tools to best contribute to Product Team results.', 'Collaboration: Cooperate with Product Teams to support them in their product related decisions based on data.', 'Process automation: Lead efforts to automate data management and reporting processes, working closely with Business Intelligence Analysts and Data Engineers to enhance efficiency.', 'Ownership: Take ownership of the more complex business challenges, owning them end to end with minimal levels of support.', 'Collaboration with stakeholders: Consulting with senior stakeholders across the organisation to understand their business needs and determine the best data / analytics approach delivering value from our data in a clear and actionable manner.', 'Co-responsible for driving the development of the domain, managing and mentoring less experienced analysts, and ensuring the quality and timely delivery of their outputs.']\n",
      "requirements: ['Experience: 4+ years of experience in an analytical role dealing with large volumes of data.', 'Data Skills: 4+ years of experience working with databases and excellent knowledge of SQL (BigQuery would be an asset) to create complex queries and analyses.', 'Product Optimization: 4+ years experience with product optimization or growth work i.e. product A/B testing, funnel/website optimization, conversion analysis,\\xa0events monitoring.', 'Collaboration: Ability to collaborate and communicate effectively within the team, with internal stakeholders i.e. product managers, engineers, UX designers, data engineers.', 'Mobile App Experience: 2+ years experience as an analyst in the mobile application environment.', 'Data-Driven Insights: Proven experience in providing insights and recommendations based on data analysis.', 'Communication Skills: Excellent verbal and written communication skills, with the ability to present complex findings to both technical and non-technical audiences.', 'Data Visualization: Familiarity with data visualization tools like PowerBI, Looker, Tableau, QlikView, or others.', 'Educational Background: A higher degree in statistics, mathematics, or another technical field.', 'Language Skills: Proficiency in spoken and written English.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-product-data-analyst-warszawa-prosta-67,oferta,1003922631?apid=ecac25f4-e77c-4c27-bb9f-ada7c5a58cfd&oca=None&acv=0\n",
      "\n",
      "--- Job Record 400 ---\n",
      "id: 453\n",
      "url: https://www.pracuj.pl/praca/senior-data-scientist-warszawa-chmielna-89,oferta,1003889729\n",
      "title: Senior Data Scientist\n",
      "work_location: None\n",
      "validity: ważna jeszcze 10 dnido 08 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Chmielna 89, Wola, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Python', 'Google Cloud Platform', 'Linux', 'Bash', 'Git']\n",
      "responsibilities: ['budujesz modele NLP - deep learning/transformery/large language models za pomocą PyTorch/TensorFlow,', 'tworzysz modele ML/AI [problemy klasyfikacyjne/regresyjne/analizy przestrzenne GEO /sieci społeczne] - od prototypowania do produkcji,', 'zajmujesz się utrzymaniem, monitorowaniem oraz ewaluacją modeli ML w czasie [fabryka modeli],', 'przeprowadzasz detekcję zdarzeń o dużej mocy predykcyjnej,', 'budujesz kompleksowe przepływy danych i ML [AirFlow],', 'uczestniczysz w innowacyjnych projektach, np. hiperpersonalizacji (m.in. z użyciem GenAI).']\n",
      "requirements: ['masz wiedzę i doświadczenie w machine learning i przetwarzaniu danych,', 'jesteś osobą samodzielną, dobrze zorganizowaną, proaktywną i kreatywną,', 'posiadasz wykształcenie wyższe,', 'chcesz się dalej rozwijać i podnosić swoje kwalifikacje.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-scientist-warszawa-chmielna-89,oferta,1003889729?apid=580c0001-755c-44a0-98a6-bd7b6194c93b&oca=None&acv=0\n",
      "\n",
      "--- Job Record 401 ---\n",
      "id: 454\n",
      "url: https://www.pracuj.pl/praca/bi-data-engineer-wroclaw-powstancow-slaskich-7a,oferta,1003865570\n",
      "title: BI & Data Engineer\n",
      "work_location: Miejsce pracyCała Polska (praca zdalna)\n",
      "validity: None\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['ważna 5 godzin']\n",
      "technologies: ['Microsoft Power BI', 'Microsoft Excel', 'SQL', 'DAX', 'MS Excel Power Query (M language)', 'Python', 'ETL processes', 'Data warehousing', 'CI/CD (Git)', 'MS Power Platform', 'Power Apps', 'Power Automate', 'SSIS', 'wewnątrz organizacji', 'rozwijasz kilka projektów jednocześnie', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'masz wpływ na produkt']\n",
      "responsibilities: ['Twoim zadaniem będzie rozwijanie i ulepszanie naszych systemów BI oraz struktur raportowania. Będziesz usprawniać procesy cyfrowe i automatyzować systemy, aby wszystko działało sprawniej i bardziej przejrzyście.', 'Stworzysz, sprawdzisz i udokumentujesz procesy ETL, które pomogą w wewnętrznym raportowaniu. Będziesz także rozwijać i utrzymywać dashboardy oraz raporty, które ułatwią monitorowanie i analizę kluczowych wskaźników (KPI).', 'Współpracując z różnymi zespołami, będziesz szukać sposobów na ulepszenia, łącząc techniczne rozwiązania z celami biznesowymi.', 'Krótko mówiąc - Twoje analizy i rekomendacje oparte na danych będą wspierać podejmowanie kluczowych decyzji w całej organizacji.']\n",
      "requirements: ['Minimum 2 lata doświadczenia', 'Angielski C1', 'Umiejętności analityczne', 'Efektywna komunikacja', 'Zainteresowanie raportowaniem finansowym', 'Chęć rozwoju w dynamicznym środowisku danych', 'Power BI', 'DAX', 'MS Excel', 'MS Excel Power Query (M language)', 'SQL', 'Python (pandas, requests)', 'ETL processes', 'Data warehousing', 'CI/CD (Git)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/bi-data-engineer-wroclaw-powstancow-slaskich-7a,oferta,1003865570?apid=4ab8fb45-64c0-4f23-881e-236bc456c9af&oca=None&acv=0\n",
      "\n",
      "--- Job Record 402 ---\n",
      "id: 455\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-warszawa-aleje-jerozolimskie-134,oferta,1003922406\n",
      "title: Senior Data Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 25 dnido 23 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Aleje Jerozolimskie 134, Ochota, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL', 'Python', 'Pandas', 'NumPy', 'GitHub', 'Big Query', 'Pub/Sub', 'Data Flow', 'Google Cloud Platform']\n",
      "responsibilities: ['Wdrażanie i usprawnianie narzędzi raportowych i analitycznych', 'Tworzenie i rozbudowa struktur/baz danych', \"Budowanie pipeline'ów przepływu danych pomiędzy różnymi źródłami\", 'Przygotowywanie analiz i raportów zarządczych na potrzeby spółki holdingowej i spółek zależnych', 'Tworzenie i rozwijanie repozytorium kodu', 'Prowadzenie projektów zapewniających poprawę jakości danych w organizacji']\n",
      "requirements: ['Wykształcenie wyższe – finanse, ekonomia, matematyka, informatyka, itp.- mile widziane', 'Praktyczna znajomość SQL na zaawansowanym poziomie,', 'Praktyczne doświadczenie w pracy na środowisku Google Cloud, w szczególności: Big Query, Pub/Sub, Data Flow - mile widziane', 'Znajomość języka Python wraz z bibliotekami Pandas, NumPy - wymóg konieczny.', 'Zawodowe doświadczenie w pracy z GitHub.', 'Zdolności analityczne, umiejętność oceny sytuacji, samodzielnego podejmowania decyzji oraz przygotowywania rekomendacji,', 'Inicjatywa, pomysłowość, umiejętność znajdowania rozwiązań, otwartość na zdobywanie nowej wiedzy zwłaszcza w zakresie nowych narzędzi i technologii,', 'Team Player - ważna jest dla Ciebie dobra atmosfera w zespole oraz możliwość dzielenia się wiedzą,', 'Znajomość zagadnień rachunkowości finansowej i zarządczej - mile widziana.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-warszawa-aleje-jerozolimskie-134,oferta,1003922406?apid=97dafa93-b55a-4afd-a8d4-b4b27d598e5e&oca=None&acv=0\n",
      "\n",
      "--- Job Record 403 ---\n",
      "id: 456\n",
      "url: https://www.pracuj.pl/praca/devops-engineer-warszawa,oferta,1003873527\n",
      "title: DevOps Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 3 dnido 01 marca 2025\n",
      "contract_type: umowa o pracę, umowa zlecenie, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Python', 'Java', 'C#', 'C++', '.NET', 'JQuery', 'React.js', 'Angular', 'GitLab', 'Logstash', 'Kibana', 'Grafana', 'Zabbix', 'AWS', 'Microsoft Azure', 'Kubernetes', 'Red Hat', 'OpenStack']\n",
      "responsibilities: ['Projektowanie, konfiguracja i optymalizacja systemów Elastic Stack (Elasticsearch, Logstash, Kibana, Beats)', 'Integracja rozwiązań AI/ML z Elastic w celu analizy dużych zbiorów danych', 'Tworzenie inteligentnych dashboardów w Kibana i automatyzacja procesów analizy danych', 'Optymalizacja zapytań i konfiguracji Elasticsearch pod kątem wydajności', 'Współpraca z zespołami Data Science, DevOps, Cybersecurity w zakresie przetwarzania i monitorowania logów', 'Implementacja algorytmów AI/ML do wykrywania anomalii i analiz predykcyjnych', 'Zarządzanie infrastrukturą Elastic w środowiskach on-premise i chmurowych (AWS, Azure, GCP)']\n",
      "requirements: ['Doświadczenie w konfiguracji i administracji Elasticsearch (min. 1 rok)', 'Znajomość Elastic AI Assistant, ELSER (Elastic Learned Sparse Encoder) i integracji AI/ML', 'Umiejętność pracy z Logstash, Kibana, Beats i optymalizacji zapytań w Elasticsearch', 'Znajomość języków Python, Bash, SQL oraz technologii JSON, REST API', 'Podstawowa znajomość Linux i środowisk kontenerowych (Docker, Kubernetes)', 'Mile widziana znajomość narzędzi TensorFlow, PyTorch, Scikit-learn w kontekście analizy danych w Elastic', 'Znajomość rozwiązań SIEM i systemów cyberbezpieczeństwa będzie dodatkowym atutem']\n",
      "application_link: https://www.pracuj.pl/aplikuj/devops-engineer-warszawa,oferta,1003873527?apid=11787054-4b00-4852-b7b6-c478fa39bb89&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 404 ---\n",
      "id: 457\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-zapewnienia-ciaglosci-dostepu-do-danych-pabianice,oferta,1003905827\n",
      "title: Specjalista ds. zapewnienia ciągłości dostępu do danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 10 dnido 08 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['PabianicePabianice, łódzkie']\n",
      "technologies: []\n",
      "responsibilities: ['Administrowanie systemami backupu oraz storage w środowisku klasy enterprise,', 'Dbanie o zgodność infrastruktury zgodnie z najlepszymi praktykami w zakresie budowania, administracji oraz bezpieczeństwa infrastruktury,', 'Proaktywny monitoring infrastruktury oraz zapobieganie potencjalnym awariom i przestojom,', 'Realizacja zleceń wewnętrznych i projektowych,', 'Nadzorowanie nad wykorzystaniem zasobów i monitoring użycia SAN,', 'Wprowadzanie zmian, poprawek i aktualizacji bezpieczeństwa,', 'Współpraca z zespołami IT oraz zewnętrznymi dostawcami,', 'Nadzór i prowadzenie dokumentacji administracyjnej i technicznej.']\n",
      "requirements: ['Min 3 letnie doświadczenie w administracji systemami backupu z rodziny produktów Veritas/Commvault,', 'Min. roczne doświadczenie w administracji bibliotek taśmowych (preferowane urządzenia Quantum),', 'Min 3 letnie doświadczenie w administracji systemami pamięci masowej, w tym macierze blokowe, plikowe, obiektowe. Preferowane znajomość urządzeń marek IBM/Dell/HPE/Netapp,', 'Praktycznej znajomości SAN, w tym także administracji sieciami fabric FC,', 'Znajomość rozwiązań i zasad budowy wysokodostępnych rozwiązań backup/storage,', 'Znajomości rozwiązań VMware vSphere, vSAN, w zakresie backup/storage,', 'Dobra znajomość systemów operacyjnych Linux oraz Windows,', 'Dobra znajomość narzędzi monitoringu rozwiązań backup/storage,', 'Umiejętność i chęć do pracy w zespole specjalistów obszarowych,', 'Znajomość języka angielskiego na poziomie komunikatywnym w mowie i piśmie.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-zapewnienia-ciaglosci-dostepu-do-danych-pabianice,oferta,1003905827?apid=73a9f056-78e0-4596-8c2a-26afc7a8d5f8&oca=None&acv=0\n",
      "\n",
      "--- Job Record 405 ---\n",
      "id: 458\n",
      "url: https://www.pracuj.pl/praca/business-intelligence-analyst-warszawa-zelazna-51-53,oferta,1003891843\n",
      "title: Business Intelligence Analyst\n",
      "work_location: None\n",
      "validity: valid for 11 daysto 09 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Żelazna 51/53, Wola, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['SQL', 'Python', 'Google Cloud Platform', 'agile']\n",
      "responsibilities: ['Delivering value with data analysis across multiple FCP areas.', 'Acting as a bridge and speaking partner between business and IT. translating business needs into BI-deliverables.', 'Assisting the Product Owner in driving different projects.', 'Accelerating the process of allowing others to explore and analyse data.']\n",
      "requirements: ['University Degree, preferably in quantitative topics such as computer science, engineering, mathematics, physics or quantitative finance/economics.', 'Proficiency in Tableau.', 'Basic SQL competencies.', 'Fluency in English language (spoken and written).', 'Excellent analytical skills.', 'Minimum 2 years of experience in data analytics, business intelligence, data visualization or similar roles.', 'Ability to describe complex material simply and clearly.', 'Ability to communicate effectively and collaborate well with other people.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/business-intelligence-analyst-warszawa-zelazna-51-53,oferta,1003891843?apid=6dfae3b5-6324-44fc-b13e-9399a2d3b55a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 406 ---\n",
      "id: 459\n",
      "url: https://www.pracuj.pl/praca/data-engineer-cloud-etl-warszawa-czerska-8-10,oferta,1003888935\n",
      "title: Data Engineer (cloud, ETL)\n",
      "work_location: None\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Czerska 8/10, Mokotów, WarszawaWarszawa, Masovian']\n",
      "technologies: ['Python', 'SQL', 'GIT', 'Google Cloud Platform', 'BigQuery', 'Airflow', 'DBT']\n",
      "responsibilities: [\"Designing, implementing, and maintaining a business-facing data warehouse (Big Query) that will support our company's commitment to analytics, as well as our data-driven business philosophy.\", 'Constructing ETL data pipelines (Google Cloud Functions, DBT, Data Transfers, Python, even a little R)', 'Creating and monitoring daily ETL tasks and data sync processes for cleaning and structuring data for many different sources - marketing APIs (tools and ad systems), Data Transfers, internal backend exports', 'Connecting data sources (e.g. Google Tag Manager, Google Analytics, Braze, Insider, Facebook- and Google Adwords-APIs) with our data warehouse.', 'Designing and building healthy, scalable architecture for real-time and batch data processing.', 'Proactively coming up with suggestions on how the team can leverage new technologies and architectures to improve the data platform and implement new methodologies to improve the team performance.']\n",
      "requirements: ['Data processing in cloud environment (GCP preferred)', 'Capability of modeling and implementing cloud workflows and processes', 'Python', 'SQL', 'GIT', 'Ability to make autonomous decisions', 'Proactiveness in finding solutions', 'Fluent Polish and good command of English', 'Working from our Warsaw office 3 days a week']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-cloud-etl-warszawa-czerska-8-10,oferta,1003888935?apid=1cca2a4c-dd92-411b-aaf1-fc1b27670a37&oca=None&acv=0\n",
      "\n",
      "--- Job Record 407 ---\n",
      "id: 460\n",
      "url: https://www.pracuj.pl/praca/senior-material-supply-planning-expert-poznan-male-garbary-9,oferta,1003891640\n",
      "title: Senior Material Supply Planning Expert\n",
      "work_location: None\n",
      "validity: valid for 11 daysto 09 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Małe Garbary 9, Stare Miasto, PoznańPoznań, Greater Poland', 'contract of employment', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['Microsoft Excel', 'Microsoft Power BI']\n",
      "responsibilities: ['Leading reporting agenda: create reports and auxiliary tools allowing fast data collection and data aggregation at different granularity as input for capacity planning and supplier volume forecasting', 'Creating reports on material availability, and supplier performance as instructed (including high-level data accuracy checks)', 'Leading automation agenda (ie. macro developments) for existing tools, and processes. Identify new opportunities and improvements', 'Preparing, launching, and rolling out data visualization tools (eg. Power BI dashboards)', 'Generating reports for inventory monitoring (for inventory optimization) & obsolete management', 'Managing portfolio of Raw & Pack materials by using reporting tools in discussions with Procurement, Suppliers, and Local Sourcing Companies to ensure material availability in mid to long-term', 'Supporting potential short-term material supply issues with portfolio knowledge and proper coordination of respective teams', 'Participating in annual budget preparation by supervising material data workstream for material volume submission', 'Embedding continuous improvement on material availability by weekly service monitoring on materials, identifying root cause for shortage, and implementing mitigation actions']\n",
      "requirements: ['Has working experience with various data basis (2-4 years) in in-depth application knowledge (Excel, VBA, Power BI)', 'Is experienced in data and process automation (macros)', 'Has good analytical skills and is able to set priorities as well as perform in a multitasking workplace', 'Can communicate with local stakeholders about various planning topics, out of the scope of responsibility', 'Has good communication skills', 'Has proficiency in written and spoken English (min. B2 level)', 'Has a teamwork approach and enjoys working in a diverse, international environment']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-material-supply-planning-expert-poznan-male-garbary-9,oferta,1003891640?apid=04dcba2d-14b4-44fa-bd78-c4560966724e&oca=None&acv=0\n",
      "\n",
      "--- Job Record 408 ---\n",
      "id: 461\n",
      "url: https://www.pracuj.pl/praca/data-engineer-poznan-gnieznienska-32,oferta,1003864484\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: None\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Gnieźnieńska 32, Nowe Miasto, PoznańPoznań, Greater Poland', 'valid 5 hours', 'contract of employment']\n",
      "technologies: ['SQL', 'Azure', 'Python', 'NoSQL']\n",
      "responsibilities: ['Code advanced data processing solutions within Azure tech stack', 'Concept and implement relational and NoSQL database solutions within Azure', 'Development/code management using Azure DevOps', 'Manage our data lake which stores structured, semi-structured data and unstructured data']\n",
      "requirements: ['Completed studies of Business informatics or informatics or related ones', '4 years professional experience', 'Strong knowledge of SQL databases. NoSQL is a plus', 'Very good analytical and conceptual skills as well as high self-initiative', 'Experience with Azure tech stack and Python coding', 'An understanding of the different requirements that can arise in a moving data environment', 'Very good ability to work in functional and cross functional) teams', 'Fluent in written and spoken English. German is a plus', 'Passion for BI, Analytics and data', 'Open and interested in new technologies within Data & Analytics area']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-poznan-gnieznienska-32,oferta,1003864484?apid=8bbfb186-e439-4682-80ec-54291e60c1b0&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 409 ---\n",
      "id: 462\n",
      "url: https://www.pracuj.pl/praca/administrator-baz-danych-warszawa,oferta,1003888591\n",
      "title: Administrator Baz Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 10 dnido 08 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Microsoft SQL Server', 'T-SQL', 'Windows Server', 'Foglight', 'Jira', 'Zabbix', 'wewnątrz organizacji']\n",
      "responsibilities: ['Rozwój i utrzymanie rozwiązania w środowisku wysokiej dostępności z zachowaniem najwyższych standardów bezpieczeństwa', 'Współpraca z developerami i administratorami aplikacji wewnętrznych i zewnętrznych dostawców', 'Wsparcie i rozwój hurtowni danych', 'Utrzymywanie wysokiej dostępności usług SQL Serwer (SSIS, SSRS i inne)', 'Udział w innowacyjnych projektach z wykorzystaniem AI i technologii chmurowych']\n",
      "requirements: ['Posiadasz min. 4 lata doświadczenia na stanowisku Administrator Baz Danych MS SQL SERVER', 'Potrafisz tworzyć i analizować skrypty w językach T-SQL', 'Znasz obsługę platformy Windows Server 2016 lub nowszy w usłudze klastra', 'Masz podstawową wiedzę z obszaru SSIS i SSRS', 'Masz podstawową wiedzę z obszaru Sieci, Hardware, Acitive Directory', 'Nie przeraża Cię praca z dużymi wolumenami danych', 'Chcesz rozwijać swoje kompetencje z nowymi technologiami', 'Jesteś komunikatywny/-a i nastawiony/-a na długofalową współpracę', 'Znasz język angielski na poziomie minimum B2']\n",
      "application_link: https://www.pracuj.pl/aplikuj/administrator-baz-danych-warszawa,oferta,1003888591?apid=322503ab-9e3b-4669-a713-1225939b9ae9&oca=None&acv=0\n",
      "\n",
      "--- Job Record 410 ---\n",
      "id: 463\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-przetwarzania-danych-i-analiz-wsparcie-it-lublin-ledzian-86,oferta,1003888492\n",
      "title: Specjalista ds. przetwarzania danych i analiz - wsparcie IT\n",
      "work_location: None\n",
      "validity: ważna jeszcze 10 dnido 08 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), młodszy specjalista (Junior)\n",
      "work_arrangement: None\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Lędzian 86, LublinLublin, lubelskie', 'praca stacjonarna']\n",
      "technologies: ['SQL', 'PHP', 'Node.js', 'Pentaho Data Integration']\n",
      "responsibilities: ['przetwarzanie danych', 'tworzenie narzędzi do analizy danych', 'analizy funkcjonalne  i techniczne', 'rozwój hurtowni danych', 'tworzenie raportów']\n",
      "requirements: ['znajomość zagadnień bazodanowych', 'znajomość języka SQL', 'podstawowa znajomość PHP', 'podstawowa znajomość Node.js']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-przetwarzania-danych-i-analiz-wsparcie-it-lublin-ledzian-86,oferta,1003888492?apid=7b6dfcfc-922e-44ff-9584-1cddf4170508&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 411 ---\n",
      "id: 464\n",
      "url: https://www.pracuj.pl/praca/analityk-wroclaw-legnicka-48,oferta,1003888335\n",
      "title: Analityk\n",
      "work_location: None\n",
      "validity: ważna jeszcze 10 dnido 08 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Legnicka 48, Fabryczna, WrocławWrocław, dolnośląskie']\n",
      "technologies: ['SQL', 'Microsoft Excel', 'Qlik Sense', 'Microsoft Power BI', 'Python', 'wewnątrz organizacji']\n",
      "responsibilities: ['opracowywanie baz danych według przyjętych założeń,', 'przygotowywanie raportów na podstawie danych wieloźródłowych,', 'analizowanie potrzeb biznesowych dla zespołu medycznego;', 'przygotowywanie wizualizacji analiz danych,', 'testowanie i rozwijanie narzędzi Business Intelligence,', 'współpraca z Zespołem Medycznym i innymi działami.']\n",
      "requirements: ['wykształcenie wyższe z zakresu analizy danych, statystyki lub dziedzin pokrewnych,', 'doświadczenie na podobnym stanowisku min. 2 lata,', 'bardzo dobra organizacja pracy własnej, samodzielność i inicjatywa,', 'bardzo dobra znajomość pakietu MS Office, szczególnie Excela,', 'doświadczenie w pracy z VBA,', 'umiejętność posługiwania się językiem programowania SQL,', 'znajomość i stosowanie narzędzi Business Intelligence (Qlik Sense, Power BI),', 'komunikatywność w języku angielskim na poziomie min. B2.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/analityk-wroclaw-legnicka-48,oferta,1003888335?apid=940db285-0b6e-404f-9866-a90ce7194ceb&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 412 ---\n",
      "id: 465\n",
      "url: https://www.pracuj.pl/praca/database-developer-krakow,oferta,1003920419\n",
      "title: Database Developer\n",
      "work_location: None\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['KrakówKraków, Lesser Poland', 'contract of employment']\n",
      "technologies: ['C', 'C#', 'Python']\n",
      "responsibilities: ['Maintain and enhance the functionality of database structures (MS SQL) using SQL.', 'Create and revise software documentation to ensure clarity and usability.', 'Analyze production requirements and propose database-driven software solutions.', \"Adhere to and promote the company's good coding practices.\", 'Collaborate with the team to ensure high-quality deliverables and share knowledge with other developers.']\n",
      "requirements: ['To be successful in this role, you should have:', 'Extensive knowledge of database structure design, maintenance, and processing large datasets.', 'Experience in maintaining manufacturing databases, backed by at least 4 years of professional experience.', 'A strong understanding of the software development process.', 'A willingness to expand your coding skillset, follow guidance from experienced developers, and share your expertise with others.', 'Good communication skills and the ability to work collaboratively.', 'Proficiency in English at a communicative level.', 'A high level of self-organization and ability to prioritize tasks effectively.', 'Able to visit workplace 3 times a week.', 'Preferred Qualifications (Nice to Have):', 'Development skills in C/C#/Python.', 'Experience in web interface development.', 'Familiarity with Labview', 'Understanding of optical switching engineering.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/database-developer-krakow,oferta,1003920419?apid=71921638-27d3-4b0b-8dc2-d522d47f1f7a&oca=None&acv=0\n",
      "\n",
      "--- Job Record 413 ---\n",
      "id: 466\n",
      "url: https://www.pracuj.pl/praca/data-ai-advisor-warszawa-aleja-niepodleglosci-18,oferta,1003919857\n",
      "title: Data & AI Advisor\n",
      "work_location: None\n",
      "validity: ważna jeszcze 24 dnido 22 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior), ekspert\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Aleja Niepodległości 18, Mokotów, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Azure', 'AI', 'Data Platform']\n",
      "responsibilities: ['Budowanie i rozwijanie relacji z największymi klientami, pełniąc rolę ich zaufanego doradcy w obszarze Data & AI.', 'Identyfikowanie nowych możliwości biznesowych i poszerzanie zakresu świadczonych usług.', 'Tworzenie strategii i architektury rozwiązań Data & AI dostosowanych do potrzeb klientów.', 'Współpraca z zespołem sprzedażowym w realizacji celów przychodowych.']\n",
      "requirements: ['Doświadczenia w roli architekta Data & AI oraz doradcy technologicznego.', 'Umiejętności prowadzenia rozmów na poziomie strategicznym i technicznym z klientami.', 'Znajomości ekosystemu Microsoft (Azure, AI, Data Platform).', 'Nastawienia na rozwój biznesu i osiąganie celów sprzedażowych.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-ai-advisor-warszawa-aleja-niepodleglosci-18,oferta,1003919857?apid=d2db2718-6d4a-472f-8482-085f5d498975&oca=None&acv=0\n",
      "\n",
      "--- Job Record 414 ---\n",
      "id: 467\n",
      "url: https://www.pracuj.pl/praca/senior-azure-data-engineer-wroclaw,oferta,1003919815\n",
      "title: Senior Azure Data Engineer\n",
      "work_location: Company locationWrocławWrocław, Lower Silesia\n",
      "validity: valid for 24 daysto 22 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)']\n",
      "technologies: ['ETL', 'Oracle', 'Microsoft Azure', 'Databricks', 'Azure Data Factory', 'Python', 'PySpark', 'SQL', 'Azure Synapse Analytics', 'AI/ML', 'Scala', 'you focus on a single project at a time', 'you have influence on the choice of tools and technologies', 'you have influence on the technological solutions applied', 'agile', 'scrum']\n",
      "responsibilities: ['Projektowanie i implementacja rozwiązań ETL w chmurze Azure', 'Tworzenie i optymalizacja przepływów danych marketingowych', 'Integracja danych z różnych źródeł, w tym z bazy danych Oracle, do hurtowni danych na Azure', 'Wdrażanie i zarządzanie narzędziami do analizy danych (Azure Data Factory, Databricks)', 'Monitorowanie i optymalizacja procesów przetwarzania danych w czasie rzeczywistym', 'Zapewnienie wysokiej jakości danych, walidacja oraz implementacja rozwiązań raportowych', 'Bliska współpraca z zespołem analityków i architektów rozwiązań', 'Utrzymanie zgodności z najlepszymi praktykami w zakresie bezpieczeństwa danych oraz regulacjami prawnymi']\n",
      "requirements: ['Minimum 5-letnie doświadczenie na stanowisku Data Engineer', 'Doświadczenie w pracy z danymi oraz procesami ETL/ELT', 'Minimum 3-letnie praktyczne doświadczenie w pracy z chmurą Microsoft Azure (Azure Data Factory, Databricks, Azure Synapse Analytics)', 'Znajomość baz danych Oracle i SQL', 'Umiejętność programowania w Pythonie oraz PySpark', 'Doświadczenie w pracy z dużymi zbiorami danych i optymalizacją procesów przetwarzania', 'Znajomość zasad bezpieczeństwa danych', 'Umiejętność pracy w metodykach Agile/SCRUM', 'Doświadczenie w budowaniu modeli predykcyjnych oraz rozwiązań big data', 'Znajomość narzędzi CI/CD (Azure DevOps, YAML)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-azure-data-engineer-wroclaw,oferta,1003919815?apid=7f534a28-94d5-4da8-a91f-e90dd317b1b8&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 415 ---\n",
      "id: 468\n",
      "url: https://www.pracuj.pl/praca/senior-data-analyst-with-sql-warszawa,oferta,1003889178\n",
      "title: Senior Data Analyst with SQL\n",
      "work_location: None\n",
      "validity: valid for 10 daysto 08 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: []\n",
      "responsibilities: ['develop and maintain complex data models to identify market trends, growth opportunities, and enhance client experience through data-driven insights,', 'collect, analyze, and transform business data while ensuring accuracy, consistency, and adherence to data governance standards,', 'design interactive dashboards and visualizations to present key insights, performance metrics, and KPIs for leadership and stakeholders,', 'support the development of lead scoring and predictive models to identify high-value opportunities and optimize campaign strategies,', 'analyze client feedback and survey data to uncover trends, measure satisfaction, and provide actionable recommendations for improvement.']\n",
      "requirements: ['bachelor’s degree in Data Science, Statistics, Computer Science, or a related field,', 'at least 3 years of experience in complex data modeling and statistical analysis,', 'proficiency in SQL, Power BI, and/or Tableau for data reporting and visualization,', 'prior experience working in large, global organizations.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-analyst-with-sql-warszawa,oferta,1003889178?apid=a81e4d53-5a48-4c23-af85-472efefe7bc1&oca=None&acv=0\n",
      "\n",
      "--- Job Record 416 ---\n",
      "id: 469\n",
      "url: https://www.pracuj.pl/praca/analityk-danych-warszawa-czeska-22a,oferta,1003918552\n",
      "title: Analityk Danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 24 dnido 22 marca 2025\n",
      "contract_type: umowa o pracę, umowa zlecenie, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Czeska 22A, Praga-Południe, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL', 'Python', 'Git', 'MySQL', 'Django', 'Elasticsearch', 'JavaScript']\n",
      "responsibilities: ['Analizowanie danych oraz dbanie o ich jakość', 'Rozwiązywanie zgłoszeń od klientów systemu ABTShield: analiza danych w dashboardach oraz analiza surowych danych w Google BigQuery', 'Konfigurowanie oraz integrowanie usługi (przygotowywanie dashboardów w tym konfiguracja kont klientów, generowanie kodów integracyjnych)', 'Wsparcie w rozwoju oprogramowania i rozwiązywaniu problemów ABTShield', 'Współpraca z innymi zespołami w celu zrozumienia potrzeb raportowych oraz zapewnienia spójności i integralności danych', 'Udział w projektowaniu i implementacji nowych rozwiązań informatycznych opartych na danych']\n",
      "requirements: ['Co najmniej 2-letnie doświadczenie w analizie danych liczbowych: tworzenie zestawień / agregacji, przetwarzanie dużych zbiorów (np. w pandas), filtrowanie danych, interpretacja danych', 'Umiejętność tworzenia raportów i wizualizacji danych w Google BigQuery', 'Dobra znajomość języka SQL i zdolność wykorzystania go przy analizie danych', 'Znajomość podstaw działania protokołu HTTP oraz javascript', 'Umiejętność logicznego myślenia, formułowania i weryfikacji hipotez', 'Proaktywna postawa w identyfikacji oraz implementacji rozwiązań']\n",
      "application_link: https://www.pracuj.pl/aplikuj/analityk-danych-warszawa-czeska-22a,oferta,1003918552?apid=76663462-e15b-432f-86bb-5928f277cb0a&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 417 ---\n",
      "id: 470\n",
      "url: https://www.pracuj.pl/praca/administrator-baz-danych-wrzesnia,oferta,1003918414\n",
      "title: Administrator baz danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 24 dnido 22 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['WrześniaWrześnia, wielkopolskie', 'praca stacjonarna']\n",
      "technologies: ['PostgreSQL', 'MSSQL', 'SQL', 'Bash', 'Python', 'C#']\n",
      "responsibilities: ['Obsługa zgłoszenia IT helpdesk', 'Zapewnienie wsparcia IT użytkownikom w zakresie problemów związanych ze sprzętem i oprogramowaniem', 'Organizowanie zewnętrznego wsparcia technicznego w przypadku, gdy problemy nie mogą być rozwiązane wewnętrznie', 'Zapewnienie działania obecnych systemów wdrożonych w firmie ERP, RCP, systemów produkcyjnych', 'Prowadzenie dokumentacji związanej z IT', 'Wsparcie, rozwój istniejącego systemu ERP']\n",
      "requirements: ['Wykształcenie kierunkowe: Informatyka, Elektronika, Telekomunikacja lub pokrewne', 'Doświadczenie w pracy w roli administratora baz danych, bardzo dobra znajomość SQL', 'Umiejętność administracji bazami danych PostgreSQL i MSSQL', 'Znajomość programowania w SQL, Bash, Python, C#', 'Znajomość zagadnień związanych z siecią komputerową', 'Reagowanie na awarie oprogramowania', 'Znajomość języka angielskiego na poziomie komunikatywnym', 'Umiejętności komunikacyjne, interpersonalne i analityczne', 'Umiejętności rozwiązywania problemów technicznych', 'Czynne prawo jazdy kat.B']\n",
      "application_link: https://www.pracuj.pl/aplikuj/administrator-baz-danych-wrzesnia,oferta,1003918414?apid=d8fb7b13-262e-443a-98ca-79e8bd3df574&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 418 ---\n",
      "id: 471\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-optymalizacji-procesow-lodz-boleslawa-limanowskiego-179,oferta,1003903290\n",
      "title: Specjalista ds. optymalizacji procesów\n",
      "work_location: None\n",
      "validity: ważna jeszcze 16 dnido 14 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Bolesława Limanowskiego 179, Bałuty, ŁódźŁódź, łódzkie', 'praca stacjonarna']\n",
      "technologies: ['BPMN', 'DMN', 'RPA']\n",
      "responsibilities: ['tworzenie i wprowadzanie nowych procesów cyfrowych oraz optymalizacja istniejących procesów w odniesieniu do kompleksowego krajobrazu procesów,', 'tworzenie modeli procesów biznesowych z wykorzystaniem BPMN,', 'bieżące monitorowanie wdrożonych procesów,', 'monitorowanie prawidłowego działania i optymalizacji wszystkich rozwiązań automatyzacji (RPA: Robotic Process Automation),', 'przeprowadzanie analiz danych w celu optymalizacji procesów,', 'monitorowanie jakości usług, rozpoznawanie problemów, raportowanie poziomu usług,', 'opracowywanie i weryfikacja raportów,']\n",
      "requirements: ['wykształcenie wyższe', 'dobra znajomość języka niemieckiego lub angielskiego', 'doświadczenie w analizowaniu i dalszym rozwijaniu procesów, procedur i struktur', 'wiedza metodyczna (moderacja, Agile, BPMN, DMN, RPA itp.)', 'znajomość programów SAP i MS Office, w szczególności MS Excel', 'wysokie zdolności analityczne']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-optymalizacji-procesow-lodz-boleslawa-limanowskiego-179,oferta,1003903290?apid=152412f7-0e64-4aaf-8f87-3bb5cf639d0f&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 419 ---\n",
      "id: 472\n",
      "url: https://www.pracuj.pl/praca/senior-data-quality-analyst-szczecin-jerzego-janosika-17,oferta,1003884673\n",
      "title: Senior Data Quality Analyst\n",
      "work_location: None\n",
      "validity: ważna jeszcze 8 dnido 06 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Jerzego Janosika 17, SzczecinSzczecin, zachodniopomorskie']\n",
      "technologies: ['SQL', 'Tableau']\n",
      "responsibilities: ['Zapewniasz i monitorujesz jakość danych', 'Tworzysz oraz zarządzasz dashboardami i raportami w Tableau', 'Definiujesz cele zarządzania danymi i monitorujesz ich realizację', 'Wdrażasz wewnętrzne wytyczne dotyczące zarządzania danymi', 'Zarządzasz masterdata i metadanymi', 'Współpracujesz z zespołem inżynierów danych, rozwiązując problemy związane z danymi oraz opracowując nowe ramy poprawy ich jakości', 'Edukujesz interesariuszy oraz zespoły wewnętrzne w zakresie Data Governance', 'Rekomendujesz narzędzia i rozwiązania wspierające jakość danych oraz decyzje biznesowe oparte na danych', 'Ściśle współpracujesz z zespołami procesowymi, aby dostosować dane do procesów biznesowych', 'Analizujesz i raportujesz jakość danych oraz śledzisz i rozwiązujesz problemy z dashboardami']\n",
      "requirements: ['Posiadasz wykształcenie wyższe w dziedzinie informatyki, ekonomii, business intelligence lub pokrewnej', 'Znasz język angielski na poziomie zaawansowanym', 'Masz minimum 3-letnie doświadczenie zawodowe w analizie danych lub business intelligence, w tym co najmniej rok doświadczenia w zarządzaniu jakością danych', 'Pracowałeś/aś z narzędziami takimi jak SQL, Google BigQuery, Python, Jenkins, dbt, Tableau (lub podobnym stosie technologicznym)', 'Efektywnie komunikujesz się zarówno z zespołem, jak i interesariuszami', 'Cechuje Cię analityczne myślenie, samodzielność oraz umiejętność pracy w zespole']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-quality-analyst-szczecin-jerzego-janosika-17,oferta,1003884673?apid=46f3e7c9-bd9c-4a63-a245-ebb97bf2dfcb&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 420 ---\n",
      "id: 473\n",
      "url: https://www.pracuj.pl/praca/senior-data-analyst-szczecin-jerzego-janosika-17,oferta,1003884672\n",
      "title: Senior Data Analyst\n",
      "work_location: None\n",
      "validity: ważna jeszcze 8 dnido 06 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Jerzego Janosika 17, SzczecinSzczecin, zachodniopomorskie']\n",
      "technologies: ['SQL']\n",
      "responsibilities: ['Tworzysz wizualizacje danych, opracowując dashboardy i raporty w Tableau', 'Współpracujesz z interesariuszami w celu zebrania ich wymagań, potrzeb i oczekiwań dotyczących dashboardów', 'Zbierasz, czyścisz i analizujesz dane biznesowe', 'Współpracujesz z inżynierami danych, aby przygotować modele danych', 'Jesteś odpowiedzialny/a za identyfikowanie wzorców w danych oraz rekomendowanie obszarów do usprawnień biznesowych', 'Prezentujesz wyniki swojej pracy i przygotowujesz rekomendacje na podstawie analizy', 'Dokumentujesz logikę biznesową i rozwiązania frontendowe', 'Śledzisz i rozwiązujesz problemy z dashboardami, dbając o ich poprawne działanie', 'Szkolisz innych w zakresie narzędzi wizualizacyjnych i analizy danych']\n",
      "requirements: ['Posiadasz wykształcenie wyższe z informatyki, analizy biznesowej, ekonomii lub pokrewnej dziedziny', 'Posługujesz się językiem angielskim w stopniu umożliwiającym swobodną komunikację', 'Masz minimum 2 lata doświadczenia zawodowego w obszarze Business Intelligence lub analizy danych', 'Znasz narzędzia i technologie takie jak: Google BigQuery, SQL, dbt, Tableau,', 'Wykazujesz umiejętność skutecznej komunikacji z interesariuszami na różnych poziomach organizacji.', 'Posiadasz silne zdolności analityczne i umiejętność rozwiązywania problemów.', 'Jesteś samodzielny, zmotywowany i potrafisz podejmować inicjatywę']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-analyst-szczecin-jerzego-janosika-17,oferta,1003884672?apid=79876f4d-a7c7-4952-a823-2ed947edb739&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 421 ---\n",
      "id: 474\n",
      "url: https://www.pracuj.pl/praca/fullstack-data-scientist-szczecin-jerzego-janosika-17,oferta,1003884669\n",
      "title: Fullstack Data Scientist\n",
      "work_location: None\n",
      "validity: ważna jeszcze 8 dnido 06 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Jerzego Janosika 17, SzczecinSzczecin, zachodniopomorskie']\n",
      "technologies: ['Python', 'SQL', 'Tableau', 'AWS']\n",
      "responsibilities: ['Zbieranie i Integracja Danych: Oceniasz źródła danych do integracji, wdrażasz mechanizmy zbierania danych z uwzględnieniem ich integralności i zarządzania', 'Zarządzanie Przechowywaniem Danych: Projektujesz rozwiązania magazynowania opartych na potrzebach wydajności i wolumenu danych, wdrażasz mechanizmy kompresji, partycjonowania i cache’owania', 'Przetwarzanie Danych: Oczyszczasz i przekształcasz surowe dane, normalizujesz struktury danych oraz budujesz procesy ETL', 'Infrastruktura i Przepływy Danych: Tworzysz przepływy danych oraz utrzymujesz infrastruktury dla skalowalnych i wydajnych operacji', 'Potoki danych i infrastruktura: Projektujesz i optymalizujesz potoki danych oraz utrzymujesz skalowaną i wydajną infrastrukturę.', 'Współpraca i Komunikacja: Współpracujesz z analitykami oraz umiejętnie tłumaczysz pojęcia techniczne dla osób nietechnicznych', 'Wybór technologii i metod: Dobierasz algorytmy, narzędzia oraz infrastruktury odpowiednie dla wymagań projektowych oraz podejmujesz decyzję w zakresie inżynierii funkcji, optymalizacji modeli i monitoringu']\n",
      "requirements: ['Ukończyłeś/aś studia w dziedzinie IT, ekonomii lub podobnych', 'Posiadasz minimum 2 letnie doświadczenie w analizie danych i bardzo dobrą znajomość języka angielskiego', 'Masz doświadczenie z narzędziami: Python, SQL, Google BigQuery, dbt, Tableau, Jenkins', 'Znasz rozwiązania chmurowe (GCP, AWS) oraz posiadasz doświadczenie z modelami uczenia maszynowego, hurtowniami danych, ETL', 'Znasz i realizujesz cele organizacji przez dostosowanie rozwiązań Data Science i inżynierii danych', 'Dążysz do rozwoju i dzielenia się wiedzą z zespołem']\n",
      "application_link: https://www.pracuj.pl/aplikuj/fullstack-data-scientist-szczecin-jerzego-janosika-17,oferta,1003884669?apid=cb1707ef-e0b9-4dec-a59f-f63fb0cedea9&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 422 ---\n",
      "id: 475\n",
      "url: https://www.pracuj.pl/praca/llm-junior-developer-warszawa,oferta,1003917214\n",
      "title: LLM Junior Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 23 dnido 21 marca 2025\n",
      "contract_type: umowa zlecenie\n",
      "employment_type: pełny etat, część etatu\n",
      "position: młodszy specjalista (Junior)\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['RAG', 'Multimodal LLM', 'Data Engineering', 'Semantic Kernel', 'Python']\n",
      "responsibilities: ['Współtworzenie MVP oraz post-MVP aplikacji opartej o system .NET, Semantic Kernel oraz LLMs.', 'Współpraca przy tworzeniu i testowaniu nowych rozwiązań dla Silnika AI.', 'Udział w analizie wymagań biznesowych i spotkaniach projektowych.', 'Pomoc w przeprowadzaniu testów oraz wdrażaniu nowych funkcjonalności.', 'Prowadzenie dokumentacji technicznej projektów.']\n",
      "requirements: ['Status studenta studiów na kierunkach technicznych (preferowana informatyka lub pokrewne).', 'Podstawowa znajomość LLM i chęć dalszego rozwoju w tym kierunku.', 'Doświadczenie w projektach IT, hackatonach czy działalność w kole naukowym']\n",
      "application_link: https://www.pracuj.pl/aplikuj/llm-junior-developer-warszawa,oferta,1003917214?apid=4a96fb9e-e50e-49bd-a823-11b97e413302&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 423 ---\n",
      "id: 476\n",
      "url: https://www.pracuj.pl/praca/senior-master-data-steward-warszawa,oferta,1003886933\n",
      "title: Senior Master Data Steward\n",
      "work_location: None\n",
      "validity: valid for 9 daysto 07 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: []\n",
      "responsibilities: ['manage day-to-day operations and data quality control activities,', 'ensure Master Data adherence to definitions, standards, and processes,', 'monitor, report, update, and maintain Master Data for assigned domains,', 'define and implement data quality standards in alignment with Data Owner’s guidelines,', 'conduct periodic data quality reviews to ensure compliance with established standards,', 'provide reports to the Data Owner and MDM Manager based on predefined intervals or ad hoc requests,', 'support organizational and system alignment, data standardization, and ERP projects,', 'assist the Data Owner in data quality improvement initiatives across regions,', 'support Data Consumers with inquiries and requests related to Master Data,', 'engage in change requests impacting data categories,', 'define data definitions, processes, KPIs, and business rules for Data Objects,', 'proactively manage issues and implement solutions in coordination with the Data Owner,', 'manage data consumption requests on the Data Marketplace,', 'ensure Master Data adheres to internal and external standards,', 'provide timely follow-ups on data quality issues (measured monthly/quarterly).']\n",
      "requirements: ['fluency in English (C1/C2 level) – advanced language skills essential for communication with global stakeholders and project teams,', 'demonstrated expertise in managing Master Data processes, including process building, optimization, and addressing gaps,', 'knowledge of data governance, data quality, and lifecycle management,', 'understanding vendor management processes and working effectively with vendors,', 'ability to manage relationships with stakeholders, including procurement teams, and collaborate with cross-functional teams,', 'comfortable with data analysis and identifying areas for process improvement,', 'experience in managing complex projects and delivering results within deadlines,', 'strong presentation and communication skills,', 'flexibility and travel – occasional travel (once a month to the factory in Opole, Poland) and occasional visits to the office in Warsaw (2-3 times per year), with 100% remote work otherwise.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-master-data-steward-warszawa,oferta,1003886933?apid=d483a72d-b287-4d24-9adc-2de4254a93d6&oca=None&acv=0\n",
      "\n",
      "--- Job Record 424 ---\n",
      "id: 477\n",
      "url: https://www.pracuj.pl/praca/data-scientist-warszawa-domaniewska-32,oferta,1003881920\n",
      "title: Data Scientist\n",
      "work_location: None\n",
      "validity: valid for 7 daysto 05 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Domaniewska 32, Mokotów, WarszawaWarszawa, Masovian', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['Python', 'R', 'SQL', 'Git']\n",
      "responsibilities: ['Actively participate in each stage of the data science lifecycle, from business understanding through model development to industrialization', 'Design, build and maintain statistical/machine learning models to optimize business operations', 'Explore and experiment with innovative ML techniques and approaches to maximize value generated by the team', 'Be a thought leader in development of methodology within big scale transformational projects', 'Communicate model & analysis results to a non-technical audience that includes stakeholders and senior management, and win their buy-in', 'Coach and mentor other team members on multiple aspects of a data scientist role']\n",
      "requirements: ['2+ years of professional experience building and industrializing machine learning solutions', 'Proficiency in at least one language employed in ML & statistical analysis (Python or R preferred) and its use with version control (git) and testing', 'Practical knowledge of SQL', 'Experience in application of ML concepts and methodologies (regression and classification, time series modeling, feature engineering and selection, regularization etc.)', 'Solid knowledge of ML techniques and algorithms (incl. various regression types, ensembles, clustering, decision trees, boosting, etc.) and their pros and cons', 'Ability to reframe business challenges into data science problems and adjust ML methods & tools accordingly', 'Good communication and presentation skills, in particular facilitation of grasping complex topics by broad audiences, incl. non-technical stakeholders', 'Track record of effective collaboration with IT / data engineering teams in development and production phase']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-warszawa-domaniewska-32,oferta,1003881920?apid=0a6f5072-dbce-4114-8c48-7cfe29ce6907&oca=None&acv=0\n",
      "\n",
      "--- Job Record 425 ---\n",
      "id: 478\n",
      "url: https://www.pracuj.pl/praca/ml-ops-engineer-warszawa-domaniewska-32,oferta,1003881879\n",
      "title: ML Ops Engineer\n",
      "work_location: None\n",
      "validity: valid for 7 daysto 05 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: home office work, hybrid work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Domaniewska 32, Mokotów, WarszawaWarszawa, Masovian', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: ['SQL', 'Python', 'Git', 'Microsoft Azure', 'R']\n",
      "responsibilities: ['Optimize, standardize and implement data science and machine learning solutions at scale and in cloud-based environments (Azure)', 'Participate in the end-to-end lifecycle of data science projects through the use of DevOps, code, experiment and model management, CI/CD and further industry best practices', 'Write well-designed, testable, efficient code', 'Work closely with engineering to continuously improve the way we consume data and deploy models in production', 'Design and lead on monitoring, troubleshooting, debugging and incident management for our ML pipelines', 'Be a trusted advisor and evangelist to the team and stakeholders on various aspects of ML Ops, from scaling and throughput, to infrastructure and deployment strategies']\n",
      "requirements: ['2+ years of professional experience in ML Ops or ML engineering, particularly in productionizing and scaling ML models', 'Broad familiarity with Azure cloud environment and Databricks, including its setup and maintenance as an ML platform', 'Advanced proficiency with Python and SQL in version control systems (git) plus their use in building ML & data pipelines', 'Proven experience with software development best practices including testing, continuous integration, and DevOps tools', 'Good understanding of data science lifecycle and the way data scientists work to deliver value', 'Familiarity with agile software development lifecycle (SCRUM, Kanban, etc.)', 'Attention to clarity of code, ease of development, and correctness of implementations', 'Business-ready command of English, written and spoken']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ml-ops-engineer-warszawa-domaniewska-32,oferta,1003881879?apid=ee19d52b-f1ef-4d08-9692-54693c8cfc20&oca=None&acv=0\n",
      "\n",
      "--- Job Record 426 ---\n",
      "id: 479\n",
      "url: https://www.pracuj.pl/praca/golang-developer-warszawa-bakalarska-34,oferta,1003884709\n",
      "title: Golang Developer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 8 dnido 06 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular), starszy specjalista (Senior)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Bakalarska 34, Włochy, WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatów']\n",
      "technologies: ['Go', 'SQL', 'Redis', 'RabbitMQ', 'Docker', 'REST', 'GraphQL', 'AWS', 'Kubernetes', 'JavaScript', 'TypeScript', 'Node.js', 'gRPC', 'React.js', 'Next.js', 'wewnątrz organizacji', 'u klienta', 'koncentrujesz się na rozwoju produktu', 'agile', 'scrum', 'backend developer', 'fullstack developer', 'lider techniczny', 'architekt', 'devOps', 'programista testów automatycznych', 'tester manualny', 'project manager', 'analityk systemowy']\n",
      "responsibilities: ['Projektowanie oraz implementacja oprogramowania Golang', 'Dbałość o jakość kodu i zgodność z obowiązującymi standardami', 'Wsparcie przy analizie i naprawie błędów produkcyjnych', 'Przygotowywanie i aktualizacja dokumentacji technicznej']\n",
      "requirements: ['Golang - minimum 2 lata doświadczenia', 'Znajomość baz danych (mongodb, mysql)', 'Redis/Memcached/Varnish', 'RabbitMQ', 'Znajomości API RESTful oraz GraphQL', 'Znajomości protokołu HTTP/2', 'Docker', 'CI/CD (mile widziany Github Actions)', 'AWS Cloud (Lambda, DynamoDB, ELB, Cloudfront, S3)', 'K8S/EKS', 'Umiejętności pisania testów jednostkowych oraz integracyjnych', 'Znajomości zagadnień optymalizacji', 'Język angielski na poziomie umożliwiającym korzystanie z dokumentacj']\n",
      "application_link: https://www.pracuj.pl/aplikuj/golang-developer-warszawa-bakalarska-34,oferta,1003884709?apid=ee207fb4-1c35-4540-b582-b5f907107ec8&oca=None&acv=0\n",
      "\n",
      "--- Job Record 427 ---\n",
      "id: 480\n",
      "url: https://www.pracuj.pl/praca/analityk-danych-warszawa-rownolegla-4a,oferta,1003884639\n",
      "title: Analityk danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze 8 dnido 06 marca 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['Równoległa 4A, Włochy, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Microsoft Power BI']\n",
      "responsibilities: ['Kompleksowa obsługa projektów pod względem analitycznym, raportowym i systemowym.', 'Przygotowywanie raportów biznesowych dla Klientów oraz na potrzeby wewnętrzne.', 'Przygotowywanie oraz rozwój narzędzi analitycznych.', 'Monitorowanie wyników oraz przygotowywanie na ich podstawie wniosków i rekomendacji.', 'Przygotowywanie prezentacji i wizualizacji danych.', 'Spotkania z klientami wewnętrznymi i zewnętrznymi w celu wypracowania koncepcji dla raportowania.']\n",
      "requirements: ['Zaawansowana i praktyczna znajomość MS Excel - warunek konieczny.', 'Znajomość Power BI i Power Query.', 'Bardzo dobra znajomość języka angielskiego min. B2.', 'Zdolność do wyciągania wniosków na podstawie analizy danych oraz rekomendowania rozwiązań biznesowych.', 'Komunikatywność, samodzielność i zaangażowanie.', 'Umiejętność rozwiązywania problemów, elastyczność oraz dążenie do inicjatywy.', 'Otwartość na zmiany i chęć dostosowania się do dynamicznego środowiska.', 'Świadomość zagadnień księgowych i finansowych oraz ich wpływu na analizę danych i procesy biznesowe.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/analityk-danych-warszawa-rownolegla-4a,oferta,1003884639?apid=e90cb18e-6cb3-4bdb-95b5-ec921151a090&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 428 ---\n",
      "id: 481\n",
      "url: https://www.pracuj.pl/praca/dyrektor-ds-rozwoju-produkty-big-data-warszawa,oferta,1003881499\n",
      "title: Dyrektor ds. rozwoju (produkty Big Data)\n",
      "work_location: None\n",
      "validity: ważna jeszcze 7 dnido 05 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: None\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'dyrektor']\n",
      "technologies: ['Agile', 'Big Data']\n",
      "responsibilities: ['Opracowywanie produktów i usług na podstawie analizy danych i potrzeb rynku', 'Odpowiedzialność za strategię rozwoju działu, P&L, wyniki sprzedażowe i realizację celów', 'Definiowanie, organizowanie i nadzorowanie pracy zespołu', 'Prowadzenie projektów zgodnie z metodyką Agile', 'Przekładanie strategii organizacji na konkretne plany biznesowe i roadmapę rozwoju produktów/usług', 'Współpraca przy projektach związanych z ETL, analityką i dużymi zbiorami danych', 'Koordynacja współpracy między działami oraz z partnerami zewnętrznymi', 'Bieżąca współpraca i raportowanie do zarządu firmy']\n",
      "requirements: ['Kilkuletnie doświadczenie na analogicznym stanowisku w branży finansowej/ IT', 'Doświadczenie w zarządzaniu dużymi projektami- umiejętność radzenia sobie z wyzwaniami biznesowymi i technologicznymi', 'Rozumienie narzędzi ETL, przetwarzania danych oraz optymalizacji procesów', 'Umiejętność strategicznego myślenia i definiowania długoterminowych celów', 'Zdolności przywódcze, zarządzanie zespołem i skuteczna współpraca międzydziałowa', 'Znajomość metodyk Agile i doświadczenie w pracy z zespołami deweloperskimi', 'Doświadczenie w sprzedaży oraz komercjalizacji produktów bazujących na analizie i przetwarzaniu danych', 'Umiejętnością identyfikowania potrzeb klientów oraz dopasowywania rozwiązań do ich oczekiwań biznesowych', 'Znajomość j. angielskiego na poziomie zaawansowanym']\n",
      "application_link: https://www.pracuj.pl/aplikuj/dyrektor-ds-rozwoju-produkty-big-data-warszawa,oferta,1003881499?apid=8103ec37-50f8-4fbd-aaa4-d873a5b86586&oca=None&acv=0\n",
      "\n",
      "--- Job Record 429 ---\n",
      "id: 482\n",
      "url: https://www.pracuj.pl/praca/inzynier-danych-azure-databricks-wroclaw,oferta,1003883875\n",
      "title: Inżynier Danych (Azure Databricks)\n",
      "work_location: None\n",
      "validity: ważna jeszcze 8 dnido 06 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['WrocławWrocław, dolnośląskie']\n",
      "technologies: ['Python', 'SQL']\n",
      "responsibilities: ['Projektowanie, implementacja i optymalizacja potoków przetwarzania danych w Azure Databricks i Microsoft Fabric', 'Tworzenie i zarządzanie infrastrukturą danych w środowisku chmurowym Microsoft Azure', 'Integracja danych z różnych źródeł oraz ich przetwarzanie i analiza', 'Automatyzacja procesów ETL/ELT', 'Czyszczenie, transformacja i wzbogacanie danych w celu zapewnienia ich jakości', 'Współpraca z Zespołem Analiz Biznesowych i Działem IT w zakresie źródeł i budowy modeli danych', 'Monitorowanie i zapewnianie bezpieczeństwa danych oraz ich zgodności z regulacjami']\n",
      "requirements: ['Minimum 3 lata doświadczenia na stanowisku Inżyniera Danych', 'Praktyczna znajomość Azure Databricks/MS Fabric', 'Dobra znajomość języka Python oraz SQL', 'Doświadczenie w implementacji procesów ETL/ELT', 'Zdolności analityczne oraz umiejętność rozwiązywania problemów', 'Znajomość zasad zarządzania danymi oraz dobrych praktyk w zakresie optymalizacji zapytań']\n",
      "application_link: https://www.pracuj.pl/aplikuj/inzynier-danych-azure-databricks-wroclaw,oferta,1003883875?apid=4ef5181e-e091-4866-950d-9ef4b5b38c3c&oca=None&acv=0\n",
      "\n",
      "--- Job Record 430 ---\n",
      "id: 483\n",
      "url: https://www.pracuj.pl/praca/programista-programistka-c%2b%2b-qt-warszawa-modularna-11,oferta,1003869957\n",
      "title: Programista / Programistka C++/QT\n",
      "work_location: None\n",
      "validity: ważna jeszcze 2 dnido 28 lutego 2025\n",
      "contract_type: umowa o pracę, umowa o dzieło, umowa zlecenie, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca zdalna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Modularna 11, Włochy, WarszawaWarszawa, mazowieckie', 'Szukamy wielu kandydatów']\n",
      "technologies: ['C++', 'qt', 'aplikacje mobilne']\n",
      "responsibilities: ['Rozwój oprogramowania systemów monitorowania obiektów,', 'Testowanie tworzonego oprogramowania,', 'Tworzenie dokumentacji technicznej.']\n",
      "requirements: ['Wykształcenie wyższe techniczne,', 'Biegła praktyczna znajomość języka C++ z wykorzystaniem biblioteki Qt,', 'Dobra znajomość środowiska Qt Creator,', 'Znajomość środowiska Linux,', 'Znajomość języka angielskiego pozwalająca na czytanie dokumentacji technicznej,']\n",
      "application_link: https://www.pracuj.pl/aplikuj/programista-programistka-c%2b%2b-qt-warszawa-modularna-11,oferta,1003869957?apid=6921a4f0-03ee-4097-9ce9-dc03ffc171b1&oca=None&acv=0\n",
      "\n",
      "--- Job Record 431 ---\n",
      "id: 484\n",
      "url: https://www.pracuj.pl/praca/cloud-data-analyst-wroclaw-plac-powstancow-slaskich-1,oferta,1003880715\n",
      "title: Cloud Data Analyst\n",
      "work_location: None\n",
      "validity: ważna jeszcze 7 dnido 05 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca zdalna\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['plac Powstańców Śląskich 1, Krzyki, WrocławWrocław, dolnośląskie']\n",
      "technologies: ['SQL', 'Python', 'Tableau']\n",
      "responsibilities: ['Obróbka, transformacja i konsolidacja danych na potrzeby raportów', 'Tworzenie funkcji prognostycznych w oparciu o czynniki wewnętrzne i zewnętrzne', 'Modelowanie danych o ruchu, konwersjach i interakcjach użytkowników', 'Wizualizacja KPI i ich projekcji w Tableau', 'Wykrywanie braków, niespójności i anomalii w danych', 'Tworzenie raportów i wyciąganie danych na potrzeby innych działów', 'Współpraca z pozostałymi obszarami zespołu Danych i Analiz']\n",
      "requirements: ['Doświadczenie w pracy z chmurowymi hurtowniami danych (najlepiej GCP)', 'Umiejętność samodzielnego projektowania procesów ETL', 'Znajomość SQL i Python w stopniu zaawansowanym', 'Umiejętność logicznego, analitycznego myślenia', 'Wiedza z zakresu statystyki i modeli matematycznych', 'Znajomość narzędzi wizualizacji danych, najlepiej Tableau', 'Dbałość o szczegóły i uporządkowanie efektów swojej pracy', 'Samodzielność i odpowiedzialność']\n",
      "application_link: https://www.pracuj.pl/aplikuj/cloud-data-analyst-wroclaw-plac-powstancow-slaskich-1,oferta,1003880715?apid=9e865eab-83c1-4157-9895-64fbfaa461f4&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 432 ---\n",
      "id: 485\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-warszawa-prosta-67,oferta,1003880635\n",
      "title: Senior Data Engineer\n",
      "work_location: Company locationProsta 67, Włochy, WarszawaWarszawa, Masovian\n",
      "validity: valid for 7 daysto 05 March 2025\n",
      "contract_type: contract of employment, B2B contract\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: Immediate employment\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Work locationwhole Poland (remote work)', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['SQL', 'BigQuery', 'Dataflow', 'Pub/Sub', 'Google Cloud Platform', 'GitLab', 'in house', 'you develop several projects simultaneously', 'you have influence on the technological solutions applied', 'you have influence on the product']\n",
      "responsibilities: ['5+ years of experience in backend and data engineering, with strong system design skills.', 'Practical proficiency in cloud technologies (ideally GCP), with expertise in tools like BigQuery, Dataflow, Pub/Sub, or similar.', 'Hands-on experience with CI/CD tools (e.g., GitLab CI) and infrastructure as code.', 'A strong focus on data quality, governance, and building scalable, automated workflows.', 'Experience designing self-service data platforms and infrastructure.', 'Proven ability to mentor and support others, fostering data literacy across teams.']\n",
      "requirements: ['A strong understanding of schema designs and dimensional data modelling.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-warszawa-prosta-67,oferta,1003880635?apid=13ec6fca-90ec-4177-9b8f-4f8466d921a4&oca=None&acv=0\n",
      "\n",
      "--- Job Record 433 ---\n",
      "id: 486\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-rozwoju-i-wdrazania-sztucznej-inteligencji-poznan-walbrzyska-13,oferta,1003879079\n",
      "title: Specjalista ds. Rozwoju i Wdrażania Sztucznej Inteligencji\n",
      "work_location: None\n",
      "validity: ważna jeszcze 6 dnido 04 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Wałbrzyska 13, Grunwald, PoznańPoznań, wielkopolskie', 'praca stacjonarna', 'Запрошуємо працівників з України']\n",
      "technologies: ['Azure Cloud Platform', 'Google Cloud Platform', 'Tableau', 'Microsoft Power BI']\n",
      "responsibilities: ['Zarządzanie projektami opartymi o technologię AI - w zakresie audytowania potrzeb przedsiębiorstwa, planowania, wdrażania w środowisku biznesowym', 'Przeprowadzenie audytu technologii obecnych na rynku, wydanie rekomendacji dotyczących rozwiązań AI w celu wzrostu konkurencyjności firmy i adaptacji rozwiązań na potrzeby firmy.', 'Wybór oraz wdrożenie technologii AI dla wsparcia wewnętrznych procesów analitycznych', 'Wybór i wdrożenie rozwiązań AI dla Chat Bots dla udoskonalenia dystrybucji wiedzy w firmie', 'Przedstawianie rekomendacji dotyczących integracji sztucznej inteligencji w celu wsparcia rozwoju produktów i wzrostu konkurencyjności przedsiębiorstwa.', 'Identyfikowanie potrzeb wewnętrznych firmy oraz dopasowanie rozwiązań AI do wymagań biznesowych.', 'Wdrażanie automatyzacji procesów w organizacji dostępnymi narzędziami i technologiami rynkowymi.']\n",
      "requirements: ['Doświadczenie zawodowe:  2 lata doświadczenia w zarządzaniu projektami technologicznymi (mile widziane doświadczenie w AI, ML lub Big Data) oraz udokumentowane wdrożenia projektów opartych o sztuczną inteligencję', 'Znajomość technologii AI: Bardzo dobra wiedza na temat sztucznej inteligencji i jej zastosowania w środowisku biznesowym oraz jednej z platform Azure Cloud Platform, AWS, Google Cloud', 'Doświadczenie w zarządzaniu projektami: Wiedza z zakresu metod zarządzania Agile/Scrum', 'Umiejętności analityczno - biznesowe: Zdolność szybkiego opanowywania i zrozumienia potrzeb biznesowych', 'Komunikacja: Umiejętność efektywnej komunikacji ze współpracownikami, podwykonawcami oraz klientami biznesowymi']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-rozwoju-i-wdrazania-sztucznej-inteligencji-poznan-walbrzyska-13,oferta,1003879079?apid=18290922-804d-4596-8f4c-3eec16f6fa2f&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 434 ---\n",
      "id: 487\n",
      "url: https://www.pracuj.pl/praca/data-engineer-business-intelligence-lodz-aleksandrowska-67,oferta,1003877003\n",
      "title: Data Engineer - Business Intelligence\n",
      "work_location: None\n",
      "validity: valid for 4 daysto 02 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Aleksandrowska 67, Bałuty, ŁódźŁódź, Łódź', 'contract of employment', 'full office work']\n",
      "technologies: ['Microsoft Power BI', 'SQL', 'SSIS', 'MS Azure Synapse', 'ADF', 'Azure DevOps', 'Agile', 'Scrum', 'agile', 'scrum']\n",
      "responsibilities: ['Documenting, and analyzing business requirements, processes, and workflows.', 'Designing and building new Power BI reports, as well as developing new Power BI data sets and data flows.', 'Maintaining existing reporting solutions.', 'Designing, building, testing, and deploying components for the Data Analytics Platform.', 'Implementing solutions for processing large datasets.', 'Improving data processing efficiency.', 'Creating advanced data flow models.', 'Refining and implementing business and technical requirements.']\n",
      "requirements: ['Technical Master’s or Bachelor’s degree or equivalent experience.', 'Experience in gathering business requirements and designing analytics solutions and understanding of fundamental programming principles.', 'Proficiency in Business Intelligence, SQL, Data Warehouses, Data Integration, Big Data solutions, and Streaming processing.', 'Experience in analytics and reporting, including SSAS, SSRS, and Power BI.', 'Hands-on experience with ETL tools, including SSIS, MS Azure Synapse, and ADF.', 'Familiarity with modern project management tools and processes (Azure DevOps, Agile, Scrum).', 'Fluency in English, both written and spoken.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-business-intelligence-lodz-aleksandrowska-67,oferta,1003877003?apid=bc376bcb-3392-4b00-81d0-c313c38bd10c&oca=None&acv=0\n",
      "\n",
      "--- Job Record 435 ---\n",
      "id: 488\n",
      "url: https://www.pracuj.pl/praca/senior-data-engineer-business-intelligence-lodz-aleksandrowska-67,oferta,1003877004\n",
      "title: Senior Data Engineer - Business Intelligence\n",
      "work_location: None\n",
      "validity: valid for 4 daysto 02 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior)\n",
      "work_arrangement: None\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Aleksandrowska 67, Bałuty, ŁódźŁódź, Łódź', 'contract of employment', 'full office work']\n",
      "technologies: ['Microsoft Power BI', 'SQL', 'SSIS', 'MS Azure Synapse', 'ADF', 'Azure DevOps', 'Agile', 'Scrum', 'agile', 'scrum']\n",
      "responsibilities: ['Documenting, and analyzing business requirements, processes, and workflows.', 'Designing and building new Power BI reports, as well as developing new Power BI data sets and data flows.', 'Maintaining existing reporting solutions.', 'Designing, building, testing, and deploying components for the Data Analytics Platform.', 'Implementing solutions for processing large datasets.', 'Improving data processing efficiency.', 'Creating advanced data flow models.', 'Refining and implementing business and technical requirements.']\n",
      "requirements: ['Technical Master’s or Bachelor’s degree or equivalent experience.', 'Experience in gathering business requirements and designing analytics solutions and understanding of fundamental programming principles.', 'Proficiency in Business Intelligence, SQL, Data Warehouses, Data Integration, Big Data solutions, and Streaming processing.', 'Experience in analytics and reporting, including SSAS, SSRS, and Power BI.', 'Hands-on experience with ETL tools, including SSIS, MS Azure Synapse, and ADF.', 'Familiarity with modern project management tools and processes (Azure DevOps, Agile, Scrum).', 'Fluency in English, both written and spoken.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/senior-data-engineer-business-intelligence-lodz-aleksandrowska-67,oferta,1003877004?apid=c193fa26-81d3-478d-983c-4b335c9202d5&oca=None&acv=0\n",
      "\n",
      "--- Job Record 436 ---\n",
      "id: 489\n",
      "url: https://www.pracuj.pl/praca/master-data-analyst-wroclaw,oferta,1003875549\n",
      "title: Master Data Analyst\n",
      "work_location: None\n",
      "validity: ważna jeszcze 4 dnido 02 marca 2025\n",
      "contract_type: umowa o pracę tymczasową\n",
      "employment_type: pełny etat\n",
      "position: młodszy specjalista (Junior)\n",
      "work_arrangement: praca zdalna\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['WrocławWrocław, dolnośląskie']\n",
      "technologies: ['Microsoft Excel', 'SAP', 'S/4 Hana']\n",
      "responsibilities: ['tworzenie i aktualizacja komponentów danych podstawowych (Klient/Dostawca/Materiał) w systemie SAP lub innych systemach ERP', 'wykonywanie zadań zgodnie z zasadami zarządzania danymi (data governance) i standardowymi procedurami', 'odpowiedzialność za monitorowanie i realizację zadań w przypisanej kolejce roboczej, rozwiązaniach workflow lub ogólnej skrzynce e-mailowej', 'współpraca z dostawcami danych funkcjonalnych lub wnioskodawcami w celu uzyskania odpowiednich danych', 'odpowiedzialność za terminowe wykonywanie przypisanych zadań zgodnie z ustalonymi wskaźnikami SLA (Service Level Agreement)', 'identyfikowanie możliwości poprawy jakości danych, efektywności procesów lub skrócenia czasu realizacji zadań', 'współpraca z innymi funkcjami lub grupami biznesowymi w celu rozwiązywania problemów w procesie end-to-end (E2E)', 'możliwość uczestnictwa w zarządzaniu projektami lub warsztatach mających na celu poprawę procesów lub efektywności operacyjnej']\n",
      "requirements: ['wykształcenie wyższe', 'znajomość języka angielskiego na poziomie minimum B2', 'biegła obsługa programu MS Excel', 'preferowane wcześniejsze doświadczenie w pracy z systemem SAP', 'mile widziane wcześniejsze doświadczenie w zakresie jakości danych, zarządzania danymi podstawowymi i/lub zarządzania danymi (data governance) w globalnym środowisku danych podstawowych lub znajomość procesów i danych w łańcuchu dostaw, produkcji lub zarządzaniu materiałami', 'doświadczenie w pracy z systemem S/4 Hana', 'dbałość o szczegóły i zamiłowanie do pracy z procesami operacyjnymi', 'umiejętność pracy zespołowej i współpracy', 'analityczne podejście do rozwiązywania problemów', 'umiejętność ustalania priorytetów zadań']\n",
      "application_link: https://www.pracuj.pl/aplikuj/master-data-analyst-wroclaw,oferta,1003875549?apid=119513a9-93c5-4864-aa13-7ec65a3b0bf9&oca=None&acv=0\n",
      "\n",
      "--- Job Record 437 ---\n",
      "id: 490\n",
      "url: https://www.pracuj.pl/praca/ekspertka-ekspert-ds-analiz-data-scientist-warszawa-chmielna-89,oferta,1003874798\n",
      "title: Ekspertka / Ekspert ds. analiz (Data Scientist)\n",
      "work_location: None\n",
      "validity: ważna jeszcze 3 dnido 01 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: None\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Chmielna 89, Wola, WarszawaWarszawa, mazowieckie', 'ekspert']\n",
      "technologies: ['SQL', 'Python', 'R', 'Microsoft Excel']\n",
      "responsibilities: ['analizujemy naprawdę duże zbiory danych, na temat aktywności ponad 10 mln klientów PKO BP,', 'pracujemy z szerokim zestawem narzędzi analitycznych i raportowych: SQL, R, Python, Power BI, Excel,', 'współtworzymy strategię banku w zakresie aktywacji i lojalizacji klienta detalicznego,', 'analizujemy zachowanie klientów i optymalizujemy formy korzystania z usług bankowych,', 'zamieniamy dane w informacje, informacje w wiedzę, a wiedzę w konkretne rozwiązania.']\n",
      "requirements: ['znasz dobrze SQL (warunek konieczny),', 'chcesz codziennie nurkować w oceanie danych i znajdować w nim odpowiedzi na trudne pytania,', 'znasz lub chcesz nauczyć się Pythona lub R (zapewniamy szkolenia).']\n",
      "application_link: https://www.pracuj.pl/aplikuj/ekspertka-ekspert-ds-analiz-data-scientist-warszawa-chmielna-89,oferta,1003874798?apid=ab0193a5-eac0-49b6-8ec9-d281ba6fc2c0&oca=None&acv=0\n",
      "\n",
      "--- Job Record 438 ---\n",
      "id: 491\n",
      "url: https://www.pracuj.pl/praca/data-scientist-warszawa,oferta,1003873953\n",
      "title: Data Scientist\n",
      "work_location: None\n",
      "validity: ważna jeszcze 3 dnido 01 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['SQL', 'Python']\n",
      "responsibilities: ['Realizacja innowacyjnych projektów w obszarze inteligentnej automatyzacji procesów', 'Projektowanie i wdrażanie rozwiązań dla różnych problemów biznesowych, m.in. w zakresie obsługi klienta, efektywności i optymalizacji procesów', 'Tworzenie modeli do przetwarzania tekstu i analizy języka naturalnego (NLP) oraz obrazów (Computer Vision) z wykorzystaniem różnych technik i algorytmów, w tym uczenia maszynowego', 'Praca w metodykach zwinnych oraz dzielenie się doświadczeniem z zespołem', 'Ciągłe zdobywanie wiedzy i rozwijanie swoich kompetencji']\n",
      "requirements: ['Znajomość zagadnień z obszaru analizy danych, statystyki, machine learning, drzew decyzyjnych, sieci neuronowych, analizy tekstu.', 'Wykształcenie wyższe (preferowane kierunki: informatyka, matematyka lub pokrewne).', 'Praktyczne doświadczenie w budowaniu modeli (LDA, LSTM, NER) w obszarze NLP i Computer Vision', 'Doświadczenie w pracy z bazami SQL oraz rozwiązaniami BigData (np. Hadoop, PySpark, Hive)', 'Praktyczne doświadczenie w programowaniu w Python z wykorzystaniem np. keras, tensorflow, scikit-learn, pandas, spacy, pytesseract, opencv, fastapi, flask', 'Umiejętności analityczne, kreatywność i samodzielność w poszukiwaniu rozwiązań', 'Praktyczne doświadczenie we wdrażaniu MLOps i znajomość narzędzi git, Kedro, ML Flow', 'Dobra znajomość systemów Linux/Unix oraz języków skryptowych']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-warszawa,oferta,1003873953?apid=3fe7689a-9209-4b38-bfa1-1b33e3875c26&oca=None&acv=0\n",
      "\n",
      "--- Job Record 439 ---\n",
      "id: 492\n",
      "url: https://www.pracuj.pl/praca/data-scientist-warszawa,oferta,1003892520\n",
      "title: Data Scientist\n",
      "work_location: None\n",
      "validity: valid for 11 daysto 09 March 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular), senior specialist (Senior)\n",
      "work_arrangement: home office work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['Python', 'AI/ML', 'NLP', 'Scikit-learn', 'Azure', 'Docker', 'Flask/Django', 'API']\n",
      "responsibilities: ['Design, develop, and deploy AI/ML models using Python and Scikit-learn.', 'Implement NLP techniques to analyze and interpret complex datasets.', 'Utilize general AI methodologies to enhance model performance and capabilities.', 'Leverage Azure AI services to build scalable and efficient AI solutions.', 'Develop and integrate APIs to enable seamless data exchange between systems.', 'Containerize AI/ML models using Docker for consistent deployment environments.', 'Manage the end-to-end deployment process of AI/ML applications.', 'Conduct basic testing of AI/ML applications to ensure quality and reliability.', 'Develop web applications and services using Flask or Django frameworks.', 'Communicate effectively with team members and stakeholders to align on project goals.', 'Collaborate with a global team to contribute to and drive projects forward.', 'Work independently on assignments, taking ownership of tasks and deliverables.']\n",
      "requirements: ['Certifications in Azure, Docker, or related technologies.', 'Contributions to open-source projects or a strong GitHub portfolio.', 'Prior experience in a similar role within a fast-paced, tech-driven environment.', 'Experience in agile development environments.', 'Knowledge of English at a minimum level is also required. B2.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-warszawa,oferta,1003892520?apid=3c961594-c508-4e1d-9268-57c97696ebea&oca=None&acv=0\n",
      "\n",
      "--- Job Record 440 ---\n",
      "id: 493\n",
      "url: https://www.pracuj.pl/praca/data-scientist-wroclaw,oferta,1003892477\n",
      "title: Data Scientist\n",
      "work_location: None\n",
      "validity: ważna jeszcze 11 dnido 09 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['WrocławWrocław, dolnośląskie']\n",
      "technologies: ['Python', 'SQL']\n",
      "responsibilities: ['Pobieranie, eksploracja i przetwarzanie danych z różnych źródeł w środowisku GCP (np. BigQuery, Cloud Storage)', 'Współpraca z Data Engineerami przy projektowaniu wydajnych pipeline’ów danych w GCP', 'Projektowanie, trenowanie i ewaluacja modeli (np. klasyfikacja, regresja, rekomendacje, NLP) z wykorzystaniem narzędzi ML w Pythonie (scikit-learn, TensorFlow, PyTorch)', 'Wykorzystywanie usług GCP do zarządzania cyklem życia modeli (Vertex AI, AI Platform, BigQuery ML)', 'Wdrażanie gotowych rozwiązań do środowisk testowych oraz produkcyjnych', 'Monitorowanie jakości i wydajności modeli (m.in. zbieranie metryk, alertowanie, retraining)', 'Doskonalenie procesów uczenia maszynowego, w tym feature engineering i dobór hiperparametrów', 'Konsultacje z interesariuszami biznesowymi w zakresie wymagań analitycznych oraz możliwości zastosowania rozwiązań ML', 'Bliska współpraca z Data Engineerami i DevOpsami w celu zapewnienia skalowalności i niezawodności rozwiązań w chmurze', 'Prezentacja wniosków i wyników w sposób przystępny dla odbiorców nietechnicznych', 'Udział w tworzeniu i rozwijaniu procesów MLOps (konteneryzacja modeli, CI/CD, automatyzacja pipeline’ów, monitoring)', 'Ciągłe poszukiwanie nowych metod i narzędzi w obszarze Machine Learning, data science oraz usług GCP', 'Tworzenie i utrzymywanie dokumentacji dotyczącej procesów analitycznych, modeli oraz pipeline’ów danych w środowisku GCP']\n",
      "requirements: ['Ukończone studia wyższe (preferowane kierunki: Informatyka, Matematyka, Statystyka, Fizyka lub pokrewne)', 'Minimum 2 lata doświadczenia na stanowisku Data Scientist lub pokrewnym (analityk danych, ML Engineer)', 'Biegła obsługa Python w zakresie analizy danych (pandas, NumPy) i Machine Learning (scikit-learn, TensorFlow, PyTorch)', 'Bardzo dobra umiejętność pisania i optymalizacji zapytań SQL (szczególnie w kontekście baz typu BigQuery)', 'Doświadczenie w pracy z usługami GCP (BigQuery, Cloud Storage, Vertex AI / AI Platform, Dataflow, Pub/Sub, Composer itp.)', 'Rozumienie zasad projektowania i optymalizacji rozwiązań w chmurze', 'Wiedza z zakresu statystyki, metod eksploracji danych oraz praktyk ML (przygotowanie danych, walidacja, interpretacja wyników)', 'Doświadczenie w MLOps (CI/CD, Docker, Kubernetes, pipeline’y ML w GCP)', 'Znajomość innych narzędzi chmurowych (AWS, Azure) i/lub frameworków Big Data (Spark)', 'Umiejętność tworzenia przejrzystych raportów i wizualizacji (matplotlib, seaborn, Looker, Data Studio)', 'Umiejętność współpracy w interdyscyplinarnym zespole', 'Dobre zdolności komunikacyjne, w tym prezentowania wyników i rekomendacji', 'Samodzielność i proaktywne podejście do rozwiązywania problemów']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-scientist-wroclaw,oferta,1003892477?apid=f76c79e7-4bfb-405f-8d8f-de6c3b487577&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 441 ---\n",
      "id: 494\n",
      "url: https://www.pracuj.pl/praca/data-engineer-wroclaw,oferta,1003892471\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 11 dnido 09 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['WrocławWrocław, dolnośląskie']\n",
      "technologies: ['SQL', 'Docker', 'Kubernetes']\n",
      "responsibilities: ['Projektowanie i implementacja przepływów danych (ETL/ELT) w GCP', 'Wykorzystanie usług takich jak Dataflow, Pub/Sub, BigQuery, Dataproc, Composer', 'Zapewnianie skalowalności, bezpieczeństwa i wydajności rozwiązań', 'Tworzenie i optymalizacja zapytań SQL', 'Odpowiedzialność za efektywną analizę i integrację danych', 'Monitorowanie i poprawa wydajności zapytań', 'Programowanie w języku Python', 'Automatyzacja procesów przetwarzania danych, integracja z zewnętrznymi systemami', 'Tworzenie narzędzi wspierających pracę zespołu, testowanie kodu i utrzymanie wysokiej jakości rozwiązań', 'Implementacja przetwarzania wsadowego i/lub w czasie rzeczywistym', 'Korzystanie z usług GCP (Dataproc, Composer, Cloud Functions) w zależności od potrzeb projektowych', 'Projektowanie architektur przetwarzania danych dostosowanych do konkretnych wymagań biznesowych', 'Zapewnianie jakości danych (Data Quality)', 'Monitorowanie procesów, szybkie wykrywanie i rozwiązywanie problemów związanych z wydajnością oraz niezawodnością', 'Stosowanie najlepszych praktyk w zakresie walidacji i kontroli jakości danych', 'Współpraca z zespołami Data Science / BI', 'Przygotowywanie i dostarczanie danych do celów analitycznych', 'Udział w tworzeniu hurtowni danych oraz innych repozytoriów (Data Lake)', 'Tworzenie i utrzymywanie dokumentacji technicznej', 'Dokumentowanie rozwiązań, architektury i procesów przetwarzania danych', 'Dbanie o przejrzystość i aktualność informacji technicznych dla zespołu', 'Optymalizacja kosztów i bezpieczeństwa w GCP', 'Analiza wykorzystania zasobów chmurowych w celu ograniczenia kosztów', 'Dbanie o odpowiednie zabezpieczenia (IAM, VPC, szyfrowanie danych)']\n",
      "requirements: ['Ukończone studia wyższe (preferowane kierunki: Informatyka, Matematyka, Statystyka, Fizyka lub pokrewne)', 'Minimum 2-3 lata na stanowisku Data Engineera lub pokrewnym', 'Praktyczne doświadczenie w pracy z usługami takimi jak BigQuery, Dataflow, Pub/Sub, Dataproc, Composer', 'Zaawansowane pisanie i optymalizacja zapytań SQL w celu analizy i integracji danych', 'Tworzenie skryptów, automatyzacja procesów, testowanie kodu', 'Doświadczenie w projektowaniu i utrzymywaniu procesów ETL/ELT, najlepiej z wykorzystaniem narzędzi GCP lub rozwiązań open source (np. Airflow)', 'Wersjonowanie, testowanie, CI/CD, monitoring, a także zrozumienie architektury danych (hurtownie, Data Lake)', 'Doświadczenie w pobieraniu i przetwarzaniu danych z różnych źródeł, w tym REST API, SOAP, GraphQL lub innych usług zewnętrznych', 'Projektowanie skalowalnych i niezawodnych mechanizmów integracji danych', 'Umiejętność współpracy w zespole, komunikatywność, inicjatywa w rozwiązywaniu problemów', 'Doświadczenie w analizie danych i/lub Data Science', 'Znajomość narzędzi DevOps, Dockera, Kubernetes (lub innych platform konteneryzacyjnych)']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-wroclaw,oferta,1003892471?apid=34a5a6bb-8aa0-4500-8094-5356debc2323&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 442 ---\n",
      "id: 495\n",
      "url: https://www.pracuj.pl/praca/koordynator-projektow-it-i-wsparcia-biznesu-gniezno-ignacego-paderewskiego-25,oferta,1003876180\n",
      "title: Koordynator Projektów IT i Wsparcia Biznesu\n",
      "work_location: None\n",
      "validity: ważna jeszcze 4 dnido 02 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: None\n",
      "work_arrangement: praca stacjonarna, praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Ignacego Paderewskiego 25, GnieznoGniezno, wielkopolskie', 'kierownik / koordynator']\n",
      "technologies: ['ERP Gardens', 'Symfonia R2płatnik', 'Transact-SQL', 'T-SQL', 'wewnątrz organizacji', 'rozwijasz kilka projektów jednocześnie', 'masz wpływ na wybór narzędzi i technologii', 'agile', 'scrum']\n",
      "responsibilities: ['Analiza wymagań biznesowych i komunikacja z użytkownikami, z osobami z obszarów IT oraz kierownictwem w celu ustalenia nowych rozwiązań.', 'Koordynowanie przydzielonych projektów IT, we wszystkich etapach wdrożenia.', 'Odpowiedzialność za terminową realizację harmonogramu projektu.', 'Rozliczanie projektów oraz dbałość o dokumentację formalną i projektową.', 'Ścisła współpraca z innymi działami firmy w celu skutecznej realizacji projektów.', 'Testowanie i wspieranie biznesu w testowaniu oraz rozwoju systemów.', 'Wsparcie użytkowników w rozwiązywaniu problemów podczas używania aplikacji biznesowych.', 'Współadministrowanie i utrzymanie systemu po wdrożeniu.', 'Techniczny i merytoryczny nadzór nad działaniem aplikacji biznesowych.', 'Organizacja szkoleń i warsztatów dla pracowników z zakresu nowych funkcji.']\n",
      "requirements: ['Około 2-letnie doświadczenie  w rozwoju oprogramowania lub we wdrażaniu systemów, (kandydaci z mniejszym doświadczeniem będą brani pod uwagę na stanowisko Analityk Biznesowo-Systemowy.', 'Umiejętność współpracy z biznesem i zrozumienia potrzeb biznesowych oraz przełożenia ich na rozwiązania informatyczne.', 'Umiejętność samodzielnego rozwiązywania problemów.', 'Asertywność w prowadzeniu projektów oraz inicjatywa w działaniu.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/koordynator-projektow-it-i-wsparcia-biznesu-gniezno-ignacego-paderewskiego-25,oferta,1003876180?apid=f9b238e3-6cf0-453f-90df-1b78fa6da1ab&oca=None&acv=0\n",
      "\n",
      "--- Job Record 443 ---\n",
      "id: 496\n",
      "url: https://www.pracuj.pl/praca/data-engineer-warszawa-chmielna-89,oferta,1003872701\n",
      "title: Data Engineer\n",
      "work_location: None\n",
      "validity: ważna jeszcze 3 dnido 01 marca 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Chmielna 89, Wola, WarszawaWarszawa, mazowieckie']\n",
      "technologies: ['Google Cloud Platform', 'Python', 'Oracle', 'PostgreSQL', 'Kubernetes']\n",
      "responsibilities: ['budujemy procesy zasilania operacyjnej składnicy danych,', 'projektujemy i realizujemy rozwiązania informatyczne w obszarze zasilania danych,', 'analizujemy dane oraz przygotowujemy dla nich modele logiczne i fizyczne,', 'stroimy zapytania SQL na bazie relacyjnych i hurtowianych w on-prem i na Chmurze GCP,', 'tworzymy przepływy off-linowe oraz strumieniowe danych,', 'przygotowujemy datamarty tematyczne na bazach relacyjnych i kolumnowych,', 'tworzymy koncepcje rozwiązań i implementacje rozwiązań w oparciu o wymagania i potrzeby biznesu,', 'usuwamy incydenty i bierzemy udział w rozwiązywaniu problemów,', 'wspieramy testy i wdrażania zmian w zakresie procesów zasilania danych,', 'pomagamy przy serwisowaniu wdrożonego oprogramowania,', 'tworzymy rozwiązania ładowania danych w Python,', 'pracujemy zgodnie z modelem DevOps.']\n",
      "requirements: ['posiadasz doświadczenia jako Data Engineer:', 'w obszarze procesów integracji danych za pomocą narzędzi ETLowych (Informaitica Power Center),', 'w obszarze technologii GCP (Big Query, Pub-Sub, Google Cloud Anthos, Cloud Storage Buckets),', 'w programowaniu w języku Phyton (znajomość jednego z framework Phyton: Flask, Django),', 'w projektowaniu i tworzeniu relacyjnych baz danych (PostgresSQL, Oracle),', 'pracujesz w środowisku chmurowym na Google Cloud Platform (Big Query, Cloud SQL, Kubernetes),', 'znasz środowisko Linux (doświadczenie w pracy z terminalem, podstawowa znajomość skryptów powłoki bash),', 'znasz narzędzie harmonogramujące: Apache Airflow, AutomateNow,', 'masz doświadczenie w pracy z dużymi wolumenami danych ~100TB.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-engineer-warszawa-chmielna-89,oferta,1003872701?apid=1476bb64-5ecf-4d5a-9e16-4c55c7e4c669&oca=None&acv=0\n",
      "\n",
      "--- Job Record 444 ---\n",
      "id: 497\n",
      "url: https://www.pracuj.pl/praca/specjalista-specjalistka-ds-baz-danych-i-raportowania-warszawa-domaniewska-48,oferta,1003881283\n",
      "title: Specjalista / Specjalistka ds. Baz Danych i Raportowania\n",
      "work_location: None\n",
      "validity: ważna jeszcze 2 dnido 28 lutego 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Domaniewska 48, Mokotów, WarszawaWarszawa, mazowieckie', 'praca stacjonarna']\n",
      "technologies: ['SQL', 'T-SQL', 'Python', 'R', 'VBA']\n",
      "responsibilities: ['Projektowanie i architektura baz danych w tym planowanie i wdrażanie procesów ETL (Extract, Transform, Load)', 'Zarządzanie procesami i danymi: Opracowywanie i dokumentowanie procesów przepływu danych, Zapewnienie spójności i integralności danych w systemach', 'Rozwiązywanie problemów związanych z jakością i bezpieczeństwem danych', 'Automatyzacja procesów: Implementacja rozwiązań pozwalających na automatyczne monitorowanie i raportowanie', 'Przygotowywanie raportów i zestawień w narzędziach takich jak Microsoft Excel, Power BI, lub inne narzędzia raportowe', 'Projektowanie i budowanie dashboardów wizualizacyjnych', 'Wsparcie ad hoc czyli rozwiązywanie bieżących problemów związanych z raportowaniem oraz analiza danych na żądanie', 'Zbieranie wymagań od zespołów biznesowych dotyczących raportowania i wdrażanie nowych rozwiązań']\n",
      "requirements: ['Min. 2 lata doświadczenia w pracy z bazami danych [SQL SERVER] oraz projektowaniem procesów przepływu danych', 'Doświadczenie w tworzeniu raportów i analizie danych', 'Bardzo dobra znajomość SQL (T-SQL, SQL) i umiejętność optymalizacji zapytań / procedur / widoków', 'Doświadczenie z systemami zarządzania bazami danych (MS SQL Server)', 'Praktyczna znajomość narzędzi ETL (np. SSIS)', 'Umiejętność pracy z narzędziami raportowymi (Power BI, Excel)', 'Znajomość modelowania danych oraz zasad projektowania relacyjnych i hurtowni danych', 'Umiejętność analitycznego myślenia i rozwiązywania problemów', 'Umiejętność organizacji pracy własnej i zarządzania priorytetami']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-specjalistka-ds-baz-danych-i-raportowania-warszawa-domaniewska-48,oferta,1003881283?apid=07834c4c-4556-4f8e-be92-52f5221ec7b0&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 445 ---\n",
      "id: 498\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-wdrozen-platformy-webcon-bps-wroclaw,oferta,1003873166\n",
      "title: Specjalista ds. Wdrożeń platformy Webcon BPS\n",
      "work_location: None\n",
      "validity: ważna jeszcze 3 dnido 01 marca 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['WrocławWrocław, dolnośląskie']\n",
      "technologies: ['SQL']\n",
      "responsibilities: ['Administrowanie oraz rozwój aplikacji budowanych na platformie Webcon BPS.', 'Konfigurowanie procesów (Webcon BPS)', 'Rozwiązywanie problemów technicznych.', 'Tworzenie dokumentacji analityczne, powdrożeniowej.', 'Prowadzenie szkoleń, testów wewnętrznych, testów UAT.', 'Migracje i wdrożenia rozwiązań opartych na Webcon BPS.']\n",
      "requirements: ['Minimum 2-letnie doświadczenie w rozwoju systemów obiegów dokumentów/workflow na bazie Webcon BPS.', 'Umiejętność administrowania, budowania i wdrażania rozwiązań na platformie Webcon BPS.', 'Znajomość systemów klasy ERP, CRM.', 'Znajomość pracy z bazami danych oraz języka SQL.', 'Znajomość Webservices, SOAP, WSDL, REST API, JSON.', 'Komunikatywność i zdolność analitycznego myślenia.', 'Umiejętność rozwiązywania problemów i proponowania optymalnych rozwiązań.', 'Otwartość na rozwój.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-wdrozen-platformy-webcon-bps-wroclaw,oferta,1003873166?apid=08790638-e853-4fed-bfc6-dd8b360b79a5&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 446 ---\n",
      "id: 499\n",
      "url: https://www.pracuj.pl/praca/infrastructure-consultant-for-bosch-location-infrastructure-services-lis-site-le-lodz-skladowa-35,oferta,1003870161\n",
      "title: Infrastructure Consultant for Bosch Location Infrastructure Services (LIS Site Lead)\n",
      "work_location: None\n",
      "validity: valid for 2 daysto 28 February 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Składowa 35, Śródmieście, ŁódźŁódź, Łódź', 'contract of employment', 'Запрошуємо працівників з України']\n",
      "technologies: []\n",
      "responsibilities: []\n",
      "requirements: []\n",
      "application_link: https://www.pracuj.pl/aplikuj/infrastructure-consultant-for-bosch-location-infrastructure-services-lis-site-le-lodz-skladowa-35,oferta,1003870161?apid=de44170a-d9cd-416a-aafd-009723634e12&oca=None&acv=0\n",
      "\n",
      "--- Job Record 447 ---\n",
      "id: 500\n",
      "url: https://www.pracuj.pl/praca/lead-data-scientist-revenue-growth-warszawa-prosta-51,oferta,1003871873\n",
      "title: Lead Data Scientist Revenue Growth\n",
      "work_location: None\n",
      "validity: valid for 2 daysto 28 February 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: senior specialist (Senior), expert\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Prosta 51, Wola, WarszawaWarszawa, Masovian', 'contract of employment']\n",
      "technologies: ['Python', 'R', 'SQL']\n",
      "responsibilities: ['Conduct comprehensive analysis of large datasets to proactively identify insights and provide recommendations on possible improvements in commercial planning area', 'Apply data science to identify new patterns and correlations across multiple factors (e.g. pricing, merchandising, promotion, communication channels etc.)', 'Explore transactional data to discover insights in purchase behaviour to enrich customer database and ensure better personalization and more targeted marketing strategies and communication', 'Take an active part in the design and implementation of AI/ML solutions to improve and streamline existing processes as well as suggest new areas for application of data science to meet evolving business requirements', 'Collaborate with the stakeholders and cross-functional teams to ensure alignment across the organization and present findings and insights, adapting communication for diverse audiences both with technical and non-technical backgrounds']\n",
      "requirements: ['3+ years of experience in science and data engineering', 'A proven track record of successful data science projects or AI / ML solutions implementation; experience of leading a team is an advantage', 'Bachelor’s or Master’s degree in Statistics, Mathematics, Computer science or another related field', 'Proficient in Microsoft applications esp. Azure, SQL, programming languages such as Python, R', 'Knowledge of libraries (e.g. pandas, scikit learn) and notebooks (e.g. Jupyter)', 'Customer oriented with excellent communication and presentation skills and ability to communicate complex data clearly and concisely in both verbal and written forms', 'Proficiency in English', 'Result driven, proactive and team oriented']\n",
      "application_link: https://www.pracuj.pl/aplikuj/lead-data-scientist-revenue-growth-warszawa-prosta-51,oferta,1003871873?apid=94ed4aa0-5cd3-466e-a9d8-1f597ad7e4d3&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 448 ---\n",
      "id: 501\n",
      "url: https://www.pracuj.pl/praca/programista-programistka-sql-bielsko-biala-stefana-okrzei-2,oferta,1003869146\n",
      "title: Programista / Programistka SQL\n",
      "work_location: None\n",
      "validity: ważna jeszcze dzieńdo 27 lutego 2025\n",
      "contract_type: kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca zdalna\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Stefana Okrzei 2, Bielsko-BiałaBielsko-Biała, śląskie', 'Запрошуємо працівників з України', 'Робота для іноземцівбез польської']\n",
      "technologies: ['T-SQL', 'C#', '.NET', 'Angular']\n",
      "responsibilities: ['tworzenie i optymalizowanie zapytań/procedur w języku T-SQL', 'budowanie raportów od podstaw oraz rozwijanie już istniejących', 'tworzenie rekomendacji i wdrażanie najlepszych praktyk w zakresie przetwarzania danych', 'współtworzenie planów rozwoju oprogramowania', 'monitorowanie wydajności oraz zapewnienie poprawności generowanych wyników', 'rozwijanie własnych umiejętności współpracując z doświadczonym zespołem']\n",
      "requirements: ['bardzo dobra znajomość T-SQL/SQL oraz struktur baz danych', 'analityczne podejście do problemów biznesowych', 'Znajomość MS SQL Server', 'minimum 2 lata doświadczenia w pracy z bazami danych', 'wykształcenie wyższe', 'dobra znajomość języka angielskiego, umożliwiająca komunikację w wewnętrznym zespole międzynarodowym', 'samodzielność, komunikatywność, dobra organizacja pracy']\n",
      "application_link: https://www.pracuj.pl/aplikuj/programista-programistka-sql-bielsko-biala-stefana-okrzei-2,oferta,1003869146?apid=9f696595-f236-451a-8f8c-3df8e176a19d&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 449 ---\n",
      "id: 502\n",
      "url: https://www.pracuj.pl/praca/data-management-analyst-krakow-opolska-110,oferta,1003868060\n",
      "title: Data Management Analyst\n",
      "work_location: None\n",
      "validity: valid for a dayto 27 February 2025\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: full office work, hybrid work\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['Opolska 110, Prądnik Biały, KrakówKraków, Lesser Poland', 'contract of employment']\n",
      "technologies: ['SAP']\n",
      "responsibilities: ['Great knowledge and hands on practise in SAP (MDS will be a great assset)', 'Knowledge of the materials transactios for material creation/change/deletion, (mm01,mm02,mm03,mm17)', 'Knowledge of tables transaction se16n/se16/sqvi', 'Job related knowledge – what is material, what data material can cointain, types of materials, what it is info record and how to create this', 'LSMW is a huge plus', 'Knowledge how to write guidelines or operating procedures (experience with this is a plus)', 'Expieriens 4 -5 years in Master Data Materials domain']\n",
      "requirements: []\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-management-analyst-krakow-opolska-110,oferta,1003868060?apid=f2736c53-94c5-4038-911f-517fd21bef65&oca=None&acv=0\n",
      "\n",
      "--- Job Record 450 ---\n",
      "id: 503\n",
      "url: https://www.pracuj.pl/praca/business-intelligence-analyst-krakow-pawia-21,oferta,1003866320\n",
      "title: Business Intelligence Analyst\n",
      "work_location: None\n",
      "validity: None\n",
      "contract_type: None\n",
      "employment_type: full-time\n",
      "position: specialist (Mid / Regular)\n",
      "work_arrangement: hybrid work\n",
      "start: None\n",
      "recruitment_method: remote recruitment\n",
      "additional_info: ['Pawia 21, Stare Miasto, KrakówKraków, Lesser Poland', 'valid 5 hours', 'contract of employment']\n",
      "technologies: ['SQL', 'Microsoft SQL Server', 'Oracle', 'Google Cloud Platform', 'Hadoop', 'Apache Spark', 'Apache Kafka', 'Tableau', 'Microsoft Power BI', 'Python', 'R', 'Scala', 'AI/ML']\n",
      "responsibilities: ['Proficient in data integration, ETL processes, and data pipeline development.', 'Strong knowledge of SQL, relational databases (e.g., Microsoft, Oracle), and cloud data platforms (e.g., Google Cloud, BigQuery).', 'Experience with data processing frameworks (e.g., Hadoop, Spark) and stream processing systems (e.g., Kafka).', 'Demonstrated ability to analyze complex datasets and generate actionable insights.', 'Experience with data visualization tools (e.g., Tableau, Power BI) and building executive dashboards.', 'Strong programming skills in languages such as Python, R, or Scala.', 'Excellent communication skills, with the ability to translate technical data into business insights for diverse audiences.', 'Proven ability to collaborate cross-functionally and lead projects from concept to completion.', 'Familiarity with data security, governance, and compliance best practices.']\n",
      "requirements: ['Experience with machine learning, AI, and big data techniques.', 'Knowledge of hybrid cloud environments and distributed systems.', 'Familiarity with BI tools and data warehousing solutions.', 'Operational experience in business analytics and data-driven decision-making.', 'Experience with agile methodologies and user-centered design processes.', 'Understanding of enterprise data governance and security frameworks.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/business-intelligence-analyst-krakow-pawia-21,oferta,1003866320?apid=56a7c3fd-ce02-4e30-866a-7b2ee5865ccc&oca=None&acv=0\n",
      "\n",
      "--- Job Record 451 ---\n",
      "id: 504\n",
      "url: https://www.pracuj.pl/praca/specjalista-ds-aplikacji-i-baz-danych-poznan,oferta,1003868974\n",
      "title: Specjalista ds. aplikacji i baz danych\n",
      "work_location: None\n",
      "validity: ważna jeszcze dzieńdo 27 lutego 2025\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: None\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['PoznańPoznań, wielkopolskie', 'praca stacjonarna']\n",
      "technologies: []\n",
      "responsibilities: ['udzielanie bezpośredniego (na miejscu), telefonicznego oraz zdalnego wsparcia technicznego dla pracowników Spółki w zakresie działania aplikacji', 'zarządzanie, monitorowanie i rozwiązywanie problemów dotyczących użytkowania aplikacji działających na serwerach posiadanych przez Spółkę', 'wsparcie użytkowników komputerów w rozwiązywaniu bieżących problemów z działaniem aplikacji', 'zarządzanie i administrowanie kontami użytkowników w zakresie aplikacji i poczty elektronicznej', 'tworzenie dokumentacji technicznej', 'przygotowywanie i tworzenie dokumentacji związanej z udzielaniem zamówień publicznych w zakresie nabywania i utrzymywania aplikacji']\n",
      "requirements: ['wykształcenie techniczne, informatyczne lub pokrewne', 'wiedza na temat instalacji i konfiguracji aplikacji w środowisku klient-serwer', 'znajomość instalacji, konfiguracji oraz obsługi systemów operacyjnych Microsoft Windows i pakietu Microsoft 365', 'umiejętność konfiguracji użytkowników, poczty e-mail, Sharepoint, itp.', 'podstawowa wiedza na temat zagadnień sieciowych LAN i baz danych', 'umiejętność diagnozowania usterek i pomocy pracownikom na miejscu i zdalnie', 'komunikatywność i umiejętność pracy w zespole', 'wysoka kultura osobista i umiejętności interpersonalne', 'sumienność, odpowiedzialność i zaangażowanie', 'samodzielność w działaniu', 'chęć poszerzania wiedzy i podnoszenia kwalifikacji']\n",
      "application_link: https://www.pracuj.pl/aplikuj/specjalista-ds-aplikacji-i-baz-danych-poznan,oferta,1003868974?apid=21dd3a5c-72e0-441f-8175-9d03ade18cc7&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 452 ---\n",
      "id: 505\n",
      "url: https://www.pracuj.pl/praca/data-science-business-analyst-gdansk-aleja-grunwaldzka-413,oferta,1003868608\n",
      "title: Data Science & Business Analyst\n",
      "work_location: None\n",
      "validity: ważna jeszcze dzieńdo 27 lutego 2025\n",
      "contract_type: umowa o pracę, kontrakt B2B\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: Praca od zaraz\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['aleja Grunwaldzka 413, Oliwa, GdańskGdańsk, pomorskie']\n",
      "technologies: ['MMSQL', 'Python', 'R', 'PostgreSQL', 'Google Analytics', 'GA4', 'BigQuery', 'GTM', 'Looker Studio', 'wewnątrz organizacji', 'rozwijasz kilka projektów jednocześnie', 'masz wpływ na wybór narzędzi i technologii', 'masz wpływ na rozwiązania technologiczne', 'analityk biznesowy']\n",
      "responsibilities: []\n",
      "requirements: []\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-science-business-analyst-gdansk-aleja-grunwaldzka-413,oferta,1003868608?apid=0f629a72-828a-461c-a84c-87a8889668a0&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 453 ---\n",
      "id: 506\n",
      "url: https://www.pracuj.pl/praca/data-analyst-warszawa,oferta,1003864913\n",
      "title: Data Analyst\n",
      "work_location: None\n",
      "validity: None\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: None\n",
      "additional_info: ['WarszawaWarszawa, mazowieckie', 'ważna 5 godzin']\n",
      "technologies: ['Microsoft Power BI', 'Microsoft Excel', 'Power Automate', 'SharePoint']\n",
      "responsibilities: ['Create reports and presentations to support business process improvements;', 'Collaborate with IT on database modeling, operational reporting, and data optimization;', 'Manage and maintain the Master Data Management (MDM) strategy, ensuring data accuracy and consistency;', 'Utilize tools like PowerBI, SharePoint, and Power Automate to support data management and reporting;', 'Lead User Acceptance Testing (UAT) for operational systems.']\n",
      "requirements: ['Proficiency in PowerBI or similar tools;', 'Advanced Excel skills; familiarity with Power Automate and SharePoint;', 'Strong analytical, numerical, and project management skills.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/data-analyst-warszawa,oferta,1003864913?apid=c7faad4a-ea56-492c-933d-b71fa9fcf693&oca=None&acv=0\n",
      "\n",
      "--- Job Record 454 ---\n",
      "id: 507\n",
      "url: https://www.pracuj.pl/praca/mlodszy-specjalista-mlodsza-specjalistka-ds-zarzadzania-danymi-produktowymi-jaslo,oferta,1003866554\n",
      "title: Młodszy Specjalista / Młodsza Specjalistka ds. Zarządzania Danymi Produktowymi\n",
      "work_location: None\n",
      "validity: None\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: młodszy specjalista (Junior)\n",
      "work_arrangement: None\n",
      "start: Praca od zaraz\n",
      "recruitment_method: None\n",
      "additional_info: ['JasłoJasło, podkarpackie', 'ważna 5 godzin', 'praca stacjonarna']\n",
      "technologies: []\n",
      "responsibilities: ['tworzenie oraz zarządzenie bazą danych produktu opartą na konfiguracyjnym i relacyjno-obiektowym modelu danych', 'tworzenie połączeń danych z bazy graficznej z danymi komercyjnymi', 'utrzymanie i rozbudowa modelu danych produktowych', 'dbanie o jakość, poprawność oraz kompletność danych dotyczących produktów w wewnętrznych systemach i aplikacjach zarządzanych przez Zespół', 'przeprowadzanie testów, wprowadzanie oraz aktualizacja danych produktowych w zarządzanych wewnątrz działu programach i aplikacjach', 'udział w projektach związanych z wprowadzaniem nowych produktów na rynek']\n",
      "requirements: ['wykształcenie wyższe, preferowane kierunki ścisłe: np. informatyka, matematyka', 'znajomość obsługi programów komputerowych MS Office włącznie z MS Access', 'znajomość języka angielskiego umożliwiająca swobodne porozumiewanie się w mowie i piśmie', 'zdolności analityczne, umiejętność kojarzenia danych z różnych źródeł oraz trafne wyciągania wniosków', 'umiejętność pracy w zespole']\n",
      "application_link: https://www.pracuj.pl/aplikuj/mlodszy-specjalista-mlodsza-specjalistka-ds-zarzadzania-danymi-produktowymi-jaslo,oferta,1003866554?apid=449b0ad9-02b4-441b-8b23-6ffdc34df2b2&oca=CouldNotUse&acv=0\n",
      "\n",
      "--- Job Record 455 ---\n",
      "id: 508\n",
      "url: https://www.pracuj.pl/praca/programista-pl-sql-programistka-pl-sql-warszawa-aleja-niepodleglosci-188b,oferta,1003865412\n",
      "title: Programista PL/ SQL / Programistka PL/ SQL\n",
      "work_location: None\n",
      "validity: None\n",
      "contract_type: umowa o pracę\n",
      "employment_type: pełny etat\n",
      "position: specjalista (Mid / Regular)\n",
      "work_arrangement: praca hybrydowa\n",
      "start: None\n",
      "recruitment_method: rekrutacja zdalna\n",
      "additional_info: ['Aleja Niepodległości 188B, Śródmieście, WarszawaWarszawa, mazowieckie', 'ważna 5 godzin']\n",
      "technologies: ['SQL', 'Git', 'MySQL', 'PostgreSQL']\n",
      "responsibilities: ['Projektowanie, implementacja i utrzymanie hurtowni danych,', 'Tworzenie zapytań SQL i pakietów PL/SQL,', 'Aktualizacja dokumentacji projektowej.']\n",
      "requirements: ['Wykształcenie wyższe kierunkowe', 'Minimum dwuletnie doświadczenie w operacyjnej pracy na podobnym stanowisku,', 'Dobra znajomość relacyjnych baz danych Oracle: projektowania struktur bazodanowych, biegła znajomość SQL, biegła znajomość PL/SQL - umiejętność wykorzystywania i tworzenia funkcji, procedur i pakietów, znajomość metod optymalizacji zapytań SQL, doświadczenie w pracy z dokumentami JSON (lub XML) przetwarzanymi w bazie danych,', 'Umiejętność pisania wydajnych zapytań w języku SQL,', 'Umiejętność optymalizacji złożonych zapytań SQL,', 'Wiedza z zakresu DWH,', 'Doświadczenie w pracy z narzędziami ETL,', 'Znajomość podstaw administracji serwerem bazodanowym Oracle,', 'Znajomość zagadnień wydajnościowych i zagadnień bezpieczeństwa w warstwie bazodanowej,', 'Zdolności analityczne oraz umiejętność modelowania danych.']\n",
      "application_link: https://www.pracuj.pl/aplikuj/programista-pl-sql-programistka-pl-sql-warszawa-aleja-niepodleglosci-188b,oferta,1003865412?apid=0ff9c23e-67b6-47dc-98e6-95138a938639&oca=CouldNotUse&acv=0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    job_records = query_db(\"jobs.db\")\n",
    "    for idx, record in enumerate(job_records, start=1):\n",
    "        print(f\"\\n--- Job Record {idx} ---\")\n",
    "        for key, value in record.items():\n",
    "            print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf8ac1f-f388-4915-a0ac-df195f909ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
